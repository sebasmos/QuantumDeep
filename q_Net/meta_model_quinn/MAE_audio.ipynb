{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0340b2b5-04a4-4a7f-b5ad-19ff4a30207f",
   "metadata": {},
   "source": [
    "# Model for audio data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fa3c46-27d1-4fd0-8380-bc701cf44772",
   "metadata": {
    "tags": []
   },
   "source": [
    "Ensure the following requirements are ready: \n",
    "\n",
    "```\n",
    "numpy==1.23.5\n",
    "torch==2.0.0+cu118\n",
    "matplotlib==3.7.1\n",
    "pillow==10.2.0\n",
    "timm==0.3.2\n",
    "```\n",
    "\n",
    "Then `git clone https://github.com/facebookresearch/mae.git`\n",
    "\n",
    "Run this notebook within `mae/` folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce1c58f-d275-4b39-82cb-9ec06349b322",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "370eb3e4-97f2-401c-83ba-b849b83761c6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sebastian/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n",
      "2024-02-11 21:27:00.659029: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-11 21:27:00.659063: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-11 21:27:00.660121: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-11 21:27:00.665389: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-11 21:27:01.442657: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy version: 1.23.5\n",
      "CUDA version: 11.8 - Torch versteion: 2.0.0+cu118 - device count: 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f19bcd0b470>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import Subset, DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import pandas as pd \n",
    "# from MAE code\n",
    "from util.datasets import build_dataset\n",
    "import argparse\n",
    "import util.misc as misc\n",
    "import argparse\n",
    "import datetime\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import timm\n",
    "\n",
    "assert timm.__version__ == \"0.3.2\" # version check\n",
    "from timm.models.layers import trunc_normal_\n",
    "from timm.data.mixup import Mixup\n",
    "from timm.loss import LabelSmoothingCrossEntropy, SoftTargetCrossEntropy\n",
    "\n",
    "import util.lr_decay as lrd\n",
    "import util.misc as misc\n",
    "from util.datasets import build_dataset\n",
    "from util.pos_embed import interpolate_pos_embed\n",
    "from util.misc import NativeScalerWithGradNormCount as NativeScaler\n",
    "\n",
    "import models_vit\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import models_mae\n",
    "import torch; print(f'numpy version: {np.__version__}\\nCUDA version: {torch.version.cuda} - Torch versteion: {torch.__version__} - device count: {torch.cuda.device_count()}')\n",
    "\n",
    "from engine_finetune import train_one_epoch, evaluate\n",
    "from timm.data import Mixup\n",
    "from timm.utils import accuracy\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "imagenet_mean = np.array([0.485, 0.456, 0.406])\n",
    "imagenet_std = np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "def show_image(image, title=''):\n",
    "    # image is [H, W, 3]\n",
    "    assert image.shape[2] == 3\n",
    "    plt.imshow(torch.clip((image * imagenet_std + imagenet_mean) * 255, 0, 255).int())\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.axis('off')\n",
    "    return\n",
    "\n",
    "def prepare_model(chkpt_dir, arch='mae_vit_large_patch16'):\n",
    "    # build model\n",
    "    model = getattr(models_mae, arch)()\n",
    "    # load model\n",
    "    checkpoint = torch.load(chkpt_dir, map_location='cpu')\n",
    "    msg = model.load_state_dict(checkpoint['model'], strict=False)\n",
    "    print(msg)\n",
    "    return model\n",
    "\n",
    "def run_one_image(img, model):\n",
    "    x = torch.tensor(img)\n",
    "\n",
    "    # make it a batch-like\n",
    "    x = x.unsqueeze(dim=0)\n",
    "    x = torch.einsum('nhwc->nchw', x)\n",
    "\n",
    "    # run MAE\n",
    "    loss, y, mask = model(x.float(), mask_ratio=0.75)\n",
    "    y = model.unpatchify(y)\n",
    "    y = torch.einsum('nchw->nhwc', y).detach().cpu()\n",
    "\n",
    "    # visualize the mask\n",
    "    mask = mask.detach()\n",
    "    mask = mask.unsqueeze(-1).repeat(1, 1, model.patch_embed.patch_size[0]**2 *3)  # (N, H*W, p*p*3)\n",
    "    mask = model.unpatchify(mask)  # 1 is removing, 0 is keeping\n",
    "    mask = torch.einsum('nchw->nhwc', mask).detach().cpu()\n",
    "\n",
    "    x = torch.einsum('nchw->nhwc', x)\n",
    "\n",
    "    # masked image\n",
    "    im_masked = x * (1 - mask)\n",
    "\n",
    "    # MAE reconstruction pasted with visible patches\n",
    "    im_paste = x * (1 - mask) + y * mask\n",
    "\n",
    "    # make the plt figure larger\n",
    "    plt.rcParams['figure.figsize'] = [24, 24]\n",
    "\n",
    "    plt.subplot(1, 4, 1)\n",
    "    show_image(x[0], \"original\")\n",
    "\n",
    "    plt.subplot(1, 4, 2)\n",
    "    show_image(im_masked[0], \"masked\")\n",
    "\n",
    "    plt.subplot(1, 4, 3)\n",
    "    show_image(y[0], \"reconstruction\")\n",
    "\n",
    "    plt.subplot(1, 4, 4)\n",
    "    show_image(im_paste[0], \"reconstruction + visible\")\n",
    "\n",
    "    plt.show()\n",
    "# Set the seed for PyTorch\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f503539d-e62d-446e-86a1-e5f43a7e84e2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Parametrize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ac00b4c-52e2-4b86-a7d3-0a2734bc76c8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not using distributed mode\n",
      "[21:27:01.819611] Namespace(batch_size=32,\n",
      "epochs=100,\n",
      "accum_iter=4,\n",
      "model='vit_base_patch16',\n",
      "input_size=224,\n",
      "drop_path=0.1,\n",
      "clip_grad=None,\n",
      "weight_decay=0.05,\n",
      "lr=None,\n",
      "blr=0.0005,\n",
      "layer_decay=0.65,\n",
      "min_lr=1e-06,\n",
      "warmup_epochs=5,\n",
      "color_jitter=None,\n",
      "aa='rand-m9-mstd0.5-inc1',\n",
      "smoothing=0.1,\n",
      "reprob=0.25,\n",
      "remode='pixel',\n",
      "recount=1,\n",
      "resplit=False,\n",
      "mixup=0.8,\n",
      "cutmix=1.0,\n",
      "cutmix_minmax=None,\n",
      "mixup_prob=1.0,\n",
      "mixup_switch_prob=0.5,\n",
      "mixup_mode='batch',\n",
      "finetune='mae_pretrain_vit_base.pth',\n",
      "global_pool=True,\n",
      "data_path='/media/enc/vera1/sebastian/data/Datos_Entrenamiento/',\n",
      "nb_classes=7,\n",
      "output_dir='EXP_base_biggest_dataset',\n",
      "log_dir='./output_dir',\n",
      "device='cuda',\n",
      "seed=0,\n",
      "resume='',\n",
      "start_epoch=0,\n",
      "eval=True,\n",
      "dist_eval=False,\n",
      "num_workers=10,\n",
      "pin_mem=True,\n",
      "world_size=1,\n",
      "local_rank=-1,\n",
      "dist_on_itp=False,\n",
      "dist_url='env://',\n",
      "distributed=False)\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser('MAE fine-tuning for image classification', add_help=False)\n",
    "parser.add_argument('--batch_size', default=32, type=int,\n",
    "                        help='Batch size per GPU (effective batch size is batch_size * accum_iter * # gpus')\n",
    "parser.add_argument('--epochs', default=100, type=int)\n",
    "parser.add_argument('--accum_iter', default=4, type=int,\n",
    "                        help='Accumulate gradient iterations (for increasing the effective batch size under memory constraints)')\n",
    "\n",
    "    # Model parameters\n",
    "parser.add_argument('--model', default='vit_base_patch16', type=str, metavar='MODEL',\n",
    "                        help='Name of model to train')\n",
    "\n",
    "parser.add_argument('--input_size', default=224, type=int,\n",
    "                        help='images input size')\n",
    "\n",
    "parser.add_argument('--drop_path', type=float, default=0.1, metavar='PCT',\n",
    "                        help='Drop path rate (default: 0.1)')\n",
    "\n",
    "    # Optimizer parameters\n",
    "parser.add_argument('--clip_grad', type=float, default=None, metavar='NORM',\n",
    "                        help='Clip gradient norm (default: None, no clipping)')\n",
    "parser.add_argument('--weight_decay', type=float, default=0.05,\n",
    "                        help='weight decay (default: 0.05)')\n",
    "\n",
    "parser.add_argument('--lr', type=float, default=None, metavar='LR',\n",
    "                        help='learning rate (absolute lr)')\n",
    "parser.add_argument('--blr', type=float, default=5e-4, metavar='LR',\n",
    "                        help='base learning rate: absolute_lr = base_lr * total_batch_size / 256')\n",
    "parser.add_argument('--layer_decay', type=float, default=0.65,\n",
    "                        help='layer-wise lr decay from ELECTRA/BEiT')\n",
    "\n",
    "parser.add_argument('--min_lr', type=float, default=1e-6, metavar='LR',\n",
    "                        help='lower lr bound for cyclic schedulers that hit 0')\n",
    "\n",
    "parser.add_argument('--warmup_epochs', type=int, default=5, metavar='N',\n",
    "                        help='epochs to warmup LR')\n",
    "\n",
    "    # Augmentation parameters\n",
    "parser.add_argument('--color_jitter', type=float, default=None, metavar='PCT',\n",
    "                        help='Color jitter factor (enabled only when not using Auto/RandAug)')\n",
    "parser.add_argument('--aa', type=str, default='rand-m9-mstd0.5-inc1', metavar='NAME',\n",
    "                        help='Use AutoAugment policy. \"v0\" or \"original\". \" + \"(default: rand-m9-mstd0.5-inc1)'),\n",
    "parser.add_argument('--smoothing', type=float, default=0.1,\n",
    "                        help='Label smoothing (default: 0.1)')\n",
    "\n",
    "    # * Random Erase params\n",
    "parser.add_argument('--reprob', type=float, default=0.25, metavar='PCT',\n",
    "                        help='Random erase prob (default: 0.25)')\n",
    "parser.add_argument('--remode', type=str, default='pixel',\n",
    "                        help='Random erase mode (default: \"pixel\")')\n",
    "parser.add_argument('--recount', type=int, default=1,\n",
    "                        help='Random erase count (default: 1)')\n",
    "parser.add_argument('--resplit', action='store_true', default=False,\n",
    "                        help='Do not random erase first (clean) augmentation split')\n",
    "    # * Mixup params\n",
    "parser.add_argument('--mixup', type=float, default=0.8,\n",
    "                        help='mixup alpha, mixup enabled if > 0.')\n",
    "parser.add_argument('--cutmix', type=float, default=1.0,\n",
    "                        help='cutmix alpha, cutmix enabled if > 0.')\n",
    "parser.add_argument('--cutmix_minmax', type=float, nargs='+', default=None,\n",
    "                        help='cutmix min/max ratio, overrides alpha and enables cutmix if set (default: None)')\n",
    "parser.add_argument('--mixup_prob', type=float, default=1.0,\n",
    "                        help='Probability of performing mixup or cutmix when either/both is enabled')\n",
    "parser.add_argument('--mixup_switch_prob', type=float, default=0.5,\n",
    "                        help='Probability of switching to cutmix when both mixup and cutmix enabled')\n",
    "\n",
    "parser.add_argument('--mixup_mode', type=str, default='batch',\n",
    "                        help='How to apply mixup/cutmix params. Per \"batch\", \"pair\", or \"elem\"')\n",
    "\n",
    "    # * Finetuning params\n",
    "parser.add_argument('--finetune', default='mae_pretrain_vit_base.pth',\n",
    "                        help='finetune from checkpoint')\n",
    "parser.add_argument('--global_pool', action='store_true')\n",
    "parser.set_defaults(global_pool=True)\n",
    "parser.add_argument('--cls_token', action='store_false', dest='global_pool',\n",
    "                        help='Use class token instead of global pool for classification')\n",
    "\n",
    "    # Dataset parameters\n",
    "parser.add_argument('--data_path', default='/media/enc/vera1/sebastian/data/Datos_Entrenamiento/', type=str,\n",
    "                        help='dataset path')\n",
    "parser.add_argument('--nb_classes', default=7, type=int,\n",
    "                        help='number of the classification types')\n",
    "\n",
    "parser.add_argument('--output_dir', default='EXP_base_biggest_dataset',\n",
    "                        help='path where to save, empty for no saving')\n",
    "parser.add_argument('--log_dir', default='./output_dir',\n",
    "                        help='path where to tensorboard log')\n",
    "parser.add_argument('--device', default='cuda',\n",
    "                        help='device to use for training / testing')\n",
    "parser.add_argument('--seed', default=0, type=int)\n",
    "parser.add_argument('--resume', default='',\n",
    "                        help='resume from checkpoint')\n",
    "\n",
    "parser.add_argument('--start_epoch', default=0, type=int, metavar='N',\n",
    "                        help='start epoch')\n",
    "parser.add_argument('--eval',default=True, action='store_true',\n",
    "                        help='Perform evaluation only')\n",
    "parser.add_argument('--dist_eval', action='store_true', default=False,\n",
    "                        help='Enabling distributed evaluation (recommended during training for faster monitor')\n",
    "parser.add_argument('--num_workers', default=10, type=int)\n",
    "parser.add_argument('--pin_mem', action='store_true',\n",
    "                        help='Pin CPU memory in DataLoader for more efficient (sometimes) transfer to GPU.')\n",
    "parser.add_argument('--no_pin_mem', action='store_false', dest='pin_mem')\n",
    "parser.set_defaults(pin_mem=True)\n",
    "\n",
    "    # distributed training parameters\n",
    "parser.add_argument('--world_size', default=1, type=int,\n",
    "                        help='number of distributed processes')\n",
    "parser.add_argument('--local_rank', default=-1, type=int)\n",
    "parser.add_argument('--dist_on_itp', action='store_true')\n",
    "parser.add_argument('--dist_url', default='env://',\n",
    "                        help='url used to set up distributed training')\n",
    "args, unknown = parser.parse_known_args()\n",
    "misc.init_distributed_mode(args)\n",
    "print(\"{}\".format(args).replace(', ', ',\\n'))\n",
    "\n",
    "device = torch.device(args.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e52885-7e00-4ebd-8f8e-a21c2afb0011",
   "metadata": {},
   "source": [
    "since pytorch > 1.8, then have to update this i ~/anaconda3/lib/python3.11/site-packages/timm/models/layers/helpers.py with https://github.com/huggingface/pytorch-image-models/issues/420 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efcb2899-533c-4472-b263-b94f4d90f256",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'misc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m misc\u001b[38;5;241m.\u001b[39minit_distributed_mode(args)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# print('job dir: {}'.format(os.path.dirname(os.path.realpath(__file__))))\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(args)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'misc' is not defined"
     ]
    }
   ],
   "source": [
    "    misc.init_distributed_mode(args)\n",
    "\n",
    "    # print('job dir: {}'.format(os.path.dirname(os.path.realpath(__file__))))\n",
    "    print(\"{}\".format(args).replace(', ', ',\\n'))\n",
    "\n",
    "    device = torch.device(args.device)\n",
    "\n",
    "    # fix the seed for reproducibility\n",
    "    seed = args.seed + misc.get_rank()\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "    dataset_train = build_dataset(is_train=True, args=args)\n",
    "    dataset_val = build_dataset(is_train=False, args=args)\n",
    "\n",
    "    if True:  # args.distributed:\n",
    "        num_tasks = misc.get_world_size()\n",
    "        global_rank = misc.get_rank()\n",
    "        sampler_train = torch.utils.data.DistributedSampler(\n",
    "            dataset_train, num_replicas=num_tasks, rank=global_rank, shuffle=True\n",
    "        )\n",
    "        print(\"Sampler_train = %s\" % str(sampler_train))\n",
    "        if args.dist_eval:\n",
    "            if len(dataset_val) % num_tasks != 0:\n",
    "                print('Warning: Enabling distributed evaluation with an eval dataset not divisible by process number. '\n",
    "                      'This will slightly alter validation results as extra duplicate entries are added to achieve '\n",
    "                      'equal num of samples per-process.')\n",
    "            sampler_val = torch.utils.data.DistributedSampler(\n",
    "                dataset_val, num_replicas=num_tasks, rank=global_rank, shuffle=True)  # shuffle=True to reduce monitor bias\n",
    "        else:\n",
    "            sampler_val = torch.utils.data.SequentialSampler(dataset_val)\n",
    "    else:\n",
    "        sampler_train = torch.utils.data.RandomSampler(dataset_train)\n",
    "        sampler_val = torch.utils.data.SequentialSampler(dataset_val)\n",
    "\n",
    "    if global_rank == 0 and args.log_dir is not None and not args.eval:\n",
    "        os.makedirs(args.log_dir, exist_ok=True)\n",
    "        log_writer = SummaryWriter(log_dir=args.log_dir)\n",
    "    else:\n",
    "        log_writer = None\n",
    "\n",
    "    data_loader_train = torch.utils.data.DataLoader(\n",
    "        dataset_train, sampler=sampler_train,\n",
    "        batch_size=args.batch_size,\n",
    "        num_workers=args.num_workers,\n",
    "        pin_memory=args.pin_mem,\n",
    "        drop_last=True,\n",
    "    )\n",
    "\n",
    "    data_loader_val = torch.utils.data.DataLoader(\n",
    "        dataset_val, sampler=sampler_val,\n",
    "        batch_size=args.batch_size,\n",
    "        num_workers=args.num_workers,\n",
    "        pin_memory=args.pin_mem,\n",
    "        drop_last=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eca098d5-a1f3-4bf9-9bb0-d4c8a66cb083",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# #Liberia Validacion cruzada\n",
    "# from sklearn.model_selection import KFold\n",
    "# # K-fold cross-validation\n",
    "# num_splits = 5  # ajusta el número de splits según tus necesidades\n",
    "# kf = KFold(n_splits=num_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# for fold, (train_index, val_index) in enumerate(kf.split(dataset_train)):\n",
    "#     print(f\"\\nTraining on Fold {fold + 1}/{num_splits}:\")\n",
    "\n",
    "#     # create train and validation sets for this fold\n",
    "#     train_dataset_fold = Subset(dataset_train, train_index)\n",
    "#     val_dataset_fold = Subset(dataset_train, val_index)\n",
    "\n",
    "#     data_loader_train = torch.utils.data.DataLoader(\n",
    "#         train_dataset_fold, sampler=torch.utils.data.RandomSampler(train_dataset_fold),\n",
    "#         batch_size=args.batch_size,\n",
    "#         num_workers=args.num_workers,\n",
    "#         pin_memory=args.pin_mem,\n",
    "#         drop_last=True,\n",
    "#     )\n",
    "\n",
    "#     data_loader_val = torch.utils.data.DataLoader(\n",
    "#         val_dataset_fold, sampler=torch.utils.data.SequentialSampler(val_dataset_fold),\n",
    "#         batch_size=args.batch_size,\n",
    "#         num_workers=args.num_workers,\n",
    "#         pin_memory=args.pin_mem,\n",
    "#         drop_last=False\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2cd05d05-9434-4c06-a03a-801f4f2a59ea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:32:50.338500] [20:32:50.338494] [20:32:50.338521] Mixup is activated!\n",
      "[20:32:51.204577] [20:32:51.204566] [20:32:51.204632] Model = VisionTransformer(\n",
      "  (patch_embed): PatchEmbed(\n",
      "    (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
      "  )\n",
      "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "  (blocks): ModuleList(\n",
      "    (0): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (1-11): 11 x Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): DropPath()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (head): Linear(in_features=768, out_features=7, bias=True)\n",
      "  (fc_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      ")\n",
      "[20:32:51.204678] [20:32:51.204675] [20:32:51.204685] number of params (M): 85.80\n",
      "[20:32:51.204773] [20:32:51.204770] [20:32:51.204781] base lr: 5.00e-04\n",
      "[20:32:51.204806] [20:32:51.204803] [20:32:51.204812] actual lr: 2.50e-04\n",
      "[20:32:51.204836] [20:32:51.204834] [20:32:51.204843] accumulate grad iterations: 4\n",
      "[20:32:51.204866] [20:32:51.204863] [20:32:51.204872] effective batch size: 128\n",
      "[20:32:51.205812] [20:32:51.205807] [20:32:51.205821] criterion = SoftTargetCrossEntropy()\n",
      "[20:32:53.559176] [20:32:53.559154] [20:32:53.559297] Test:  [ 0/68]  eta: 0:02:39  loss: 2.2849 (2.2849)  acc1: 0.0000 (0.0000)  acc5: 62.5000 (62.5000)  time: 2.3524  data: 2.0509  max mem: 684\n",
      "[20:32:54.843348] [20:32:54.843336] [20:32:54.843401] Test:  [10/68]  eta: 0:00:19  loss: 2.2875 (2.3057)  acc1: 0.0000 (0.5682)  acc5: 53.1250 (55.1136)  time: 0.3305  data: 0.2883  max mem: 684\n",
      "[20:32:56.562391] [20:32:56.562379] [20:32:56.562458] Test:  [20/68]  eta: 0:00:12  loss: 2.2690 (2.2414)  acc1: 0.0000 (3.5714)  acc5: 53.1250 (54.1667)  time: 0.1501  data: 0.1340  max mem: 684\n",
      "[20:32:58.262496] [20:32:58.262485] [20:32:58.262584] Test:  [30/68]  eta: 0:00:08  loss: 2.1233 (2.1885)  acc1: 3.1250 (5.1411)  acc5: 56.2500 (59.5766)  time: 0.1709  data: 0.1551  max mem: 684\n",
      "[20:32:59.652175] [20:32:59.652165] [20:32:59.652227] Test:  [40/68]  eta: 0:00:05  loss: 1.7619 (2.0682)  acc1: 6.2500 (7.6220)  acc5: 100.0000 (68.8262)  time: 0.1544  data: 0.1386  max mem: 684\n",
      "[20:33:01.350967] [20:33:01.350956] [20:33:01.351020] Test:  [50/68]  eta: 0:00:03  loss: 1.6655 (1.9857)  acc1: 28.1250 (15.0735)  acc5: 100.0000 (74.9387)  time: 0.1544  data: 0.1385  max mem: 684\n",
      "[20:33:03.075717] [20:33:03.075705] [20:33:03.075775] Test:  [60/68]  eta: 0:00:01  loss: 1.7162 (2.1143)  acc1: 12.5000 (13.7807)  acc5: 100.0000 (65.0615)  time: 0.1711  data: 0.1553  max mem: 684\n",
      "[20:33:03.577862] [20:33:03.577852] [20:33:03.577910] Test:  [67/68]  eta: 0:00:00  loss: 2.9356 (2.2245)  acc1: 0.0000 (12.4135)  acc5: 0.0000 (58.7448)  time: 0.1728  data: 0.1565  max mem: 684\n",
      "[20:33:03.637412] [20:33:03.637406] [20:33:03.637426] Test: Total time: 0:00:12 (0.1828 s / it)\n",
      "[20:33:03.637459] [20:33:03.637457] [20:33:03.637465] * Acc@1 12.413 Acc@5 58.745 loss 2.224\n",
      "[20:33:03.637677] [20:33:03.637672] [20:33:03.637685] Accuracy of the network on the 2167 test images: 12.4%\n"
     ]
    }
   ],
   "source": [
    "    mixup_fn = None\n",
    "    mixup_active = args.mixup > 0 or args.cutmix > 0. or args.cutmix_minmax is not None\n",
    "    if mixup_active:\n",
    "        print(\"Mixup is activated!\")\n",
    "        mixup_fn = Mixup(\n",
    "            mixup_alpha=args.mixup, cutmix_alpha=args.cutmix, cutmix_minmax=args.cutmix_minmax,\n",
    "            prob=args.mixup_prob, switch_prob=args.mixup_switch_prob, mode=args.mixup_mode,\n",
    "            label_smoothing=args.smoothing, num_classes=args.nb_classes)\n",
    "    \n",
    "    model = models_vit.__dict__[args.model](\n",
    "        num_classes=args.nb_classes,\n",
    "        drop_path_rate=args.drop_path,\n",
    "        global_pool=args.global_pool,\n",
    "    )\n",
    "\n",
    "    if args.finetune and not args.eval:\n",
    "        checkpoint = torch.load(args.finetune, map_location='cpu')\n",
    "\n",
    "        print(\"Load pre-trained checkpoint from: %s\" % args.finetune)\n",
    "        checkpoint_model = checkpoint['model']\n",
    "        state_dict = model.state_dict()\n",
    "        for k in ['head.weight', 'head.bias']:\n",
    "            if k in checkpoint_model and checkpoint_model[k].shape != state_dict[k].shape:\n",
    "                print(f\"Removing key {k} from pretrained checkpoint\")\n",
    "                del checkpoint_model[k]\n",
    "\n",
    "        # interpolate position embedding\n",
    "        interpolate_pos_embed(model, checkpoint_model)\n",
    "\n",
    "        # load pre-trained model\n",
    "        msg = model.load_state_dict(checkpoint_model, strict=False)\n",
    "        print(msg)\n",
    "\n",
    "        if args.global_pool:\n",
    "            assert set(msg.missing_keys) == {'head.weight', 'head.bias', 'fc_norm.weight', 'fc_norm.bias'}\n",
    "        else:\n",
    "            assert set(msg.missing_keys) == {'head.weight', 'head.bias'}\n",
    "\n",
    "        # manually initialize fc layer\n",
    "        trunc_normal_(model.head.weight, std=2e-5)\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    model_without_ddp = model\n",
    "    n_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "    print(\"Model = %s\" % str(model_without_ddp))\n",
    "    print('number of params (M): %.2f' % (n_parameters / 1.e6))\n",
    "\n",
    "    eff_batch_size = args.batch_size * args.accum_iter * misc.get_world_size()\n",
    "    \n",
    "    if args.lr is None:  # only base_lr is specified\n",
    "        args.lr = args.blr * eff_batch_size / 256\n",
    "\n",
    "    print(\"base lr: %.2e\" % (args.lr * 256 / eff_batch_size))\n",
    "    print(\"actual lr: %.2e\" % args.lr)\n",
    "\n",
    "    print(\"accumulate grad iterations: %d\" % args.accum_iter)\n",
    "    print(\"effective batch size: %d\" % eff_batch_size)\n",
    "\n",
    "    if args.distributed:\n",
    "        model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.gpu])\n",
    "        model_without_ddp = model.module\n",
    "\n",
    "    # build optimizer with layer-wise lr decay (lrd)\n",
    "    param_groups = lrd.param_groups_lrd(model_without_ddp, args.weight_decay,\n",
    "        no_weight_decay_list=model_without_ddp.no_weight_decay(),\n",
    "        layer_decay=args.layer_decay\n",
    "    )\n",
    "    optimizer = torch.optim.AdamW(param_groups, lr=args.lr)\n",
    "    loss_scaler = NativeScaler()\n",
    "\n",
    "    if mixup_fn is not None:\n",
    "        # smoothing is handled with mixup label transform\n",
    "        criterion = SoftTargetCrossEntropy()\n",
    "    elif args.smoothing > 0.:\n",
    "        criterion = LabelSmoothingCrossEntropy(smoothing=args.smoothing)\n",
    "    else:\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    print(\"criterion = %s\" % str(criterion))\n",
    "\n",
    "    misc.load_model(args=args, model_without_ddp=model_without_ddp, optimizer=optimizer, loss_scaler=loss_scaler)\n",
    "\n",
    "    if args.eval:\n",
    "        test_stats = evaluate(data_loader_val, model, device)\n",
    "        print(f\"Accuracy of the network on the {len(dataset_val)} test images: {test_stats['acc1']:.1f}%\")\n",
    "        # exit(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aee713c-2882-48de-945a-c11123262717",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Train model\n",
    "Run a conda environment or alternatively just run the cell below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980c8d1d-2082-4ef6-b434-4a971277ec72",
   "metadata": {},
   "source": [
    "### Base model\n",
    "Finetuning using the base vit pretrained model  which is downloaded from here (https://github.com/facebookresearch/mae?tab=readme-ov-file#fine-tuning-with-pre-trained-checkpoints). The way to find the correct finetunng is explaned here (https://github.com/facebookresearch/mae/issues?q=is%3Aissue+is%3Aopen+classes )\n",
    "\n",
    "`python main_finetune.py --accum_iter 4 --batch_size 32 --model vit_base_patch16 --finetune mae_pretrain_vit_base.pth --epochs 100 --blr 5e-4 --layer_decay 0.65 --weight_decay 0.05 --drop_path 0.1 --mixup 0.8 --cutmix 1.0 --reprob 0.25 --dist_eval --data_path /media/enc/vera1/sebastian/data/Data-set-Urban_Esc/ --nb_classes 7`\n",
    "\n",
    "Expected results: \n",
    "\n",
    "```\n",
    "[04:05:12.377034] * Acc@1 83.388 Acc@5 99.836 loss 0.572\n",
    "[04:05:12.377152] Accuracy of the network on the 608 test images: 83.4%\n",
    "[04:05:12.377165] Max accuracy: 83.55%\n",
    "[04:05:12.378265] Training time 0:46:57\n",
    "```\n",
    "\n",
    "### Large model\n",
    "\n",
    "`python main_finetune.py --accum_iter 4 --batch_size 16 --model vit_large_patch16 --finetune mae_pretrain_vit_large.pth --epochs 100 --blr 5e-4 --layer_decay 0.65 --weight_decay 0.05 --drop_path 0.1 --mixup 0.8 --cutmix 1.0 --reprob 0.25 --dist_eval --data_path /media/enc/vera1/sebastian/data/Data-set-Urban_Esc/ --nb_classes 7 --output_dir EXP_large_vit`\n",
    "\n",
    "Expected results: \n",
    "\n",
    "```\n",
    "[06:10:18.181183] Test: Total time: 0:00:04 (0.1237 s / it)\n",
    "[06:10:18.181592] * Acc@1 83.059 Acc@5 99.671 loss 0.586\n",
    "[06:10:18.181741] Accuracy of the network on the 608 test images: 83.1%\n",
    "[06:10:18.181759] Max accuracy: 83.06%\n",
    "[06:10:18.182753] Training time 1:18:19\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfd5d05-fb33-4759-aa29-37debddc0e3e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:33:03.644881] [20:33:03.644876] [20:33:03.644903] Start training for 100 epochs\n",
      "[20:33:05.354104] [20:33:05.354073] [20:33:05.354251] Epoch: [0]  [  0/316]  eta: 0:08:59  lr: 0.000000  loss: 1.9755 (1.9755)  time: 1.7081  data: 1.5360  max mem: 3670\n",
      "[20:33:07.668131] [20:33:07.668119] [20:33:07.668185] Epoch: [0]  [ 20/316]  eta: 0:00:56  lr: 0.000003  loss: 2.0923 (2.0974)  time: 0.1157  data: 0.0622  max mem: 4666\n",
      "[20:33:10.114647] [20:33:10.114637] [20:33:10.114773] Epoch: [0]  [ 40/316]  eta: 0:00:43  lr: 0.000006  loss: 2.0099 (2.0582)  time: 0.1223  data: 0.0709  max mem: 4666\n",
      "[20:33:12.575149] [20:33:12.575138] [20:33:12.575201] Epoch: [0]  [ 60/316]  eta: 0:00:37  lr: 0.000009  loss: 1.9531 (2.0280)  time: 0.1230  data: 0.0715  max mem: 4666\n",
      "[20:33:15.052563] [20:33:15.052551] [20:33:15.052621] Epoch: [0]  [ 80/316]  eta: 0:00:33  lr: 0.000013  loss: 1.8692 (1.9887)  time: 0.1238  data: 0.0724  max mem: 4666\n",
      "[20:33:17.561571] [20:33:17.561561] [20:33:17.561633] Epoch: [0]  [100/316]  eta: 0:00:29  lr: 0.000016  loss: 1.8482 (1.9608)  time: 0.1254  data: 0.0739  max mem: 4666\n",
      "[20:33:20.044406] [20:33:20.044395] [20:33:20.044465] Epoch: [0]  [120/316]  eta: 0:00:26  lr: 0.000019  loss: 1.8546 (1.9473)  time: 0.1241  data: 0.0727  max mem: 4666\n",
      "[20:33:22.501868] [20:33:22.501857] [20:33:22.501936] Epoch: [0]  [140/316]  eta: 0:00:23  lr: 0.000022  loss: 1.8532 (1.9343)  time: 0.1228  data: 0.0714  max mem: 4666\n",
      "[20:33:24.948411] [20:33:24.948401] [20:33:24.948475] Epoch: [0]  [160/316]  eta: 0:00:20  lr: 0.000025  loss: 1.7961 (1.9182)  time: 0.1223  data: 0.0709  max mem: 4666\n",
      "[20:33:27.356125] [20:33:27.356115] [20:33:27.356176] Epoch: [0]  [180/316]  eta: 0:00:17  lr: 0.000028  loss: 1.7721 (1.9074)  time: 0.1203  data: 0.0689  max mem: 4666\n",
      "[20:33:29.806358] [20:33:29.806347] [20:33:29.806507] Epoch: [0]  [200/316]  eta: 0:00:15  lr: 0.000032  loss: 1.8286 (1.8990)  time: 0.1225  data: 0.0710  max mem: 4666\n",
      "[20:33:32.290769] [20:33:32.290759] [20:33:32.290819] Epoch: [0]  [220/316]  eta: 0:00:12  lr: 0.000035  loss: 1.7987 (1.8895)  time: 0.1242  data: 0.0727  max mem: 4666\n",
      "[20:33:34.737229] [20:33:34.737219] [20:33:34.737283] Epoch: [0]  [240/316]  eta: 0:00:09  lr: 0.000038  loss: 1.8265 (1.8839)  time: 0.1223  data: 0.0709  max mem: 4666\n",
      "[20:33:37.202379] [20:33:37.202368] [20:33:37.202437] Epoch: [0]  [260/316]  eta: 0:00:07  lr: 0.000041  loss: 1.7995 (1.8775)  time: 0.1232  data: 0.0718  max mem: 4666\n",
      "[20:33:39.723360] [20:33:39.723348] [20:33:39.723430] Epoch: [0]  [280/316]  eta: 0:00:04  lr: 0.000044  loss: 1.7724 (1.8719)  time: 0.1260  data: 0.0746  max mem: 4666\n",
      "[20:33:42.208885] [20:33:42.208874] [20:33:42.208941] Epoch: [0]  [300/316]  eta: 0:00:02  lr: 0.000047  loss: 1.7625 (1.8651)  time: 0.1242  data: 0.0729  max mem: 4666\n",
      "[20:33:43.666165] [20:33:43.666156] [20:33:43.666209] Epoch: [0]  [315/316]  eta: 0:00:00  lr: 0.000049  loss: 1.7612 (1.8633)  time: 0.1220  data: 0.0707  max mem: 4666\n",
      "[20:33:43.735148] [20:33:43.735140] [20:33:43.735166] Epoch: [0] Total time: 0:00:40 (0.1269 s / it)\n",
      "[20:33:43.735225] [20:33:43.735223] [20:33:43.735231] Averaged stats: lr: 0.000049  loss: 1.7612 (1.8633)\n",
      "[20:33:47.144381] [20:33:47.144350] [20:33:47.144590] Test:  [ 0/68]  eta: 0:01:54  loss: 1.4956 (1.4956)  acc1: 68.7500 (68.7500)  acc5: 100.0000 (100.0000)  time: 1.6851  data: 1.6677  max mem: 4666\n",
      "[20:33:48.523745] [20:33:48.523734] [20:33:48.524037] Test:  [10/68]  eta: 0:00:16  loss: 1.5733 (1.3607)  acc1: 62.5000 (62.5000)  acc5: 100.0000 (100.0000)  time: 0.2785  data: 0.2619  max mem: 4666\n",
      "[20:33:49.913475] [20:33:49.913464] [20:33:49.913569] Test:  [20/68]  eta: 0:00:10  loss: 1.0797 (1.1692)  acc1: 71.8750 (70.9821)  acc5: 100.0000 (100.0000)  time: 0.1383  data: 0.1221  max mem: 4666\n",
      "[20:33:51.328388] [20:33:51.328377] [20:33:51.328505] Test:  [30/68]  eta: 0:00:07  loss: 1.0797 (1.4497)  acc1: 68.7500 (56.3508)  acc5: 100.0000 (91.1290)  time: 0.1401  data: 0.1241  max mem: 4666\n",
      "[20:33:52.714456] [20:33:52.714444] [20:33:52.714511] Test:  [40/68]  eta: 0:00:04  loss: 2.0678 (1.6300)  acc1: 0.0000 (45.7317)  acc5: 81.2500 (83.3841)  time: 0.1400  data: 0.1239  max mem: 4666\n",
      "[20:33:54.104715] [20:33:54.104706] [20:33:54.104766] Test:  [50/68]  eta: 0:00:03  loss: 1.6292 (1.5967)  acc1: 28.1250 (48.6520)  acc5: 100.0000 (86.6422)  time: 0.1387  data: 0.1228  max mem: 4666\n",
      "[20:33:55.482817] [20:33:55.482807] [20:33:55.482871] Test:  [60/68]  eta: 0:00:01  loss: 1.6292 (1.6178)  acc1: 12.5000 (42.6230)  acc5: 100.0000 (88.8320)  time: 0.1383  data: 0.1224  max mem: 4666\n",
      "[20:33:55.621775] [20:33:55.621770] [20:33:55.621787] Test:  [67/68]  eta: 0:00:00  loss: 1.6004 (1.6178)  acc1: 6.2500 (38.9940)  acc5: 100.0000 (89.9400)  time: 0.1385  data: 0.1228  max mem: 4666\n",
      "[20:33:55.685933] [20:33:55.685925] [20:33:55.685959] Test: Total time: 0:00:10 (0.1504 s / it)\n",
      "[20:33:55.686000] [20:33:55.685998] [20:33:55.686007] * Acc@1 38.994 Acc@5 89.940 loss 1.618\n",
      "[20:33:55.686182] [20:33:55.686177] [20:33:55.686190] Accuracy of the network on the 2167 test images: 39.0%\n",
      "[20:33:55.686200] [20:33:55.686198] [20:33:55.686205] Max accuracy: 38.99%\n",
      "[20:33:57.218660] [20:33:57.218640] [20:33:57.218847] Epoch: [1]  [  0/316]  eta: 0:08:03  lr: 0.000050  loss: 1.8378 (1.8378)  time: 1.5311  data: 1.4801  max mem: 4666\n",
      "[20:33:59.702774] [20:33:59.702762] [20:33:59.702842] Epoch: [1]  [ 20/316]  eta: 0:00:56  lr: 0.000053  loss: 1.7603 (1.8050)  time: 0.1242  data: 0.0727  max mem: 4666\n",
      "[20:34:02.184164] [20:34:02.184152] [20:34:02.184214] Epoch: [1]  [ 40/316]  eta: 0:00:43  lr: 0.000056  loss: 1.8143 (1.8138)  time: 0.1240  data: 0.0726  max mem: 4666\n",
      "[20:34:04.703627] [20:34:04.703617] [20:34:04.703684] Epoch: [1]  [ 60/316]  eta: 0:00:37  lr: 0.000059  loss: 1.8139 (1.8130)  time: 0.1259  data: 0.0746  max mem: 4666\n",
      "[20:34:07.191849] [20:34:07.191839] [20:34:07.191907] Epoch: [1]  [ 80/316]  eta: 0:00:33  lr: 0.000063  loss: 1.7975 (1.8085)  time: 0.1244  data: 0.0730  max mem: 4666\n",
      "[20:34:09.698879] [20:34:09.698868] [20:34:09.698932] Epoch: [1]  [100/316]  eta: 0:00:29  lr: 0.000066  loss: 1.7845 (1.8092)  time: 0.1253  data: 0.0739  max mem: 4666\n",
      "[20:34:12.237618] [20:34:12.237607] [20:34:12.237674] Epoch: [1]  [120/316]  eta: 0:00:26  lr: 0.000069  loss: 1.8428 (1.8135)  time: 0.1269  data: 0.0754  max mem: 4666\n",
      "[20:34:14.668436] [20:34:14.668426] [20:34:14.668491] Epoch: [1]  [140/316]  eta: 0:00:23  lr: 0.000072  loss: 1.8050 (1.8158)  time: 0.1215  data: 0.0699  max mem: 4666\n",
      "[20:34:17.171091] [20:34:17.171081] [20:34:17.171146] Epoch: [1]  [160/316]  eta: 0:00:20  lr: 0.000075  loss: 1.7554 (1.8106)  time: 0.1251  data: 0.0735  max mem: 4666\n",
      "[20:34:19.669829] [20:34:19.669818] [20:34:19.669886] Epoch: [1]  [180/316]  eta: 0:00:18  lr: 0.000078  loss: 1.8213 (1.8126)  time: 0.1249  data: 0.0733  max mem: 4666\n",
      "[20:34:22.134265] [20:34:22.134255] [20:34:22.134322] Epoch: [1]  [200/316]  eta: 0:00:15  lr: 0.000082  loss: 1.8162 (1.8135)  time: 0.1232  data: 0.0716  max mem: 4666\n",
      "[20:34:24.605760] [20:34:24.605749] [20:34:24.605817] Epoch: [1]  [220/316]  eta: 0:00:12  lr: 0.000085  loss: 1.7794 (1.8114)  time: 0.1235  data: 0.0719  max mem: 4666\n",
      "[20:34:27.059630] [20:34:27.059618] [20:34:27.059684] Epoch: [1]  [240/316]  eta: 0:00:09  lr: 0.000088  loss: 1.8021 (1.8121)  time: 0.1226  data: 0.0711  max mem: 4666\n",
      "[20:34:29.523057] [20:34:29.523046] [20:34:29.523110] Epoch: [1]  [260/316]  eta: 0:00:07  lr: 0.000091  loss: 1.8036 (1.8132)  time: 0.1231  data: 0.0716  max mem: 4666\n",
      "[20:34:32.002624] [20:34:32.002611] [20:34:32.002680] Epoch: [1]  [280/316]  eta: 0:00:04  lr: 0.000094  loss: 1.7880 (1.8124)  time: 0.1239  data: 0.0724  max mem: 4666\n",
      "[20:34:34.492988] [20:34:34.492978] [20:34:34.493046] Epoch: [1]  [300/316]  eta: 0:00:02  lr: 0.000097  loss: 1.7854 (1.8096)  time: 0.1245  data: 0.0729  max mem: 4666\n",
      "[20:34:35.944674] [20:34:35.944664] [20:34:35.944720] Epoch: [1]  [315/316]  eta: 0:00:00  lr: 0.000099  loss: 1.7824 (1.8091)  time: 0.1214  data: 0.0701  max mem: 4666\n",
      "[20:34:36.017387] [20:34:36.017375] [20:34:36.017413] Epoch: [1] Total time: 0:00:40 (0.1276 s / it)\n",
      "[20:34:36.017484] [20:34:36.017479] [20:34:36.017497] Averaged stats: lr: 0.000099  loss: 1.7824 (1.8091)\n",
      "[20:34:39.377997] [20:34:39.377974] [20:34:39.378184] Test:  [ 0/68]  eta: 0:01:54  loss: 1.4309 (1.4309)  acc1: 65.6250 (65.6250)  acc5: 100.0000 (100.0000)  time: 1.6794  data: 1.6619  max mem: 4666\n",
      "[20:34:40.752090] [20:34:40.752080] [20:34:40.752198] Test:  [10/68]  eta: 0:00:16  loss: 1.5076 (1.3374)  acc1: 65.6250 (64.4886)  acc5: 100.0000 (98.8636)  time: 0.2775  data: 0.2609  max mem: 4666\n",
      "[20:34:42.152175] [20:34:42.152164] [20:34:42.152224] Test:  [20/68]  eta: 0:00:10  loss: 1.1559 (1.2094)  acc1: 65.6250 (63.6905)  acc5: 100.0000 (98.3631)  time: 0.1386  data: 0.1224  max mem: 4666\n",
      "[20:34:43.543616] [20:34:43.543605] [20:34:43.543666] Test:  [30/68]  eta: 0:00:07  loss: 1.2308 (1.5024)  acc1: 43.7500 (49.3952)  acc5: 100.0000 (89.6169)  time: 0.1395  data: 0.1235  max mem: 4666\n",
      "[20:34:44.933074] [20:34:44.933064] [20:34:44.933121] Test:  [40/68]  eta: 0:00:04  loss: 2.1561 (1.6521)  acc1: 0.0000 (41.5396)  acc5: 90.6250 (84.6799)  time: 0.1390  data: 0.1230  max mem: 4666\n",
      "[20:34:46.310175] [20:34:46.310165] [20:34:46.310226] Test:  [50/68]  eta: 0:00:03  loss: 1.3733 (1.5853)  acc1: 40.6250 (46.8137)  acc5: 100.0000 (87.6838)  time: 0.1383  data: 0.1223  max mem: 4666\n",
      "[20:34:47.693584] [20:34:47.693575] [20:34:47.693632] Test:  [60/68]  eta: 0:00:01  loss: 1.3811 (1.6037)  acc1: 37.5000 (40.6762)  acc5: 100.0000 (89.7029)  time: 0.1380  data: 0.1220  max mem: 4666\n",
      "[20:34:47.881385] [20:34:47.881376] [20:34:47.881402] Test:  [67/68]  eta: 0:00:00  loss: 1.6402 (1.6068)  acc1: 0.0000 (36.6405)  acc5: 100.0000 (90.7245)  time: 0.1387  data: 0.1230  max mem: 4666\n",
      "[20:34:47.954595] [20:34:47.954586] [20:34:47.954745] Test: Total time: 0:00:10 (0.1508 s / it)\n",
      "[20:34:47.954790] [20:34:47.954788] [20:34:47.954796] * Acc@1 36.641 Acc@5 90.725 loss 1.607\n",
      "[20:34:47.955012] [20:34:47.955009] [20:34:47.955019] Accuracy of the network on the 2167 test images: 36.6%\n",
      "[20:34:47.955028] [20:34:47.955026] [20:34:47.955033] Max accuracy: 38.99%\n",
      "[20:34:49.546479] [20:34:49.546461] [20:34:49.546638] Epoch: [2]  [  0/316]  eta: 0:08:22  lr: 0.000100  loss: 1.6233 (1.6233)  time: 1.5901  data: 1.5396  max mem: 4666\n",
      "[20:34:51.990899] [20:34:51.990889] [20:34:51.991012] Epoch: [2]  [ 20/316]  eta: 0:00:56  lr: 0.000103  loss: 1.7603 (1.7570)  time: 0.1222  data: 0.0707  max mem: 4666\n",
      "[20:34:54.463683] [20:34:54.463673] [20:34:54.463733] Epoch: [2]  [ 40/316]  eta: 0:00:43  lr: 0.000106  loss: 1.8036 (1.7906)  time: 0.1236  data: 0.0724  max mem: 4666\n",
      "[20:34:56.907428] [20:34:56.907418] [20:34:56.907501] Epoch: [2]  [ 60/316]  eta: 0:00:37  lr: 0.000109  loss: 1.7824 (1.7893)  time: 0.1221  data: 0.0707  max mem: 4666\n",
      "[20:34:59.266771] [20:34:59.266760] [20:34:59.266820] Epoch: [2]  [ 80/316]  eta: 0:00:32  lr: 0.000113  loss: 1.7777 (1.7889)  time: 0.1179  data: 0.0665  max mem: 4666\n",
      "[20:35:01.740794] [20:35:01.740783] [20:35:01.740843] Epoch: [2]  [100/316]  eta: 0:00:29  lr: 0.000116  loss: 1.8102 (1.7924)  time: 0.1237  data: 0.0723  max mem: 4666\n",
      "[20:35:04.206998] [20:35:04.206988] [20:35:04.207053] Epoch: [2]  [120/316]  eta: 0:00:26  lr: 0.000119  loss: 1.8081 (1.7979)  time: 0.1233  data: 0.0719  max mem: 4666\n",
      "[20:35:06.619942] [20:35:06.619931] [20:35:06.619998] Epoch: [2]  [140/316]  eta: 0:00:23  lr: 0.000122  loss: 1.8175 (1.8029)  time: 0.1206  data: 0.0692  max mem: 4666\n",
      "[20:35:09.050844] [20:35:09.050834] [20:35:09.051035] Epoch: [2]  [160/316]  eta: 0:00:20  lr: 0.000125  loss: 1.7589 (1.8012)  time: 0.1215  data: 0.0701  max mem: 4666\n",
      "[20:35:11.465377] [20:35:11.465365] [20:35:11.465431] Epoch: [2]  [180/316]  eta: 0:00:17  lr: 0.000128  loss: 1.8080 (1.8011)  time: 0.1207  data: 0.0693  max mem: 4666\n",
      "[20:35:13.847329] [20:35:13.847319] [20:35:13.847382] Epoch: [2]  [200/316]  eta: 0:00:14  lr: 0.000132  loss: 1.7645 (1.7993)  time: 0.1191  data: 0.0677  max mem: 4666\n",
      "[20:35:16.294253] [20:35:16.294243] [20:35:16.294307] Epoch: [2]  [220/316]  eta: 0:00:12  lr: 0.000135  loss: 1.7886 (1.7978)  time: 0.1223  data: 0.0709  max mem: 4666\n",
      "[20:35:18.695364] [20:35:18.695353] [20:35:18.695416] Epoch: [2]  [240/316]  eta: 0:00:09  lr: 0.000138  loss: 1.7716 (1.7968)  time: 0.1200  data: 0.0686  max mem: 4666\n",
      "[20:35:21.106508] [20:35:21.106498] [20:35:21.106565] Epoch: [2]  [260/316]  eta: 0:00:07  lr: 0.000141  loss: 1.8266 (1.7974)  time: 0.1205  data: 0.0692  max mem: 4666\n",
      "[20:35:23.542841] [20:35:23.542831] [20:35:23.542895] Epoch: [2]  [280/316]  eta: 0:00:04  lr: 0.000144  loss: 1.7977 (1.7974)  time: 0.1218  data: 0.0704  max mem: 4666\n",
      "[20:35:25.982265] [20:35:25.982255] [20:35:25.982318] Epoch: [2]  [300/316]  eta: 0:00:02  lr: 0.000147  loss: 1.7815 (1.7957)  time: 0.1219  data: 0.0706  max mem: 4666\n",
      "[20:35:27.900966] [20:35:27.900957] [20:35:27.901012] Epoch: [2]  [315/316]  eta: 0:00:00  lr: 0.000149  loss: 1.7351 (1.7966)  time: 0.1218  data: 0.0706  max mem: 4666\n",
      "[20:35:27.968479] [20:35:27.968470] [20:35:27.968497] Epoch: [2] Total time: 0:00:40 (0.1266 s / it)\n",
      "[20:35:27.968548] [20:35:27.968546] [20:35:27.968555] Averaged stats: lr: 0.000149  loss: 1.7351 (1.7966)\n",
      "[20:35:31.307296] [20:35:31.307274] [20:35:31.307452] Test:  [ 0/68]  eta: 0:01:54  loss: 1.6071 (1.6071)  acc1: 53.1250 (53.1250)  acc5: 100.0000 (100.0000)  time: 1.6879  data: 1.6710  max mem: 4666\n",
      "[20:35:32.674403] [20:35:32.674392] [20:35:32.674457] Test:  [10/68]  eta: 0:00:16  loss: 1.7220 (1.5071)  acc1: 50.0000 (51.7045)  acc5: 100.0000 (98.8636)  time: 0.2776  data: 0.2611  max mem: 4666\n",
      "[20:35:34.076552] [20:35:34.076540] [20:35:34.076621] Test:  [20/68]  eta: 0:00:10  loss: 1.1962 (1.2991)  acc1: 56.2500 (57.4405)  acc5: 100.0000 (98.9583)  time: 0.1384  data: 0.1221  max mem: 4666\n",
      "[20:35:35.471369] [20:35:35.471359] [20:35:35.471426] Test:  [30/68]  eta: 0:00:07  loss: 1.2388 (1.5332)  acc1: 50.0000 (45.1613)  acc5: 100.0000 (89.8185)  time: 0.1398  data: 0.1238  max mem: 4666\n",
      "[20:35:36.869720] [20:35:36.869710] [20:35:36.869774] Test:  [40/68]  eta: 0:00:04  loss: 1.9606 (1.6347)  acc1: 0.0000 (38.5671)  acc5: 84.3750 (86.8902)  time: 0.1396  data: 0.1236  max mem: 4666\n",
      "[20:35:38.253745] [20:35:38.253735] [20:35:38.253797] Test:  [50/68]  eta: 0:00:03  loss: 1.3017 (1.5460)  acc1: 50.0000 (46.5074)  acc5: 100.0000 (89.4608)  time: 0.1391  data: 0.1230  max mem: 4666\n",
      "[20:35:39.634956] [20:35:39.634946] [20:35:39.635008] Test:  [60/68]  eta: 0:00:01  loss: 1.3052 (1.5889)  acc1: 43.7500 (40.5738)  acc5: 100.0000 (91.1885)  time: 0.1382  data: 0.1222  max mem: 4666\n",
      "[20:35:39.755915] [20:35:39.755909] [20:35:39.755928] Test:  [67/68]  eta: 0:00:00  loss: 1.7780 (1.6085)  acc1: 0.0000 (36.5482)  acc5: 100.0000 (92.0628)  time: 0.1372  data: 0.1215  max mem: 4666\n",
      "[20:35:39.827713] [20:35:39.827704] [20:35:39.827728] Test: Total time: 0:00:10 (0.1501 s / it)\n",
      "[20:35:39.827765] [20:35:39.827763] [20:35:39.827771] * Acc@1 36.548 Acc@5 92.063 loss 1.608\n",
      "[20:35:39.827975] [20:35:39.827971] [20:35:39.827983] Accuracy of the network on the 2167 test images: 36.5%\n",
      "[20:35:39.827993] [20:35:39.827991] [20:35:39.827998] Max accuracy: 38.99%\n",
      "[20:35:41.433809] [20:35:41.433791] [20:35:41.433960] Epoch: [3]  [  0/316]  eta: 0:08:27  lr: 0.000150  loss: 1.7512 (1.7512)  time: 1.6045  data: 1.5533  max mem: 4666\n",
      "[20:35:43.817063] [20:35:43.817054] [20:35:43.817116] Epoch: [3]  [ 20/316]  eta: 0:00:56  lr: 0.000153  loss: 1.7311 (1.7558)  time: 0.1191  data: 0.0677  max mem: 4666\n",
      "[20:35:46.248618] [20:35:46.248608] [20:35:46.248672] Epoch: [3]  [ 40/316]  eta: 0:00:43  lr: 0.000156  loss: 1.8106 (1.7788)  time: 0.1215  data: 0.0702  max mem: 4666\n",
      "[20:35:48.717682] [20:35:48.717671] [20:35:48.717742] Epoch: [3]  [ 60/316]  eta: 0:00:37  lr: 0.000159  loss: 1.8199 (1.7891)  time: 0.1234  data: 0.0720  max mem: 4666\n",
      "[20:35:51.116933] [20:35:51.116923] [20:35:51.116994] Epoch: [3]  [ 80/316]  eta: 0:00:32  lr: 0.000163  loss: 1.7776 (1.7899)  time: 0.1199  data: 0.0685  max mem: 4666\n",
      "[20:35:53.556254] [20:35:53.556243] [20:35:53.556314] Epoch: [3]  [100/316]  eta: 0:00:29  lr: 0.000166  loss: 1.8273 (1.7971)  time: 0.1219  data: 0.0705  max mem: 4666\n",
      "[20:35:55.989999] [20:35:55.989989] [20:35:55.990064] Epoch: [3]  [120/316]  eta: 0:00:26  lr: 0.000169  loss: 1.8422 (1.8064)  time: 0.1216  data: 0.0703  max mem: 4666\n",
      "[20:35:58.415126] [20:35:58.415116] [20:35:58.415182] Epoch: [3]  [140/316]  eta: 0:00:23  lr: 0.000172  loss: 1.8342 (1.8084)  time: 0.1212  data: 0.0697  max mem: 4666\n",
      "[20:36:00.856866] [20:36:00.856855] [20:36:00.856920] Epoch: [3]  [160/316]  eta: 0:00:20  lr: 0.000175  loss: 1.7710 (1.8029)  time: 0.1220  data: 0.0706  max mem: 4666\n",
      "[20:36:03.299018] [20:36:03.299007] [20:36:03.299077] Epoch: [3]  [180/316]  eta: 0:00:17  lr: 0.000178  loss: 1.8082 (1.8053)  time: 0.1221  data: 0.0707  max mem: 4666\n",
      "[20:36:05.767156] [20:36:05.767145] [20:36:05.767213] Epoch: [3]  [200/316]  eta: 0:00:14  lr: 0.000182  loss: 1.7921 (1.8051)  time: 0.1234  data: 0.0720  max mem: 4666\n",
      "[20:36:08.194855] [20:36:08.194844] [20:36:08.194909] Epoch: [3]  [220/316]  eta: 0:00:12  lr: 0.000185  loss: 1.8075 (1.8034)  time: 0.1213  data: 0.0700  max mem: 4666\n",
      "[20:36:10.657277] [20:36:10.657266] [20:36:10.657335] Epoch: [3]  [240/316]  eta: 0:00:09  lr: 0.000188  loss: 1.8385 (1.8053)  time: 0.1231  data: 0.0717  max mem: 4666\n",
      "[20:36:13.095740] [20:36:13.095730] [20:36:13.095793] Epoch: [3]  [260/316]  eta: 0:00:07  lr: 0.000191  loss: 1.7910 (1.8031)  time: 0.1219  data: 0.0705  max mem: 4666\n",
      "[20:36:15.504790] [20:36:15.504779] [20:36:15.504857] Epoch: [3]  [280/316]  eta: 0:00:04  lr: 0.000194  loss: 1.7925 (1.8022)  time: 0.1204  data: 0.0690  max mem: 4666\n",
      "[20:36:17.937505] [20:36:17.937495] [20:36:17.937564] Epoch: [3]  [300/316]  eta: 0:00:02  lr: 0.000197  loss: 1.7544 (1.7988)  time: 0.1216  data: 0.0703  max mem: 4666\n",
      "[20:36:19.904599] [20:36:19.904590] [20:36:19.904644] Epoch: [3]  [315/316]  eta: 0:00:00  lr: 0.000199  loss: 1.7612 (1.7991)  time: 0.1203  data: 0.0690  max mem: 4666\n",
      "[20:36:19.979298] [20:36:19.979289] [20:36:19.979317] Epoch: [3] Total time: 0:00:40 (0.1271 s / it)\n",
      "[20:36:19.979377] [20:36:19.979374] [20:36:19.979384] Averaged stats: lr: 0.000199  loss: 1.7612 (1.7991)\n",
      "[20:36:23.338577] [20:36:23.338546] [20:36:23.338801] Test:  [ 0/68]  eta: 0:01:57  loss: 1.5133 (1.5133)  acc1: 65.6250 (65.6250)  acc5: 100.0000 (100.0000)  time: 1.7294  data: 1.7122  max mem: 4666\n",
      "[20:36:24.681040] [20:36:24.681030] [20:36:24.681104] Test:  [10/68]  eta: 0:00:16  loss: 1.5803 (1.4076)  acc1: 62.5000 (58.8068)  acc5: 100.0000 (96.0227)  time: 0.2792  data: 0.2627  max mem: 4666\n",
      "[20:36:26.090836] [20:36:26.090824] [20:36:26.090894] Test:  [20/68]  eta: 0:00:10  loss: 1.3211 (1.3119)  acc1: 62.5000 (60.4167)  acc5: 93.7500 (92.4107)  time: 0.1375  data: 0.1213  max mem: 4666\n",
      "[20:36:27.492744] [20:36:27.492734] [20:36:27.492945] Test:  [30/68]  eta: 0:00:07  loss: 1.4394 (1.5622)  acc1: 43.7500 (47.0766)  acc5: 90.6250 (84.7782)  time: 0.1405  data: 0.1245  max mem: 4666\n",
      "[20:36:28.879293] [20:36:28.879283] [20:36:28.879364] Test:  [40/68]  eta: 0:00:04  loss: 1.9704 (1.6460)  acc1: 0.0000 (39.8628)  acc5: 96.8750 (87.6524)  time: 0.1393  data: 0.1234  max mem: 4666\n",
      "[20:36:30.261897] [20:36:30.261887] [20:36:30.261952] Test:  [50/68]  eta: 0:00:03  loss: 1.4330 (1.5801)  acc1: 50.0000 (47.2426)  acc5: 100.0000 (90.0735)  time: 0.1384  data: 0.1225  max mem: 4666\n",
      "[20:36:31.638489] [20:36:31.638479] [20:36:31.638538] Test:  [60/68]  eta: 0:00:01  loss: 1.4330 (1.6022)  acc1: 40.6250 (41.1373)  acc5: 100.0000 (91.7008)  time: 0.1379  data: 0.1220  max mem: 4666\n",
      "[20:36:31.801126] [20:36:31.801121] [20:36:31.801137] Test:  [67/68]  eta: 0:00:00  loss: 1.6840 (1.6103)  acc1: 0.0000 (37.0558)  acc5: 100.0000 (92.5242)  time: 0.1374  data: 0.1218  max mem: 4666\n",
      "[20:36:31.868466] [20:36:31.868460] [20:36:31.868492] Test: Total time: 0:00:10 (0.1509 s / it)\n",
      "[20:36:31.868534] [20:36:31.868532] [20:36:31.868540] * Acc@1 37.056 Acc@5 92.524 loss 1.610\n",
      "[20:36:31.868694] [20:36:31.868690] [20:36:31.868701] Accuracy of the network on the 2167 test images: 37.1%\n",
      "[20:36:31.868710] [20:36:31.868709] [20:36:31.868715] Max accuracy: 38.99%\n",
      "[20:36:33.449843] [20:36:33.449815] [20:36:33.450022] Epoch: [4]  [  0/316]  eta: 0:08:19  lr: 0.000200  loss: 1.7822 (1.7822)  time: 1.5798  data: 1.5276  max mem: 4666\n",
      "[20:36:35.892004] [20:36:35.891993] [20:36:35.892053] Epoch: [4]  [ 20/316]  eta: 0:00:56  lr: 0.000203  loss: 1.7938 (1.8008)  time: 0.1221  data: 0.0705  max mem: 4666\n",
      "[20:36:38.337999] [20:36:38.337987] [20:36:38.338057] Epoch: [4]  [ 40/316]  eta: 0:00:43  lr: 0.000206  loss: 1.7785 (1.8063)  time: 0.1223  data: 0.0709  max mem: 4666\n",
      "[20:36:40.812623] [20:36:40.812614] [20:36:40.812745] Epoch: [4]  [ 60/316]  eta: 0:00:37  lr: 0.000209  loss: 1.7734 (1.7989)  time: 0.1237  data: 0.0723  max mem: 4666\n",
      "[20:36:43.264188] [20:36:43.264176] [20:36:43.264244] Epoch: [4]  [ 80/316]  eta: 0:00:33  lr: 0.000213  loss: 1.8004 (1.8007)  time: 0.1225  data: 0.0711  max mem: 4666\n",
      "[20:36:45.738654] [20:36:45.738643] [20:36:45.738706] Epoch: [4]  [100/316]  eta: 0:00:29  lr: 0.000216  loss: 1.7974 (1.8015)  time: 0.1237  data: 0.0722  max mem: 4666\n",
      "[20:36:48.179653] [20:36:48.179642] [20:36:48.179714] Epoch: [4]  [120/316]  eta: 0:00:26  lr: 0.000219  loss: 1.8065 (1.8055)  time: 0.1220  data: 0.0706  max mem: 4666\n",
      "[20:36:50.610826] [20:36:50.610817] [20:36:50.610879] Epoch: [4]  [140/316]  eta: 0:00:23  lr: 0.000222  loss: 1.8269 (1.8098)  time: 0.1215  data: 0.0701  max mem: 4666\n",
      "[20:36:53.058437] [20:36:53.058426] [20:36:53.058491] Epoch: [4]  [160/316]  eta: 0:00:20  lr: 0.000225  loss: 1.7657 (1.8050)  time: 0.1223  data: 0.0710  max mem: 4666\n",
      "[20:36:55.503769] [20:36:55.503758] [20:36:55.503824] Epoch: [4]  [180/316]  eta: 0:00:17  lr: 0.000228  loss: 1.7936 (1.8056)  time: 0.1222  data: 0.0708  max mem: 4666\n",
      "[20:36:57.915753] [20:36:57.915742] [20:36:57.915867] Epoch: [4]  [200/316]  eta: 0:00:15  lr: 0.000232  loss: 1.7893 (1.8042)  time: 0.1206  data: 0.0691  max mem: 4666\n",
      "[20:37:00.342312] [20:37:00.342301] [20:37:00.342367] Epoch: [4]  [220/316]  eta: 0:00:12  lr: 0.000235  loss: 1.7582 (1.7994)  time: 0.1213  data: 0.0698  max mem: 4666\n",
      "[20:37:02.771257] [20:37:02.771246] [20:37:02.771366] Epoch: [4]  [240/316]  eta: 0:00:09  lr: 0.000238  loss: 1.7801 (1.7987)  time: 0.1214  data: 0.0700  max mem: 4666\n",
      "[20:37:05.192477] [20:37:05.192466] [20:37:05.192529] Epoch: [4]  [260/316]  eta: 0:00:07  lr: 0.000241  loss: 1.8005 (1.7978)  time: 0.1210  data: 0.0696  max mem: 4666\n",
      "[20:37:07.621998] [20:37:07.621988] [20:37:07.622051] Epoch: [4]  [280/316]  eta: 0:00:04  lr: 0.000244  loss: 1.7770 (1.7965)  time: 0.1214  data: 0.0700  max mem: 4666\n",
      "[20:37:10.058555] [20:37:10.058543] [20:37:10.058611] Epoch: [4]  [300/316]  eta: 0:00:02  lr: 0.000247  loss: 1.7655 (1.7934)  time: 0.1218  data: 0.0704  max mem: 4666\n",
      "[20:37:11.912036] [20:37:11.912027] [20:37:11.912081] Epoch: [4]  [315/316]  eta: 0:00:00  lr: 0.000249  loss: 1.7522 (1.7940)  time: 0.1220  data: 0.0707  max mem: 4666\n",
      "[20:37:11.990804] [20:37:11.990799] [20:37:11.990816] Epoch: [4] Total time: 0:00:40 (0.1270 s / it)\n",
      "[20:37:11.990975] [20:37:11.990971] [20:37:11.990984] Averaged stats: lr: 0.000249  loss: 1.7522 (1.7940)\n",
      "[20:37:15.360347] [20:37:15.360325] [20:37:15.360507] Test:  [ 0/68]  eta: 0:01:55  loss: 1.4489 (1.4489)  acc1: 65.6250 (65.6250)  acc5: 96.8750 (96.8750)  time: 1.7007  data: 1.6838  max mem: 4666\n",
      "[20:37:16.738299] [20:37:16.738288] [20:37:16.738368] Test:  [10/68]  eta: 0:00:16  loss: 1.5823 (1.3853)  acc1: 65.6250 (62.5000)  acc5: 96.8750 (96.3068)  time: 0.2798  data: 0.2634  max mem: 4666\n",
      "[20:37:18.174645] [20:37:18.174633] [20:37:18.174814] Test:  [20/68]  eta: 0:00:10  loss: 1.1230 (1.1932)  acc1: 65.6250 (68.1548)  acc5: 96.8750 (95.8333)  time: 0.1406  data: 0.1245  max mem: 4666\n",
      "[20:37:19.571492] [20:37:19.571482] [20:37:19.571559] Test:  [30/68]  eta: 0:00:07  loss: 1.1666 (1.4643)  acc1: 59.3750 (53.7298)  acc5: 96.8750 (87.9032)  time: 0.1416  data: 0.1257  max mem: 4666\n",
      "[20:37:20.981403] [20:37:20.981392] [20:37:20.981503] Test:  [40/68]  eta: 0:00:04  loss: 1.9827 (1.5719)  acc1: 0.0000 (44.0549)  acc5: 96.8750 (90.0915)  time: 0.1403  data: 0.1244  max mem: 4666\n",
      "[20:37:22.351639] [20:37:22.351629] [20:37:22.351687] Test:  [50/68]  eta: 0:00:03  loss: 1.4583 (1.5331)  acc1: 40.6250 (48.8358)  acc5: 100.0000 (92.0343)  time: 0.1389  data: 0.1230  max mem: 4666\n",
      "[20:37:23.741113] [20:37:23.741102] [20:37:23.741169] Test:  [60/68]  eta: 0:00:01  loss: 1.4612 (1.5659)  acc1: 25.0000 (42.8791)  acc5: 100.0000 (93.3402)  time: 0.1379  data: 0.1220  max mem: 4666\n",
      "[20:37:23.848722] [20:37:23.848715] [20:37:23.848737] Test:  [67/68]  eta: 0:00:00  loss: 1.6313 (1.5716)  acc1: 6.2500 (39.4555)  acc5: 100.0000 (94.0009)  time: 0.1377  data: 0.1220  max mem: 4666\n",
      "[20:37:23.919585] [20:37:23.919576] [20:37:23.919602] Test: Total time: 0:00:10 (0.1509 s / it)\n",
      "[20:37:23.919642] [20:37:23.919640] [20:37:23.919649] * Acc@1 39.455 Acc@5 94.001 loss 1.572\n",
      "[20:37:23.919749] [20:37:23.919746] [20:37:23.919755] Accuracy of the network on the 2167 test images: 39.5%\n",
      "[20:37:23.919766] [20:37:23.919764] [20:37:23.919771] Max accuracy: 39.46%\n",
      "[20:37:25.488609] [20:37:25.488590] [20:37:25.488777] Epoch: [5]  [  0/316]  eta: 0:08:15  lr: 0.000250  loss: 1.6583 (1.6583)  time: 1.5675  data: 1.5163  max mem: 4666\n",
      "[20:37:27.959549] [20:37:27.959539] [20:37:27.959599] Epoch: [5]  [ 20/316]  eta: 0:00:56  lr: 0.000250  loss: 1.7308 (1.7546)  time: 0.1235  data: 0.0719  max mem: 4666\n",
      "[20:37:30.384243] [20:37:30.384232] [20:37:30.384298] Epoch: [5]  [ 40/316]  eta: 0:00:43  lr: 0.000250  loss: 1.7864 (1.7847)  time: 0.1212  data: 0.0696  max mem: 4666\n",
      "[20:37:32.874065] [20:37:32.874055] [20:37:32.874133] Epoch: [5]  [ 60/316]  eta: 0:00:37  lr: 0.000250  loss: 1.7944 (1.7886)  time: 0.1244  data: 0.0728  max mem: 4666\n",
      "[20:37:35.352628] [20:37:35.352619] [20:37:35.352682] Epoch: [5]  [ 80/316]  eta: 0:00:33  lr: 0.000250  loss: 1.7717 (1.7863)  time: 0.1239  data: 0.0723  max mem: 4666\n",
      "[20:37:37.849290] [20:37:37.849279] [20:37:37.849347] Epoch: [5]  [100/316]  eta: 0:00:29  lr: 0.000250  loss: 1.7936 (1.7881)  time: 0.1248  data: 0.0732  max mem: 4666\n",
      "[20:37:40.320074] [20:37:40.320061] [20:37:40.320143] Epoch: [5]  [120/316]  eta: 0:00:26  lr: 0.000250  loss: 1.7983 (1.7934)  time: 0.1235  data: 0.0719  max mem: 4666\n",
      "[20:37:42.761285] [20:37:42.761273] [20:37:42.761342] Epoch: [5]  [140/316]  eta: 0:00:23  lr: 0.000250  loss: 1.7960 (1.7955)  time: 0.1220  data: 0.0706  max mem: 4666\n",
      "[20:37:45.197801] [20:37:45.197789] [20:37:45.197860] Epoch: [5]  [160/316]  eta: 0:00:20  lr: 0.000250  loss: 1.7441 (1.7920)  time: 0.1218  data: 0.0703  max mem: 4666\n",
      "[20:37:47.706935] [20:37:47.706924] [20:37:47.706989] Epoch: [5]  [180/316]  eta: 0:00:17  lr: 0.000250  loss: 1.7928 (1.7925)  time: 0.1254  data: 0.0740  max mem: 4666\n",
      "[20:37:50.162550] [20:37:50.162538] [20:37:50.162683] Epoch: [5]  [200/316]  eta: 0:00:15  lr: 0.000250  loss: 1.7607 (1.7910)  time: 0.1227  data: 0.0713  max mem: 4666\n",
      "[20:37:52.589277] [20:37:52.589264] [20:37:52.589409] Epoch: [5]  [220/316]  eta: 0:00:12  lr: 0.000250  loss: 1.7423 (1.7884)  time: 0.1213  data: 0.0698  max mem: 4666\n",
      "[20:37:55.086118] [20:37:55.086107] [20:37:55.086175] Epoch: [5]  [240/316]  eta: 0:00:09  lr: 0.000250  loss: 1.7960 (1.7884)  time: 0.1248  data: 0.0734  max mem: 4666\n",
      "[20:37:57.529613] [20:37:57.529602] [20:37:57.529674] Epoch: [5]  [260/316]  eta: 0:00:07  lr: 0.000250  loss: 1.7439 (1.7862)  time: 0.1221  data: 0.0707  max mem: 4666\n",
      "[20:38:00.014912] [20:38:00.014901] [20:38:00.014969] Epoch: [5]  [280/316]  eta: 0:00:04  lr: 0.000250  loss: 1.7926 (1.7863)  time: 0.1242  data: 0.0728  max mem: 4666\n",
      "[20:38:02.447140] [20:38:02.447130] [20:38:02.447271] Epoch: [5]  [300/316]  eta: 0:00:02  lr: 0.000250  loss: 1.7705 (1.7854)  time: 0.1216  data: 0.0701  max mem: 4666\n",
      "[20:38:03.898503] [20:38:03.898493] [20:38:03.898550] Epoch: [5]  [315/316]  eta: 0:00:00  lr: 0.000250  loss: 1.7479 (1.7853)  time: 0.1208  data: 0.0696  max mem: 4666\n",
      "[20:38:03.980360] [20:38:03.980351] [20:38:03.980376] Epoch: [5] Total time: 0:00:40 (0.1268 s / it)\n",
      "[20:38:03.980540] [20:38:03.980536] [20:38:03.980550] Averaged stats: lr: 0.000250  loss: 1.7479 (1.7853)\n",
      "[20:38:07.451462] [20:38:07.451438] [20:38:07.451650] Test:  [ 0/68]  eta: 0:01:58  loss: 1.3432 (1.3432)  acc1: 65.6250 (65.6250)  acc5: 100.0000 (100.0000)  time: 1.7374  data: 1.7202  max mem: 4666\n",
      "[20:38:08.810730] [20:38:08.810719] [20:38:08.810781] Test:  [10/68]  eta: 0:00:16  loss: 1.4536 (1.3141)  acc1: 62.5000 (61.0795)  acc5: 100.0000 (98.8636)  time: 0.2814  data: 0.2650  max mem: 4666\n",
      "[20:38:10.224295] [20:38:10.224283] [20:38:10.224352] Test:  [20/68]  eta: 0:00:10  loss: 1.1854 (1.1870)  acc1: 65.6250 (63.6905)  acc5: 100.0000 (97.0238)  time: 0.1386  data: 0.1224  max mem: 4666\n",
      "[20:38:11.622966] [20:38:11.622955] [20:38:11.623021] Test:  [30/68]  eta: 0:00:07  loss: 1.2723 (1.4837)  acc1: 43.7500 (49.6976)  acc5: 90.6250 (86.7944)  time: 0.1405  data: 0.1246  max mem: 4666\n",
      "[20:38:13.025310] [20:38:13.025300] [20:38:13.025364] Test:  [40/68]  eta: 0:00:04  loss: 2.0708 (1.5866)  acc1: 0.0000 (41.6921)  acc5: 96.8750 (88.9482)  time: 0.1400  data: 0.1240  max mem: 4666\n",
      "[20:38:14.410589] [20:38:14.410580] [20:38:14.410649] Test:  [50/68]  eta: 0:00:03  loss: 1.3335 (1.5128)  acc1: 50.0000 (49.2647)  acc5: 100.0000 (91.1152)  time: 0.1393  data: 0.1233  max mem: 4666\n",
      "[20:38:15.787385] [20:38:15.787376] [20:38:15.787448] Test:  [60/68]  eta: 0:00:01  loss: 1.3335 (1.5429)  acc1: 50.0000 (42.7766)  acc5: 100.0000 (92.5717)  time: 0.1380  data: 0.1220  max mem: 4666\n",
      "[20:38:15.924844] [20:38:15.924839] [20:38:15.924859] Test:  [67/68]  eta: 0:00:00  loss: 1.6220 (1.5511)  acc1: 0.0000 (38.5325)  acc5: 100.0000 (93.3087)  time: 0.1392  data: 0.1235  max mem: 4666\n",
      "[20:38:15.989327] [20:38:15.989321] [20:38:15.989352] Test: Total time: 0:00:10 (0.1511 s / it)\n",
      "[20:38:15.989389] [20:38:15.989387] [20:38:15.989395] * Acc@1 38.533 Acc@5 93.309 loss 1.551\n",
      "[20:38:15.989604] [20:38:15.989597] [20:38:15.989620] Accuracy of the network on the 2167 test images: 38.5%\n",
      "[20:38:15.989638] [20:38:15.989634] [20:38:15.989650] Max accuracy: 39.46%\n",
      "[20:38:17.631148] [20:38:17.631128] [20:38:17.631308] Epoch: [6]  [  0/316]  eta: 0:08:38  lr: 0.000250  loss: 1.7067 (1.7067)  time: 1.6398  data: 1.5881  max mem: 4666\n",
      "[20:38:20.075049] [20:38:20.075039] [20:38:20.075129] Epoch: [6]  [ 20/316]  eta: 0:00:57  lr: 0.000250  loss: 1.7273 (1.7465)  time: 0.1221  data: 0.0707  max mem: 4666\n",
      "[20:38:22.516525] [20:38:22.516514] [20:38:22.516579] Epoch: [6]  [ 40/316]  eta: 0:00:43  lr: 0.000250  loss: 1.7522 (1.7653)  time: 0.1220  data: 0.0706  max mem: 4666\n",
      "[20:38:24.997991] [20:38:24.997981] [20:38:24.998041] Epoch: [6]  [ 60/316]  eta: 0:00:37  lr: 0.000250  loss: 1.7733 (1.7674)  time: 0.1240  data: 0.0726  max mem: 4666\n",
      "[20:38:27.509830] [20:38:27.509819] [20:38:27.509898] Epoch: [6]  [ 80/316]  eta: 0:00:33  lr: 0.000250  loss: 1.7491 (1.7695)  time: 0.1255  data: 0.0742  max mem: 4666\n",
      "[20:38:29.987636] [20:38:29.987626] [20:38:29.987689] Epoch: [6]  [100/316]  eta: 0:00:29  lr: 0.000250  loss: 1.7466 (1.7682)  time: 0.1239  data: 0.0724  max mem: 4666\n",
      "[20:38:32.476279] [20:38:32.476268] [20:38:32.476334] Epoch: [6]  [120/316]  eta: 0:00:26  lr: 0.000250  loss: 1.7984 (1.7713)  time: 0.1244  data: 0.0730  max mem: 4666\n",
      "[20:38:34.962685] [20:38:34.962674] [20:38:34.962737] Epoch: [6]  [140/316]  eta: 0:00:23  lr: 0.000250  loss: 1.8022 (1.7763)  time: 0.1243  data: 0.0729  max mem: 4666\n",
      "[20:38:37.410753] [20:38:37.410743] [20:38:37.410806] Epoch: [6]  [160/316]  eta: 0:00:20  lr: 0.000250  loss: 1.7600 (1.7734)  time: 0.1224  data: 0.0710  max mem: 4666\n",
      "[20:38:39.858553] [20:38:39.858543] [20:38:39.858603] Epoch: [6]  [180/316]  eta: 0:00:17  lr: 0.000250  loss: 1.8008 (1.7758)  time: 0.1224  data: 0.0710  max mem: 4666\n",
      "[20:38:42.302380] [20:38:42.302369] [20:38:42.302437] Epoch: [6]  [200/316]  eta: 0:00:15  lr: 0.000250  loss: 1.7624 (1.7757)  time: 0.1221  data: 0.0708  max mem: 4666\n",
      "[20:38:44.775403] [20:38:44.775392] [20:38:44.775461] Epoch: [6]  [220/316]  eta: 0:00:12  lr: 0.000250  loss: 1.7198 (1.7726)  time: 0.1236  data: 0.0722  max mem: 4666\n",
      "[20:38:47.252420] [20:38:47.252409] [20:38:47.252472] Epoch: [6]  [240/316]  eta: 0:00:09  lr: 0.000250  loss: 1.8034 (1.7748)  time: 0.1238  data: 0.0724  max mem: 4666\n",
      "[20:38:49.742550] [20:38:49.742540] [20:38:49.742605] Epoch: [6]  [260/316]  eta: 0:00:07  lr: 0.000250  loss: 1.7749 (1.7743)  time: 0.1245  data: 0.0731  max mem: 4666\n",
      "[20:38:52.229159] [20:38:52.229148] [20:38:52.229218] Epoch: [6]  [280/316]  eta: 0:00:04  lr: 0.000250  loss: 1.7628 (1.7734)  time: 0.1243  data: 0.0729  max mem: 4666\n",
      "[20:38:54.670617] [20:38:54.670606] [20:38:54.670672] Epoch: [6]  [300/316]  eta: 0:00:02  lr: 0.000250  loss: 1.7660 (1.7716)  time: 0.1220  data: 0.0707  max mem: 4666\n",
      "[20:38:56.119546] [20:38:56.119536] [20:38:56.119590] Epoch: [6]  [315/316]  eta: 0:00:00  lr: 0.000250  loss: 1.7166 (1.7714)  time: 0.1204  data: 0.0691  max mem: 4666\n",
      "[20:38:56.195030] [20:38:56.195020] [20:38:56.195049] Epoch: [6] Total time: 0:00:40 (0.1272 s / it)\n",
      "[20:38:56.195107] [20:38:56.195104] [20:38:56.195113] Averaged stats: lr: 0.000250  loss: 1.7166 (1.7714)\n",
      "[20:38:59.625376] [20:38:59.625358] [20:38:59.625503] Test:  [ 0/68]  eta: 0:01:56  loss: 1.3026 (1.3026)  acc1: 65.6250 (65.6250)  acc5: 100.0000 (100.0000)  time: 1.7131  data: 1.6963  max mem: 4666\n",
      "[20:39:00.984000] [20:39:00.983990] [20:39:00.984059] Test:  [10/68]  eta: 0:00:16  loss: 1.4469 (1.2868)  acc1: 62.5000 (61.6477)  acc5: 100.0000 (96.3068)  time: 0.2792  data: 0.2627  max mem: 4666\n",
      "[20:39:02.415592] [20:39:02.415582] [20:39:02.415689] Test:  [20/68]  eta: 0:00:10  loss: 1.0603 (1.0909)  acc1: 65.6250 (68.3036)  acc5: 100.0000 (96.4286)  time: 0.1394  data: 0.1232  max mem: 4666\n",
      "[20:39:03.805082] [20:39:03.805071] [20:39:03.805144] Test:  [30/68]  eta: 0:00:07  loss: 1.0646 (1.3431)  acc1: 59.3750 (57.1573)  acc5: 96.8750 (89.4153)  time: 0.1410  data: 0.1250  max mem: 4666\n",
      "[20:39:05.204614] [20:39:05.204604] [20:39:05.204713] Test:  [40/68]  eta: 0:00:04  loss: 1.7583 (1.4952)  acc1: 6.2500 (46.4939)  acc5: 93.7500 (90.9299)  time: 0.1394  data: 0.1234  max mem: 4666\n",
      "[20:39:06.577282] [20:39:06.577271] [20:39:06.577331] Test:  [50/68]  eta: 0:00:03  loss: 1.4313 (1.4691)  acc1: 34.3750 (48.6520)  acc5: 100.0000 (92.7083)  time: 0.1385  data: 0.1226  max mem: 4666\n",
      "[20:39:07.943331] [20:39:07.943321] [20:39:07.943647] Test:  [60/68]  eta: 0:00:01  loss: 1.4485 (1.5019)  acc1: 31.2500 (45.0307)  acc5: 100.0000 (93.9037)  time: 0.1369  data: 0.1209  max mem: 4666\n",
      "[20:39:08.051174] [20:39:08.051164] [20:39:08.051192] Test:  [67/68]  eta: 0:00:00  loss: 1.5824 (1.5081)  acc1: 31.2500 (45.2700)  acc5: 100.0000 (94.5085)  time: 0.1366  data: 0.1209  max mem: 4666\n",
      "[20:39:08.118848] [20:39:08.118838] [20:39:08.118867] Test: Total time: 0:00:10 (0.1501 s / it)\n",
      "[20:39:08.118907] [20:39:08.118904] [20:39:08.118915] * Acc@1 45.270 Acc@5 94.509 loss 1.508\n",
      "[20:39:08.119167] [20:39:08.119162] [20:39:08.119176] Accuracy of the network on the 2167 test images: 45.3%\n",
      "[20:39:08.119190] [20:39:08.119187] [20:39:08.119197] Max accuracy: 45.27%\n",
      "[20:39:09.725121] [20:39:09.725102] [20:39:09.725258] Epoch: [7]  [  0/316]  eta: 0:08:27  lr: 0.000250  loss: 1.7351 (1.7351)  time: 1.6045  data: 1.5543  max mem: 4666\n",
      "[20:39:12.178854] [20:39:12.178844] [20:39:12.178985] Epoch: [7]  [ 20/316]  eta: 0:00:57  lr: 0.000250  loss: 1.7180 (1.7323)  time: 0.1226  data: 0.0711  max mem: 4666\n",
      "[20:39:14.647570] [20:39:14.647560] [20:39:14.647884] Epoch: [7]  [ 40/316]  eta: 0:00:43  lr: 0.000250  loss: 1.7668 (1.7546)  time: 0.1234  data: 0.0719  max mem: 4666\n",
      "[20:39:17.128554] [20:39:17.128543] [20:39:17.128616] Epoch: [7]  [ 60/316]  eta: 0:00:37  lr: 0.000250  loss: 1.7489 (1.7586)  time: 0.1240  data: 0.0726  max mem: 4666\n",
      "[20:39:19.564632] [20:39:19.564621] [20:39:19.564685] Epoch: [7]  [ 80/316]  eta: 0:00:33  lr: 0.000250  loss: 1.7687 (1.7655)  time: 0.1218  data: 0.0703  max mem: 4666\n",
      "[20:39:22.017661] [20:39:22.017650] [20:39:22.017720] Epoch: [7]  [100/316]  eta: 0:00:29  lr: 0.000250  loss: 1.7749 (1.7695)  time: 0.1226  data: 0.0713  max mem: 4666\n",
      "[20:39:24.487415] [20:39:24.487404] [20:39:24.487561] Epoch: [7]  [120/316]  eta: 0:00:26  lr: 0.000250  loss: 1.7672 (1.7723)  time: 0.1234  data: 0.0720  max mem: 4666\n",
      "[20:39:26.972003] [20:39:26.971992] [20:39:26.972071] Epoch: [7]  [140/316]  eta: 0:00:23  lr: 0.000250  loss: 1.7771 (1.7740)  time: 0.1242  data: 0.0728  max mem: 4666\n",
      "[20:39:29.474714] [20:39:29.474703] [20:39:29.474771] Epoch: [7]  [160/316]  eta: 0:00:20  lr: 0.000250  loss: 1.7342 (1.7682)  time: 0.1251  data: 0.0737  max mem: 4666\n",
      "[20:39:31.941018] [20:39:31.941008] [20:39:31.941077] Epoch: [7]  [180/316]  eta: 0:00:17  lr: 0.000250  loss: 1.7365 (1.7659)  time: 0.1233  data: 0.0719  max mem: 4666\n",
      "[20:39:34.439454] [20:39:34.439443] [20:39:34.439506] Epoch: [7]  [200/316]  eta: 0:00:15  lr: 0.000250  loss: 1.7221 (1.7639)  time: 0.1249  data: 0.0735  max mem: 4666\n",
      "[20:39:36.929550] [20:39:36.929539] [20:39:36.929607] Epoch: [7]  [220/316]  eta: 0:00:12  lr: 0.000250  loss: 1.7339 (1.7607)  time: 0.1245  data: 0.0731  max mem: 4666\n",
      "[20:39:39.428334] [20:39:39.428323] [20:39:39.428389] Epoch: [7]  [240/316]  eta: 0:00:09  lr: 0.000249  loss: 1.7600 (1.7617)  time: 0.1249  data: 0.0735  max mem: 4666\n",
      "[20:39:41.902515] [20:39:41.902504] [20:39:41.902575] Epoch: [7]  [260/316]  eta: 0:00:07  lr: 0.000249  loss: 1.7331 (1.7609)  time: 0.1237  data: 0.0722  max mem: 4666\n",
      "[20:39:44.381981] [20:39:44.381970] [20:39:44.382035] Epoch: [7]  [280/316]  eta: 0:00:04  lr: 0.000249  loss: 1.7302 (1.7598)  time: 0.1239  data: 0.0725  max mem: 4666\n",
      "[20:39:46.853862] [20:39:46.853851] [20:39:46.853930] Epoch: [7]  [300/316]  eta: 0:00:02  lr: 0.000249  loss: 1.7219 (1.7565)  time: 0.1235  data: 0.0722  max mem: 4666\n",
      "[20:39:48.304707] [20:39:48.304698] [20:39:48.304754] Epoch: [7]  [315/316]  eta: 0:00:00  lr: 0.000249  loss: 1.7399 (1.7588)  time: 0.1214  data: 0.0702  max mem: 4666\n",
      "[20:39:48.379192] [20:39:48.379187] [20:39:48.379204] Epoch: [7] Total time: 0:00:40 (0.1274 s / it)\n",
      "[20:39:48.379233] [20:39:48.379231] [20:39:48.379239] Averaged stats: lr: 0.000249  loss: 1.7399 (1.7588)\n",
      "[20:39:51.786579] [20:39:51.786553] [20:39:51.786775] Test:  [ 0/68]  eta: 0:01:55  loss: 1.2757 (1.2757)  acc1: 71.8750 (71.8750)  acc5: 87.5000 (87.5000)  time: 1.6937  data: 1.6766  max mem: 4666\n",
      "[20:39:53.151745] [20:39:53.151735] [20:39:53.151802] Test:  [10/68]  eta: 0:00:16  loss: 1.3959 (1.3500)  acc1: 59.3750 (59.9432)  acc5: 84.3750 (85.7955)  time: 0.2780  data: 0.2615  max mem: 4666\n",
      "[20:39:54.533592] [20:39:54.533583] [20:39:54.533646] Test:  [20/68]  eta: 0:00:10  loss: 1.0922 (1.1423)  acc1: 65.6250 (65.1786)  acc5: 96.8750 (91.9643)  time: 0.1373  data: 0.1210  max mem: 4666\n",
      "[20:39:55.968728] [20:39:55.968717] [20:39:55.968782] Test:  [30/68]  eta: 0:00:07  loss: 1.1033 (1.3909)  acc1: 50.0000 (55.2419)  acc5: 100.0000 (85.8871)  time: 0.1408  data: 0.1248  max mem: 4666\n",
      "[20:39:57.361504] [20:39:57.361494] [20:39:57.361557] Test:  [40/68]  eta: 0:00:04  loss: 1.7871 (1.5028)  acc1: 9.3750 (45.5030)  acc5: 96.8750 (88.7957)  time: 0.1413  data: 0.1253  max mem: 4666\n",
      "[20:39:58.745198] [20:39:58.745188] [20:39:58.745298] Test:  [50/68]  eta: 0:00:03  loss: 1.2672 (1.4288)  acc1: 40.6250 (51.5319)  acc5: 100.0000 (90.9926)  time: 0.1388  data: 0.1226  max mem: 4666\n",
      "[20:40:00.120638] [20:40:00.120627] [20:40:00.120696] Test:  [60/68]  eta: 0:00:01  loss: 1.2699 (1.4682)  acc1: 34.3750 (47.2336)  acc5: 100.0000 (92.4693)  time: 0.1379  data: 0.1218  max mem: 4666\n",
      "[20:40:00.280787] [20:40:00.280777] [20:40:00.280804] Test:  [67/68]  eta: 0:00:00  loss: 1.6379 (1.4852)  acc1: 18.7500 (44.4855)  acc5: 100.0000 (93.2164)  time: 0.1389  data: 0.1232  max mem: 4666\n",
      "[20:40:00.344057] [20:40:00.344050] [20:40:00.344139] Test: Total time: 0:00:10 (0.1508 s / it)\n",
      "[20:40:00.344180] [20:40:00.344178] [20:40:00.344187] * Acc@1 44.485 Acc@5 93.216 loss 1.485\n",
      "[20:40:00.344654] [20:40:00.344649] [20:40:00.344664] Accuracy of the network on the 2167 test images: 44.5%\n",
      "[20:40:00.344675] [20:40:00.344673] [20:40:00.344681] Max accuracy: 45.27%\n",
      "[20:40:01.948473] [20:40:01.948454] [20:40:01.948641] Epoch: [8]  [  0/316]  eta: 0:08:26  lr: 0.000249  loss: 1.7572 (1.7572)  time: 1.6021  data: 1.5508  max mem: 4666\n",
      "[20:40:04.401056] [20:40:04.401047] [20:40:04.401117] Epoch: [8]  [ 20/316]  eta: 0:00:57  lr: 0.000249  loss: 1.6803 (1.7172)  time: 0.1226  data: 0.0709  max mem: 4666\n",
      "[20:40:06.804098] [20:40:06.804087] [20:40:06.804153] Epoch: [8]  [ 40/316]  eta: 0:00:43  lr: 0.000249  loss: 1.7896 (1.7549)  time: 0.1201  data: 0.0685  max mem: 4666\n",
      "[20:40:09.300064] [20:40:09.300053] [20:40:09.300140] Epoch: [8]  [ 60/316]  eta: 0:00:37  lr: 0.000249  loss: 1.7546 (1.7568)  time: 0.1247  data: 0.0731  max mem: 4666\n",
      "[20:40:11.787725] [20:40:11.787715] [20:40:11.787785] Epoch: [8]  [ 80/316]  eta: 0:00:33  lr: 0.000249  loss: 1.7549 (1.7588)  time: 0.1243  data: 0.0727  max mem: 4666\n",
      "[20:40:14.245898] [20:40:14.245888] [20:40:14.245957] Epoch: [8]  [100/316]  eta: 0:00:29  lr: 0.000249  loss: 1.7700 (1.7594)  time: 0.1229  data: 0.0712  max mem: 4666\n",
      "[20:40:16.745962] [20:40:16.745951] [20:40:16.746020] Epoch: [8]  [120/316]  eta: 0:00:26  lr: 0.000249  loss: 1.7575 (1.7631)  time: 0.1250  data: 0.0734  max mem: 4666\n",
      "[20:40:19.214336] [20:40:19.214322] [20:40:19.214631] Epoch: [8]  [140/316]  eta: 0:00:23  lr: 0.000249  loss: 1.7977 (1.7686)  time: 0.1234  data: 0.0718  max mem: 4666\n",
      "[20:40:21.700924] [20:40:21.700913] [20:40:21.700976] Epoch: [8]  [160/316]  eta: 0:00:20  lr: 0.000249  loss: 1.7531 (1.7671)  time: 0.1243  data: 0.0729  max mem: 4666\n",
      "[20:40:24.162164] [20:40:24.162152] [20:40:24.162217] Epoch: [8]  [180/316]  eta: 0:00:17  lr: 0.000249  loss: 1.7713 (1.7668)  time: 0.1230  data: 0.0716  max mem: 4666\n",
      "[20:40:26.667056] [20:40:26.667046] [20:40:26.667112] Epoch: [8]  [200/316]  eta: 0:00:15  lr: 0.000249  loss: 1.7475 (1.7657)  time: 0.1252  data: 0.0738  max mem: 4666\n",
      "[20:40:29.089353] [20:40:29.089343] [20:40:29.089408] Epoch: [8]  [220/316]  eta: 0:00:12  lr: 0.000249  loss: 1.7367 (1.7634)  time: 0.1211  data: 0.0695  max mem: 4666\n",
      "[20:40:31.532276] [20:40:31.532265] [20:40:31.532334] Epoch: [8]  [240/316]  eta: 0:00:09  lr: 0.000249  loss: 1.7784 (1.7630)  time: 0.1221  data: 0.0705  max mem: 4666\n",
      "[20:40:34.035894] [20:40:34.035884] [20:40:34.035947] Epoch: [8]  [260/316]  eta: 0:00:07  lr: 0.000249  loss: 1.7124 (1.7611)  time: 0.1251  data: 0.0735  max mem: 4666\n",
      "[20:40:36.537389] [20:40:36.537377] [20:40:36.537443] Epoch: [8]  [280/316]  eta: 0:00:04  lr: 0.000249  loss: 1.7285 (1.7595)  time: 0.1250  data: 0.0734  max mem: 4666\n",
      "[20:40:38.996890] [20:40:38.996880] [20:40:38.997059] Epoch: [8]  [300/316]  eta: 0:00:02  lr: 0.000249  loss: 1.7236 (1.7581)  time: 0.1229  data: 0.0714  max mem: 4666\n",
      "[20:40:40.460417] [20:40:40.460408] [20:40:40.460461] Epoch: [8]  [315/316]  eta: 0:00:00  lr: 0.000249  loss: 1.7256 (1.7579)  time: 0.1201  data: 0.0685  max mem: 4666\n",
      "[20:40:40.524936] [20:40:40.524927] [20:40:40.524953] Epoch: [8] Total time: 0:00:40 (0.1271 s / it)\n",
      "[20:40:40.525006] [20:40:40.525003] [20:40:40.525012] Averaged stats: lr: 0.000249  loss: 1.7256 (1.7579)\n",
      "[20:40:43.881194] [20:40:43.881155] [20:40:43.881363] Test:  [ 0/68]  eta: 0:01:54  loss: 1.1389 (1.1389)  acc1: 75.0000 (75.0000)  acc5: 100.0000 (100.0000)  time: 1.6894  data: 1.6726  max mem: 4666\n",
      "[20:40:45.250529] [20:40:45.250520] [20:40:45.250586] Test:  [10/68]  eta: 0:00:16  loss: 1.2372 (1.1974)  acc1: 62.5000 (62.7841)  acc5: 100.0000 (97.4432)  time: 0.2780  data: 0.2616  max mem: 4666\n",
      "[20:40:46.683533] [20:40:46.683521] [20:40:46.683607] Test:  [20/68]  eta: 0:00:10  loss: 1.0087 (1.0116)  acc1: 68.7500 (70.2381)  acc5: 100.0000 (98.0655)  time: 0.1400  data: 0.1239  max mem: 4666\n",
      "[20:40:48.079984] [20:40:48.079973] [20:40:48.080219] Test:  [30/68]  eta: 0:00:07  loss: 1.0087 (1.2932)  acc1: 62.5000 (57.8629)  acc5: 100.0000 (89.9194)  time: 0.1414  data: 0.1254  max mem: 4666\n",
      "[20:40:49.472659] [20:40:49.472648] [20:40:49.472718] Test:  [40/68]  eta: 0:00:04  loss: 1.7972 (1.4196)  acc1: 3.1250 (47.1799)  acc5: 96.8750 (91.8445)  time: 0.1394  data: 0.1234  max mem: 4666\n",
      "[20:40:50.857320] [20:40:50.857310] [20:40:50.857382] Test:  [50/68]  eta: 0:00:03  loss: 1.4032 (1.3965)  acc1: 34.3750 (48.5294)  acc5: 100.0000 (93.4436)  time: 0.1388  data: 0.1229  max mem: 4666\n",
      "[20:40:52.238702] [20:40:52.238692] [20:40:52.238757] Test:  [60/68]  eta: 0:00:01  loss: 1.4407 (1.4343)  acc1: 34.3750 (45.9016)  acc5: 100.0000 (94.5184)  time: 0.1382  data: 0.1223  max mem: 4666\n",
      "[20:40:52.455050] [20:40:52.455040] [20:40:52.455101] Test:  [67/68]  eta: 0:00:00  loss: 1.5197 (1.4415)  acc1: 40.6250 (47.3004)  acc5: 100.0000 (95.0623)  time: 0.1390  data: 0.1233  max mem: 4666\n",
      "[20:40:52.528955] [20:40:52.528947] [20:40:52.528974] Test: Total time: 0:00:10 (0.1520 s / it)\n",
      "[20:40:52.529009] [20:40:52.529007] [20:40:52.529015] * Acc@1 47.300 Acc@5 95.062 loss 1.442\n",
      "[20:40:52.529108] [20:40:52.529106] [20:40:52.529115] Accuracy of the network on the 2167 test images: 47.3%\n",
      "[20:40:52.529125] [20:40:52.529123] [20:40:52.529130] Max accuracy: 47.30%\n",
      "[20:40:54.140382] [20:40:54.140363] [20:40:54.140523] Epoch: [9]  [  0/316]  eta: 0:08:28  lr: 0.000249  loss: 1.6863 (1.6863)  time: 1.6099  data: 1.5598  max mem: 4666\n",
      "[20:40:56.587227] [20:40:56.587216] [20:40:56.587284] Epoch: [9]  [ 20/316]  eta: 0:00:57  lr: 0.000249  loss: 1.6921 (1.6964)  time: 0.1223  data: 0.0706  max mem: 4666\n",
      "[20:40:59.081440] [20:40:59.081429] [20:40:59.081492] Epoch: [9]  [ 40/316]  eta: 0:00:44  lr: 0.000249  loss: 1.7337 (1.7225)  time: 0.1247  data: 0.0731  max mem: 4666\n",
      "[20:41:01.518012] [20:41:01.518000] [20:41:01.518075] Epoch: [9]  [ 60/316]  eta: 0:00:37  lr: 0.000249  loss: 1.7373 (1.7313)  time: 0.1218  data: 0.0703  max mem: 4666\n",
      "[20:41:03.966799] [20:41:03.966789] [20:41:03.966854] Epoch: [9]  [ 80/316]  eta: 0:00:33  lr: 0.000249  loss: 1.7500 (1.7380)  time: 0.1224  data: 0.0710  max mem: 4666\n",
      "[20:41:06.395073] [20:41:06.395061] [20:41:06.395133] Epoch: [9]  [100/316]  eta: 0:00:29  lr: 0.000249  loss: 1.7552 (1.7448)  time: 0.1214  data: 0.0699  max mem: 4666\n",
      "[20:41:08.879400] [20:41:08.879390] [20:41:08.879476] Epoch: [9]  [120/316]  eta: 0:00:26  lr: 0.000249  loss: 1.7076 (1.7460)  time: 0.1242  data: 0.0728  max mem: 4666\n",
      "[20:41:11.327619] [20:41:11.327607] [20:41:11.327679] Epoch: [9]  [140/316]  eta: 0:00:23  lr: 0.000249  loss: 1.7933 (1.7500)  time: 0.1224  data: 0.0710  max mem: 4666\n",
      "[20:41:13.743883] [20:41:13.743872] [20:41:13.743941] Epoch: [9]  [160/316]  eta: 0:00:20  lr: 0.000249  loss: 1.7209 (1.7458)  time: 0.1208  data: 0.0693  max mem: 4666\n",
      "[20:41:16.244693] [20:41:16.244681] [20:41:16.244763] Epoch: [9]  [180/316]  eta: 0:00:17  lr: 0.000249  loss: 1.7095 (1.7451)  time: 0.1250  data: 0.0736  max mem: 4666\n",
      "[20:41:18.660270] [20:41:18.660259] [20:41:18.660336] Epoch: [9]  [200/316]  eta: 0:00:15  lr: 0.000249  loss: 1.7450 (1.7455)  time: 0.1207  data: 0.0691  max mem: 4666\n",
      "[20:41:21.157413] [20:41:21.157402] [20:41:21.157550] Epoch: [9]  [220/316]  eta: 0:00:12  lr: 0.000249  loss: 1.6816 (1.7432)  time: 0.1248  data: 0.0731  max mem: 4666\n",
      "[20:41:23.616645] [20:41:23.616634] [20:41:23.616698] Epoch: [9]  [240/316]  eta: 0:00:09  lr: 0.000248  loss: 1.7541 (1.7415)  time: 0.1229  data: 0.0713  max mem: 4666\n",
      "[20:41:26.062122] [20:41:26.062111] [20:41:26.062185] Epoch: [9]  [260/316]  eta: 0:00:07  lr: 0.000248  loss: 1.7628 (1.7416)  time: 0.1222  data: 0.0706  max mem: 4666\n",
      "[20:41:28.536284] [20:41:28.536272] [20:41:28.536336] Epoch: [9]  [280/316]  eta: 0:00:04  lr: 0.000248  loss: 1.7273 (1.7422)  time: 0.1237  data: 0.0722  max mem: 4666\n",
      "[20:41:30.995140] [20:41:30.995130] [20:41:30.995190] Epoch: [9]  [300/316]  eta: 0:00:02  lr: 0.000248  loss: 1.7360 (1.7405)  time: 0.1229  data: 0.0714  max mem: 4666\n",
      "[20:41:32.453826] [20:41:32.453817] [20:41:32.453872] Epoch: [9]  [315/316]  eta: 0:00:00  lr: 0.000248  loss: 1.6875 (1.7405)  time: 0.1217  data: 0.0704  max mem: 4666\n",
      "[20:41:32.525376] [20:41:32.525367] [20:41:32.525394] Epoch: [9] Total time: 0:00:39 (0.1266 s / it)\n",
      "[20:41:32.525452] [20:41:32.525449] [20:41:32.525458] Averaged stats: lr: 0.000248  loss: 1.6875 (1.7405)\n",
      "[20:41:35.935789] [20:41:35.935761] [20:41:35.935995] Test:  [ 0/68]  eta: 0:01:57  loss: 1.1385 (1.1385)  acc1: 75.0000 (75.0000)  acc5: 96.8750 (96.8750)  time: 1.7343  data: 1.7170  max mem: 4666\n",
      "[20:41:37.297547] [20:41:37.297536] [20:41:37.297613] Test:  [10/68]  eta: 0:00:16  loss: 1.2346 (1.2439)  acc1: 65.6250 (60.7955)  acc5: 96.8750 (95.4545)  time: 0.2814  data: 0.2650  max mem: 4666\n",
      "[20:41:38.706710] [20:41:38.706699] [20:41:38.706767] Test:  [20/68]  eta: 0:00:10  loss: 1.0991 (1.1045)  acc1: 65.6250 (66.3690)  acc5: 100.0000 (97.0238)  time: 0.1385  data: 0.1223  max mem: 4666\n",
      "[20:41:40.107252] [20:41:40.107241] [20:41:40.107306] Test:  [30/68]  eta: 0:00:07  loss: 1.1425 (1.3336)  acc1: 50.0000 (55.7460)  acc5: 100.0000 (90.8266)  time: 0.1404  data: 0.1245  max mem: 4666\n",
      "[20:41:41.498296] [20:41:41.498285] [20:41:41.498354] Test:  [40/68]  eta: 0:00:04  loss: 1.6447 (1.4478)  acc1: 9.3750 (46.4939)  acc5: 96.8750 (91.9970)  time: 0.1395  data: 0.1235  max mem: 4666\n",
      "[20:41:42.881200] [20:41:42.881190] [20:41:42.881250] Test:  [50/68]  eta: 0:00:03  loss: 1.3620 (1.4035)  acc1: 40.6250 (50.4902)  acc5: 100.0000 (93.5662)  time: 0.1386  data: 0.1227  max mem: 4666\n",
      "[20:41:44.262828] [20:41:44.262817] [20:41:44.262885] Test:  [60/68]  eta: 0:00:01  loss: 1.3834 (1.4251)  acc1: 46.8750 (49.1291)  acc5: 100.0000 (94.6209)  time: 0.1382  data: 0.1222  max mem: 4666\n",
      "[20:41:44.380150] [20:41:44.380143] [20:41:44.380163] Test:  [67/68]  eta: 0:00:00  loss: 1.4248 (1.4280)  acc1: 43.7500 (49.8385)  acc5: 100.0000 (95.1546)  time: 0.1367  data: 0.1210  max mem: 4666\n",
      "[20:41:44.451101] [20:41:44.451093] [20:41:44.451116] Test: Total time: 0:00:10 (0.1507 s / it)\n",
      "[20:41:44.451151] [20:41:44.451149] [20:41:44.451157] * Acc@1 49.838 Acc@5 95.155 loss 1.428\n",
      "[20:41:44.451380] [20:41:44.451375] [20:41:44.451388] Accuracy of the network on the 2167 test images: 49.8%\n",
      "[20:41:44.451399] [20:41:44.451397] [20:41:44.451405] Max accuracy: 49.84%\n",
      "[20:41:46.017751] [20:41:46.017732] [20:41:46.017909] Epoch: [10]  [  0/316]  eta: 0:08:14  lr: 0.000248  loss: 1.5662 (1.5662)  time: 1.5650  data: 1.5142  max mem: 4666\n",
      "[20:41:48.454440] [20:41:48.454429] [20:41:48.454495] Epoch: [10]  [ 20/316]  eta: 0:00:56  lr: 0.000248  loss: 1.6693 (1.7080)  time: 0.1218  data: 0.0703  max mem: 4666\n",
      "[20:41:50.937652] [20:41:50.937642] [20:41:50.937708] Epoch: [10]  [ 40/316]  eta: 0:00:43  lr: 0.000248  loss: 1.7499 (1.7310)  time: 0.1241  data: 0.0727  max mem: 4666\n",
      "[20:41:53.343746] [20:41:53.343735] [20:41:53.343802] Epoch: [10]  [ 60/316]  eta: 0:00:37  lr: 0.000248  loss: 1.7782 (1.7363)  time: 0.1203  data: 0.0688  max mem: 4666\n",
      "[20:41:55.831473] [20:41:55.831463] [20:41:55.831604] Epoch: [10]  [ 80/316]  eta: 0:00:33  lr: 0.000248  loss: 1.7460 (1.7457)  time: 0.1243  data: 0.0729  max mem: 4666\n",
      "[20:41:58.314680] [20:41:58.314670] [20:41:58.314736] Epoch: [10]  [100/316]  eta: 0:00:29  lr: 0.000248  loss: 1.7292 (1.7454)  time: 0.1241  data: 0.0726  max mem: 4666\n",
      "[20:42:00.809949] [20:42:00.809939] [20:42:00.810028] Epoch: [10]  [120/316]  eta: 0:00:26  lr: 0.000248  loss: 1.7199 (1.7456)  time: 0.1247  data: 0.0732  max mem: 4666\n",
      "[20:42:03.242445] [20:42:03.242435] [20:42:03.242526] Epoch: [10]  [140/316]  eta: 0:00:23  lr: 0.000248  loss: 1.7753 (1.7512)  time: 0.1216  data: 0.0700  max mem: 4666\n",
      "[20:42:05.674915] [20:42:05.674905] [20:42:05.674970] Epoch: [10]  [160/316]  eta: 0:00:20  lr: 0.000248  loss: 1.7427 (1.7485)  time: 0.1216  data: 0.0699  max mem: 4666\n",
      "[20:42:08.140309] [20:42:08.140299] [20:42:08.140361] Epoch: [10]  [180/316]  eta: 0:00:17  lr: 0.000248  loss: 1.7151 (1.7466)  time: 0.1232  data: 0.0715  max mem: 4666\n",
      "[20:42:10.633498] [20:42:10.633488] [20:42:10.633574] Epoch: [10]  [200/316]  eta: 0:00:15  lr: 0.000248  loss: 1.6840 (1.7431)  time: 0.1246  data: 0.0728  max mem: 4666\n",
      "[20:42:13.081268] [20:42:13.081258] [20:42:13.081318] Epoch: [10]  [220/316]  eta: 0:00:12  lr: 0.000248  loss: 1.7318 (1.7399)  time: 0.1223  data: 0.0706  max mem: 4666\n",
      "[20:42:15.517753] [20:42:15.517742] [20:42:15.517805] Epoch: [10]  [240/316]  eta: 0:00:09  lr: 0.000248  loss: 1.7478 (1.7383)  time: 0.1218  data: 0.0700  max mem: 4666\n",
      "[20:42:18.011761] [20:42:18.011750] [20:42:18.011819] Epoch: [10]  [260/316]  eta: 0:00:07  lr: 0.000248  loss: 1.6810 (1.7389)  time: 0.1247  data: 0.0729  max mem: 4666\n",
      "[20:42:20.441737] [20:42:20.441726] [20:42:20.441792] Epoch: [10]  [280/316]  eta: 0:00:04  lr: 0.000248  loss: 1.6950 (1.7371)  time: 0.1215  data: 0.0701  max mem: 4666\n",
      "[20:42:22.885875] [20:42:22.885865] [20:42:22.885931] Epoch: [10]  [300/316]  eta: 0:00:02  lr: 0.000248  loss: 1.7137 (1.7359)  time: 0.1222  data: 0.0708  max mem: 4666\n",
      "[20:42:24.390990] [20:42:24.390981] [20:42:24.391040] Epoch: [10]  [315/316]  eta: 0:00:00  lr: 0.000248  loss: 1.7025 (1.7359)  time: 0.1217  data: 0.0705  max mem: 4666\n",
      "[20:42:24.463433] [20:42:24.463420] [20:42:24.463461] Epoch: [10] Total time: 0:00:40 (0.1266 s / it)\n",
      "[20:42:24.463543] [20:42:24.463538] [20:42:24.463556] Averaged stats: lr: 0.000248  loss: 1.7025 (1.7359)\n",
      "[20:42:27.852724] [20:42:27.852691] [20:42:27.852897] Test:  [ 0/68]  eta: 0:01:56  loss: 1.2286 (1.2286)  acc1: 71.8750 (71.8750)  acc5: 96.8750 (96.8750)  time: 1.7100  data: 1.6931  max mem: 4666\n",
      "[20:42:29.245477] [20:42:29.245466] [20:42:29.245523] Test:  [10/68]  eta: 0:00:16  loss: 1.3394 (1.2781)  acc1: 59.3750 (58.8068)  acc5: 100.0000 (99.4318)  time: 0.2820  data: 0.2656  max mem: 4666\n",
      "[20:42:30.637970] [20:42:30.637960] [20:42:30.638050] Test:  [20/68]  eta: 0:00:10  loss: 1.1021 (1.1230)  acc1: 65.6250 (67.1131)  acc5: 100.0000 (99.1071)  time: 0.1392  data: 0.1230  max mem: 4666\n",
      "[20:42:32.047698] [20:42:32.047687] [20:42:32.047756] Test:  [30/68]  eta: 0:00:07  loss: 1.1172 (1.3181)  acc1: 68.7500 (60.1815)  acc5: 100.0000 (92.2379)  time: 0.1400  data: 0.1241  max mem: 4666\n",
      "[20:42:33.450447] [20:42:33.450436] [20:42:33.450503] Test:  [40/68]  eta: 0:00:04  loss: 1.5654 (1.4323)  acc1: 18.7500 (50.4573)  acc5: 90.6250 (91.3110)  time: 0.1405  data: 0.1246  max mem: 4666\n",
      "[20:42:34.833195] [20:42:34.833184] [20:42:34.833286] Test:  [50/68]  eta: 0:00:03  loss: 1.5422 (1.4192)  acc1: 25.0000 (49.2034)  acc5: 100.0000 (93.0147)  time: 0.1392  data: 0.1232  max mem: 4666\n",
      "[20:42:36.204539] [20:42:36.204529] [20:42:36.204589] Test:  [60/68]  eta: 0:00:01  loss: 1.4730 (1.4273)  acc1: 43.7500 (48.7193)  acc5: 100.0000 (94.1598)  time: 0.1376  data: 0.1217  max mem: 4666\n",
      "[20:42:36.360149] [20:42:36.360144] [20:42:36.360160] Test:  [67/68]  eta: 0:00:00  loss: 1.4245 (1.4278)  acc1: 46.8750 (49.7000)  acc5: 100.0000 (94.7393)  time: 0.1385  data: 0.1228  max mem: 4666\n",
      "[20:42:36.432582] [20:42:36.432578] [20:42:36.432608] Test: Total time: 0:00:10 (0.1513 s / it)\n",
      "[20:42:36.432642] [20:42:36.432640] [20:42:36.432648] * Acc@1 49.700 Acc@5 94.739 loss 1.428\n",
      "[20:42:36.432856] [20:42:36.432852] [20:42:36.432865] Accuracy of the network on the 2167 test images: 49.7%\n",
      "[20:42:36.432874] [20:42:36.432872] [20:42:36.432879] Max accuracy: 49.84%\n",
      "[20:42:37.992459] [20:42:37.992435] [20:42:37.992622] Epoch: [11]  [  0/316]  eta: 0:08:12  lr: 0.000248  loss: 1.5789 (1.5789)  time: 1.5584  data: 1.5081  max mem: 4666\n",
      "[20:42:40.399807] [20:42:40.399795] [20:42:40.399886] Epoch: [11]  [ 20/316]  eta: 0:00:55  lr: 0.000248  loss: 1.6872 (1.6875)  time: 0.1203  data: 0.0688  max mem: 4666\n",
      "[20:42:42.869303] [20:42:42.869292] [20:42:42.869424] Epoch: [11]  [ 40/316]  eta: 0:00:43  lr: 0.000247  loss: 1.7244 (1.7060)  time: 0.1234  data: 0.0720  max mem: 4666\n",
      "[20:42:45.284351] [20:42:45.284341] [20:42:45.284410] Epoch: [11]  [ 60/316]  eta: 0:00:37  lr: 0.000247  loss: 1.7518 (1.7166)  time: 0.1207  data: 0.0690  max mem: 4666\n",
      "[20:42:47.735307] [20:42:47.735296] [20:42:47.735367] Epoch: [11]  [ 80/316]  eta: 0:00:32  lr: 0.000247  loss: 1.7169 (1.7146)  time: 0.1225  data: 0.0709  max mem: 4666\n",
      "[20:42:50.280560] [20:42:50.280548] [20:42:50.280618] Epoch: [11]  [100/316]  eta: 0:00:29  lr: 0.000247  loss: 1.7206 (1.7220)  time: 0.1272  data: 0.0756  max mem: 4666\n",
      "[20:42:52.776001] [20:42:52.775991] [20:42:52.776060] Epoch: [11]  [120/316]  eta: 0:00:26  lr: 0.000247  loss: 1.7510 (1.7275)  time: 0.1247  data: 0.0732  max mem: 4666\n",
      "[20:42:55.249673] [20:42:55.249662] [20:42:55.249727] Epoch: [11]  [140/316]  eta: 0:00:23  lr: 0.000247  loss: 1.7754 (1.7338)  time: 0.1236  data: 0.0721  max mem: 4666\n",
      "[20:42:57.719370] [20:42:57.719359] [20:42:57.719425] Epoch: [11]  [160/316]  eta: 0:00:20  lr: 0.000247  loss: 1.7117 (1.7322)  time: 0.1234  data: 0.0718  max mem: 4666\n",
      "[20:43:00.204103] [20:43:00.204091] [20:43:00.204162] Epoch: [11]  [180/316]  eta: 0:00:17  lr: 0.000247  loss: 1.7087 (1.7345)  time: 0.1242  data: 0.0726  max mem: 4666\n",
      "[20:43:02.642141] [20:43:02.642131] [20:43:02.642401] Epoch: [11]  [200/316]  eta: 0:00:15  lr: 0.000247  loss: 1.7085 (1.7320)  time: 0.1218  data: 0.0702  max mem: 4666\n",
      "[20:43:05.104219] [20:43:05.104207] [20:43:05.104348] Epoch: [11]  [220/316]  eta: 0:00:12  lr: 0.000247  loss: 1.7259 (1.7305)  time: 0.1230  data: 0.0714  max mem: 4666\n",
      "[20:43:07.516238] [20:43:07.516228] [20:43:07.516296] Epoch: [11]  [240/316]  eta: 0:00:09  lr: 0.000247  loss: 1.7279 (1.7297)  time: 0.1206  data: 0.0689  max mem: 4666\n",
      "[20:43:10.005828] [20:43:10.005818] [20:43:10.005914] Epoch: [11]  [260/316]  eta: 0:00:07  lr: 0.000247  loss: 1.7694 (1.7290)  time: 0.1244  data: 0.0728  max mem: 4666\n",
      "[20:43:12.485878] [20:43:12.485867] [20:43:12.485935] Epoch: [11]  [280/316]  eta: 0:00:04  lr: 0.000247  loss: 1.7219 (1.7316)  time: 0.1239  data: 0.0723  max mem: 4666\n",
      "[20:43:14.977645] [20:43:14.977635] [20:43:14.977700] Epoch: [11]  [300/316]  eta: 0:00:02  lr: 0.000247  loss: 1.6785 (1.7297)  time: 0.1245  data: 0.0729  max mem: 4666\n",
      "[20:43:16.473213] [20:43:16.473204] [20:43:16.473259] Epoch: [11]  [315/316]  eta: 0:00:00  lr: 0.000247  loss: 1.7097 (1.7296)  time: 0.1237  data: 0.0723  max mem: 4666\n",
      "[20:43:16.543907] [20:43:16.543901] [20:43:16.543918] Epoch: [11] Total time: 0:00:40 (0.1269 s / it)\n",
      "[20:43:16.543948] [20:43:16.543946] [20:43:16.543954] Averaged stats: lr: 0.000247  loss: 1.7097 (1.7296)\n",
      "[20:43:19.886926] [20:43:19.886908] [20:43:19.887096] Test:  [ 0/68]  eta: 0:01:55  loss: 1.2917 (1.2917)  acc1: 59.3750 (59.3750)  acc5: 96.8750 (96.8750)  time: 1.7012  data: 1.6841  max mem: 4666\n",
      "[20:43:21.305065] [20:43:21.305055] [20:43:21.305130] Test:  [10/68]  eta: 0:00:16  loss: 1.3931 (1.2856)  acc1: 56.2500 (54.8295)  acc5: 100.0000 (97.7273)  time: 0.2835  data: 0.2671  max mem: 4666\n",
      "[20:43:22.701444] [20:43:22.701429] [20:43:22.701505] Test:  [20/68]  eta: 0:00:10  loss: 0.9183 (1.0489)  acc1: 71.8750 (67.8571)  acc5: 100.0000 (98.5119)  time: 0.1406  data: 0.1245  max mem: 4666\n",
      "[20:43:24.119257] [20:43:24.119245] [20:43:24.119354] Test:  [30/68]  eta: 0:00:07  loss: 0.9316 (1.3105)  acc1: 65.6250 (55.1411)  acc5: 100.0000 (90.4234)  time: 0.1406  data: 0.1247  max mem: 4666\n",
      "[20:43:25.504695] [20:43:25.504685] [20:43:25.504745] Test:  [40/68]  eta: 0:00:04  loss: 1.6396 (1.3953)  acc1: 18.7500 (47.7896)  acc5: 96.8750 (91.6159)  time: 0.1401  data: 0.1241  max mem: 4666\n",
      "[20:43:26.893518] [20:43:26.893507] [20:43:26.893587] Test:  [50/68]  eta: 0:00:03  loss: 1.4871 (1.3744)  acc1: 31.2500 (50.5515)  acc5: 100.0000 (93.1985)  time: 0.1386  data: 0.1227  max mem: 4666\n",
      "[20:43:28.279718] [20:43:28.279708] [20:43:28.279820] Test:  [60/68]  eta: 0:00:01  loss: 1.4821 (1.3908)  acc1: 50.0000 (50.1537)  acc5: 100.0000 (94.3135)  time: 0.1387  data: 0.1227  max mem: 4666\n",
      "[20:43:28.415278] [20:43:28.415273] [20:43:28.415290] Test:  [67/68]  eta: 0:00:00  loss: 1.3580 (1.3902)  acc1: 53.1250 (52.6996)  acc5: 100.0000 (94.8777)  time: 0.1386  data: 0.1229  max mem: 4666\n",
      "[20:43:28.482656] [20:43:28.482649] [20:43:28.482681] Test: Total time: 0:00:10 (0.1514 s / it)\n",
      "[20:43:28.482717] [20:43:28.482714] [20:43:28.482723] * Acc@1 52.700 Acc@5 94.878 loss 1.390\n",
      "[20:43:28.482873] [20:43:28.482869] [20:43:28.482880] Accuracy of the network on the 2167 test images: 52.7%\n",
      "[20:43:28.482890] [20:43:28.482889] [20:43:28.482896] Max accuracy: 52.70%\n",
      "[20:43:30.084175] [20:43:30.084156] [20:43:30.084340] Epoch: [12]  [  0/316]  eta: 0:08:25  lr: 0.000247  loss: 1.6650 (1.6650)  time: 1.5998  data: 1.5483  max mem: 4666\n",
      "[20:43:32.554289] [20:43:32.554278] [20:43:32.554340] Epoch: [12]  [ 20/316]  eta: 0:00:57  lr: 0.000247  loss: 1.6672 (1.6947)  time: 0.1235  data: 0.0717  max mem: 4666\n",
      "[20:43:35.084135] [20:43:35.084125] [20:43:35.084183] Epoch: [12]  [ 40/316]  eta: 0:00:44  lr: 0.000247  loss: 1.7437 (1.7171)  time: 0.1264  data: 0.0746  max mem: 4666\n",
      "[20:43:37.528980] [20:43:37.528969] [20:43:37.529031] Epoch: [12]  [ 60/316]  eta: 0:00:37  lr: 0.000246  loss: 1.7466 (1.7257)  time: 0.1222  data: 0.0706  max mem: 4666\n",
      "[20:43:39.962187] [20:43:39.962177] [20:43:39.962243] Epoch: [12]  [ 80/316]  eta: 0:00:33  lr: 0.000246  loss: 1.7161 (1.7248)  time: 0.1216  data: 0.0700  max mem: 4666\n",
      "[20:43:42.442246] [20:43:42.442236] [20:43:42.442300] Epoch: [12]  [100/316]  eta: 0:00:29  lr: 0.000246  loss: 1.6696 (1.7193)  time: 0.1240  data: 0.0724  max mem: 4666\n",
      "[20:43:44.896106] [20:43:44.896096] [20:43:44.896160] Epoch: [12]  [120/316]  eta: 0:00:26  lr: 0.000246  loss: 1.6715 (1.7167)  time: 0.1226  data: 0.0710  max mem: 4666\n",
      "[20:43:47.381898] [20:43:47.381887] [20:43:47.382016] Epoch: [12]  [140/316]  eta: 0:00:23  lr: 0.000246  loss: 1.7272 (1.7227)  time: 0.1242  data: 0.0726  max mem: 4666\n",
      "[20:43:49.864891] [20:43:49.864880] [20:43:49.864944] Epoch: [12]  [160/316]  eta: 0:00:20  lr: 0.000246  loss: 1.6448 (1.7155)  time: 0.1241  data: 0.0725  max mem: 4666\n",
      "[20:43:52.305125] [20:43:52.305116] [20:43:52.305175] Epoch: [12]  [180/316]  eta: 0:00:17  lr: 0.000246  loss: 1.6986 (1.7169)  time: 0.1220  data: 0.0704  max mem: 4666\n",
      "[20:43:54.772973] [20:43:54.772962] [20:43:54.773029] Epoch: [12]  [200/316]  eta: 0:00:15  lr: 0.000246  loss: 1.7034 (1.7177)  time: 0.1233  data: 0.0717  max mem: 4666\n",
      "[20:43:57.219089] [20:43:57.219077] [20:43:57.219152] Epoch: [12]  [220/316]  eta: 0:00:12  lr: 0.000246  loss: 1.6766 (1.7148)  time: 0.1222  data: 0.0706  max mem: 4666\n",
      "[20:43:59.687998] [20:43:59.687987] [20:43:59.688054] Epoch: [12]  [240/316]  eta: 0:00:09  lr: 0.000246  loss: 1.7074 (1.7157)  time: 0.1234  data: 0.0717  max mem: 4666\n",
      "[20:44:02.132359] [20:44:02.132348] [20:44:02.132413] Epoch: [12]  [260/316]  eta: 0:00:07  lr: 0.000246  loss: 1.7479 (1.7170)  time: 0.1222  data: 0.0705  max mem: 4666\n",
      "[20:44:04.610952] [20:44:04.610941] [20:44:04.611005] Epoch: [12]  [280/316]  eta: 0:00:04  lr: 0.000246  loss: 1.7183 (1.7190)  time: 0.1239  data: 0.0723  max mem: 4666\n",
      "[20:44:07.118579] [20:44:07.118569] [20:44:07.118628] Epoch: [12]  [300/316]  eta: 0:00:02  lr: 0.000246  loss: 1.6747 (1.7166)  time: 0.1253  data: 0.0737  max mem: 4666\n",
      "[20:44:08.671572] [20:44:08.671563] [20:44:08.671616] Epoch: [12]  [315/316]  eta: 0:00:00  lr: 0.000246  loss: 1.6434 (1.7147)  time: 0.1214  data: 0.0699  max mem: 4666\n",
      "[20:44:08.742702] [20:44:08.742692] [20:44:08.742719] Epoch: [12] Total time: 0:00:40 (0.1274 s / it)\n",
      "[20:44:08.742778] [20:44:08.742776] [20:44:08.742784] Averaged stats: lr: 0.000246  loss: 1.6434 (1.7147)\n",
      "[20:44:12.100012] [20:44:12.099988] [20:44:12.100204] Test:  [ 0/68]  eta: 0:01:56  loss: 1.2199 (1.2199)  acc1: 62.5000 (62.5000)  acc5: 96.8750 (96.8750)  time: 1.7149  data: 1.6979  max mem: 4666\n",
      "[20:44:13.468899] [20:44:13.468888] [20:44:13.468956] Test:  [10/68]  eta: 0:00:16  loss: 1.3371 (1.2610)  acc1: 62.5000 (56.5341)  acc5: 100.0000 (98.5795)  time: 0.2803  data: 0.2640  max mem: 4666\n",
      "[20:44:14.899434] [20:44:14.899423] [20:44:14.899508] Test:  [20/68]  eta: 0:00:10  loss: 0.8345 (1.0056)  acc1: 68.7500 (68.1548)  acc5: 100.0000 (98.8095)  time: 0.1399  data: 0.1238  max mem: 4666\n",
      "[20:44:16.309056] [20:44:16.309045] [20:44:16.309145] Test:  [30/68]  eta: 0:00:07  loss: 0.8345 (1.2495)  acc1: 65.6250 (56.8548)  acc5: 100.0000 (91.9355)  time: 0.1419  data: 0.1260  max mem: 4666\n",
      "[20:44:17.699884] [20:44:17.699873] [20:44:17.699942] Test:  [40/68]  eta: 0:00:04  loss: 1.6332 (1.3508)  acc1: 25.0000 (49.9238)  acc5: 93.7500 (92.4543)  time: 0.1399  data: 0.1240  max mem: 4666\n",
      "[20:44:19.069850] [20:44:19.069840] [20:44:19.069920] Test:  [50/68]  eta: 0:00:03  loss: 1.4900 (1.3351)  acc1: 34.3750 (51.8995)  acc5: 100.0000 (93.9338)  time: 0.1380  data: 0.1219  max mem: 4666\n",
      "[20:44:20.459008] [20:44:20.458998] [20:44:20.459093] Test:  [60/68]  eta: 0:00:01  loss: 1.3640 (1.3451)  acc1: 56.2500 (51.9980)  acc5: 100.0000 (94.9283)  time: 0.1379  data: 0.1218  max mem: 4666\n",
      "[20:44:20.574304] [20:44:20.574297] [20:44:20.574315] Test:  [67/68]  eta: 0:00:00  loss: 1.3337 (1.3451)  acc1: 62.5000 (53.7148)  acc5: 100.0000 (95.4315)  time: 0.1379  data: 0.1222  max mem: 4666\n",
      "[20:44:20.643322] [20:44:20.643311] [20:44:20.643339] Test: Total time: 0:00:10 (0.1509 s / it)\n",
      "[20:44:20.643380] [20:44:20.643378] [20:44:20.643386] * Acc@1 53.715 Acc@5 95.431 loss 1.345\n",
      "[20:44:20.643494] [20:44:20.643491] [20:44:20.643501] Accuracy of the network on the 2167 test images: 53.7%\n",
      "[20:44:20.643512] [20:44:20.643510] [20:44:20.643517] Max accuracy: 53.71%\n",
      "[20:44:22.223288] [20:44:22.223270] [20:44:22.223435] Epoch: [13]  [  0/316]  eta: 0:08:18  lr: 0.000246  loss: 1.6624 (1.6624)  time: 1.5783  data: 1.5277  max mem: 4666\n",
      "[20:44:24.690170] [20:44:24.690161] [20:44:24.690224] Epoch: [13]  [ 20/316]  eta: 0:00:57  lr: 0.000246  loss: 1.7040 (1.6840)  time: 0.1233  data: 0.0716  max mem: 4666\n",
      "[20:44:27.180386] [20:44:27.180376] [20:44:27.180454] Epoch: [13]  [ 40/316]  eta: 0:00:43  lr: 0.000246  loss: 1.6979 (1.6912)  time: 0.1245  data: 0.0728  max mem: 4666\n",
      "[20:44:29.630173] [20:44:29.630162] [20:44:29.630234] Epoch: [13]  [ 60/316]  eta: 0:00:37  lr: 0.000245  loss: 1.7511 (1.7025)  time: 0.1224  data: 0.0708  max mem: 4666\n",
      "[20:44:32.096346] [20:44:32.096335] [20:44:32.096403] Epoch: [13]  [ 80/316]  eta: 0:00:33  lr: 0.000245  loss: 1.7388 (1.7147)  time: 0.1233  data: 0.0716  max mem: 4666\n",
      "[20:44:34.545445] [20:44:34.545435] [20:44:34.545508] Epoch: [13]  [100/316]  eta: 0:00:29  lr: 0.000245  loss: 1.7044 (1.7179)  time: 0.1224  data: 0.0708  max mem: 4666\n",
      "[20:44:37.034413] [20:44:37.034402] [20:44:37.034484] Epoch: [13]  [120/316]  eta: 0:00:26  lr: 0.000245  loss: 1.7334 (1.7211)  time: 0.1244  data: 0.0727  max mem: 4666\n",
      "[20:44:39.458378] [20:44:39.458368] [20:44:39.458431] Epoch: [13]  [140/316]  eta: 0:00:23  lr: 0.000245  loss: 1.7569 (1.7263)  time: 0.1212  data: 0.0696  max mem: 4666\n",
      "[20:44:41.941561] [20:44:41.941549] [20:44:41.941618] Epoch: [13]  [160/316]  eta: 0:00:20  lr: 0.000245  loss: 1.6984 (1.7222)  time: 0.1241  data: 0.0727  max mem: 4666\n",
      "[20:44:44.405492] [20:44:44.405481] [20:44:44.405553] Epoch: [13]  [180/316]  eta: 0:00:17  lr: 0.000245  loss: 1.7290 (1.7230)  time: 0.1231  data: 0.0716  max mem: 4666\n",
      "[20:44:46.892263] [20:44:46.892251] [20:44:46.892325] Epoch: [13]  [200/316]  eta: 0:00:15  lr: 0.000245  loss: 1.6804 (1.7186)  time: 0.1243  data: 0.0726  max mem: 4666\n",
      "[20:44:49.339218] [20:44:49.339207] [20:44:49.339274] Epoch: [13]  [220/316]  eta: 0:00:12  lr: 0.000245  loss: 1.6716 (1.7176)  time: 0.1223  data: 0.0707  max mem: 4666\n",
      "[20:44:51.829257] [20:44:51.829246] [20:44:51.829316] Epoch: [13]  [240/316]  eta: 0:00:09  lr: 0.000245  loss: 1.6945 (1.7156)  time: 0.1244  data: 0.0728  max mem: 4666\n",
      "[20:44:54.257763] [20:44:54.257750] [20:44:54.257817] Epoch: [13]  [260/316]  eta: 0:00:07  lr: 0.000245  loss: 1.6960 (1.7148)  time: 0.1214  data: 0.0697  max mem: 4666\n",
      "[20:44:56.696479] [20:44:56.696469] [20:44:56.696536] Epoch: [13]  [280/316]  eta: 0:00:04  lr: 0.000245  loss: 1.6707 (1.7129)  time: 0.1219  data: 0.0702  max mem: 4666\n",
      "[20:44:59.158241] [20:44:59.158231] [20:44:59.158305] Epoch: [13]  [300/316]  eta: 0:00:02  lr: 0.000245  loss: 1.7294 (1.7142)  time: 0.1230  data: 0.0714  max mem: 4666\n",
      "[20:45:00.611904] [20:45:00.611895] [20:45:00.611952] Epoch: [13]  [315/316]  eta: 0:00:00  lr: 0.000245  loss: 1.6490 (1.7130)  time: 0.1201  data: 0.0686  max mem: 4666\n",
      "[20:45:00.678702] [20:45:00.678694] [20:45:00.678718] Epoch: [13] Total time: 0:00:40 (0.1267 s / it)\n",
      "[20:45:00.678768] [20:45:00.678766] [20:45:00.678774] Averaged stats: lr: 0.000245  loss: 1.6490 (1.7130)\n",
      "[20:45:04.076063] [20:45:04.076041] [20:45:04.076224] Test:  [ 0/68]  eta: 0:01:58  loss: 1.2060 (1.2060)  acc1: 62.5000 (62.5000)  acc5: 96.8750 (96.8750)  time: 1.7451  data: 1.7278  max mem: 4666\n",
      "[20:45:05.418023] [20:45:05.418012] [20:45:05.418351] Test:  [10/68]  eta: 0:00:16  loss: 1.3072 (1.2431)  acc1: 56.2500 (56.2500)  acc5: 100.0000 (97.7273)  time: 0.2806  data: 0.2642  max mem: 4666\n",
      "[20:45:06.813227] [20:45:06.813217] [20:45:06.813278] Test:  [20/68]  eta: 0:00:10  loss: 0.8280 (0.9903)  acc1: 75.0000 (69.4940)  acc5: 100.0000 (98.5119)  time: 0.1368  data: 0.1206  max mem: 4666\n",
      "[20:45:08.198939] [20:45:08.198928] [20:45:08.198987] Test:  [30/68]  eta: 0:00:07  loss: 0.8280 (1.2421)  acc1: 71.8750 (57.4597)  acc5: 100.0000 (92.2379)  time: 0.1390  data: 0.1230  max mem: 4666\n",
      "[20:45:09.603080] [20:45:09.603070] [20:45:09.603153] Test:  [40/68]  eta: 0:00:04  loss: 1.7497 (1.3627)  acc1: 18.7500 (47.6372)  acc5: 93.7500 (92.6829)  time: 0.1394  data: 0.1234  max mem: 4666\n",
      "[20:45:10.982582] [20:45:10.982572] [20:45:10.982632] Test:  [50/68]  eta: 0:00:03  loss: 1.5103 (1.3352)  acc1: 21.8750 (49.0196)  acc5: 100.0000 (94.0564)  time: 0.1391  data: 0.1232  max mem: 4666\n",
      "[20:45:12.351572] [20:45:12.351563] [20:45:12.351618] Test:  [60/68]  eta: 0:00:01  loss: 1.2499 (1.3287)  acc1: 56.2500 (50.6148)  acc5: 100.0000 (95.0307)  time: 0.1374  data: 0.1214  max mem: 4666\n",
      "[20:45:12.491192] [20:45:12.491182] [20:45:12.491210] Test:  [67/68]  eta: 0:00:00  loss: 1.2279 (1.3202)  acc1: 65.6250 (53.1149)  acc5: 100.0000 (95.5238)  time: 0.1374  data: 0.1217  max mem: 4666\n",
      "[20:45:12.555205] [20:45:12.555197] [20:45:12.555248] Test: Total time: 0:00:10 (0.1504 s / it)\n",
      "[20:45:12.555289] [20:45:12.555287] [20:45:12.555296] * Acc@1 53.115 Acc@5 95.524 loss 1.320\n",
      "[20:45:12.555488] [20:45:12.555484] [20:45:12.555496] Accuracy of the network on the 2167 test images: 53.1%\n",
      "[20:45:12.555505] [20:45:12.555504] [20:45:12.555511] Max accuracy: 53.71%\n",
      "[20:45:14.112661] [20:45:14.112640] [20:45:14.112835] Epoch: [14]  [  0/316]  eta: 0:08:11  lr: 0.000245  loss: 1.6887 (1.6887)  time: 1.5557  data: 1.5048  max mem: 4666\n",
      "[20:45:16.567725] [20:45:16.567715] [20:45:16.567781] Epoch: [14]  [ 20/316]  eta: 0:00:56  lr: 0.000244  loss: 1.6937 (1.6771)  time: 0.1227  data: 0.0710  max mem: 4666\n",
      "[20:45:19.083453] [20:45:19.083442] [20:45:19.083505] Epoch: [14]  [ 40/316]  eta: 0:00:43  lr: 0.000244  loss: 1.6925 (1.6875)  time: 0.1257  data: 0.0742  max mem: 4666\n",
      "[20:45:21.517897] [20:45:21.517886] [20:45:21.517955] Epoch: [14]  [ 60/316]  eta: 0:00:37  lr: 0.000244  loss: 1.7401 (1.7043)  time: 0.1217  data: 0.0700  max mem: 4666\n",
      "[20:45:23.988720] [20:45:23.988709] [20:45:23.988778] Epoch: [14]  [ 80/316]  eta: 0:00:33  lr: 0.000244  loss: 1.6616 (1.7049)  time: 0.1235  data: 0.0719  max mem: 4666\n",
      "[20:45:26.506057] [20:45:26.506047] [20:45:26.506118] Epoch: [14]  [100/316]  eta: 0:00:29  lr: 0.000244  loss: 1.7058 (1.7065)  time: 0.1258  data: 0.0742  max mem: 4666\n",
      "[20:45:28.996085] [20:45:28.996074] [20:45:28.996139] Epoch: [14]  [120/316]  eta: 0:00:26  lr: 0.000244  loss: 1.7345 (1.7088)  time: 0.1245  data: 0.0728  max mem: 4666\n",
      "[20:45:31.428786] [20:45:31.428776] [20:45:31.428841] Epoch: [14]  [140/316]  eta: 0:00:23  lr: 0.000244  loss: 1.7693 (1.7160)  time: 0.1216  data: 0.0700  max mem: 4666\n",
      "[20:45:33.865784] [20:45:33.865774] [20:45:33.865839] Epoch: [14]  [160/316]  eta: 0:00:20  lr: 0.000244  loss: 1.6624 (1.7132)  time: 0.1218  data: 0.0702  max mem: 4666\n",
      "[20:45:36.309774] [20:45:36.309763] [20:45:36.309830] Epoch: [14]  [180/316]  eta: 0:00:17  lr: 0.000244  loss: 1.7179 (1.7132)  time: 0.1221  data: 0.0705  max mem: 4666\n",
      "[20:45:38.753026] [20:45:38.753015] [20:45:38.753155] Epoch: [14]  [200/316]  eta: 0:00:15  lr: 0.000244  loss: 1.6346 (1.7087)  time: 0.1221  data: 0.0705  max mem: 4666\n",
      "[20:45:41.180283] [20:45:41.180274] [20:45:41.180335] Epoch: [14]  [220/316]  eta: 0:00:12  lr: 0.000244  loss: 1.6835 (1.7084)  time: 0.1213  data: 0.0697  max mem: 4666\n",
      "[20:45:43.639287] [20:45:43.639277] [20:45:43.639341] Epoch: [14]  [240/316]  eta: 0:00:09  lr: 0.000244  loss: 1.7144 (1.7103)  time: 0.1229  data: 0.0712  max mem: 4666\n",
      "[20:45:46.104124] [20:45:46.104114] [20:45:46.104176] Epoch: [14]  [260/316]  eta: 0:00:07  lr: 0.000243  loss: 1.6742 (1.7107)  time: 0.1232  data: 0.0715  max mem: 4666\n",
      "[20:45:48.583140] [20:45:48.583129] [20:45:48.583195] Epoch: [14]  [280/316]  eta: 0:00:04  lr: 0.000243  loss: 1.7011 (1.7120)  time: 0.1239  data: 0.0722  max mem: 4666\n",
      "[20:45:50.967098] [20:45:50.967088] [20:45:50.967154] Epoch: [14]  [300/316]  eta: 0:00:02  lr: 0.000243  loss: 1.6909 (1.7102)  time: 0.1192  data: 0.0675  max mem: 4666\n",
      "[20:45:52.416382] [20:45:52.416372] [20:45:52.416425] Epoch: [14]  [315/316]  eta: 0:00:00  lr: 0.000243  loss: 1.6832 (1.7101)  time: 0.1184  data: 0.0670  max mem: 4666\n",
      "[20:45:52.492281] [20:45:52.492272] [20:45:52.492297] Epoch: [14] Total time: 0:00:39 (0.1264 s / it)\n",
      "[20:45:52.492601] [20:45:52.492597] [20:45:52.492609] Averaged stats: lr: 0.000243  loss: 1.6832 (1.7101)\n",
      "[20:45:55.840581] [20:45:55.840558] [20:45:55.840756] Test:  [ 0/68]  eta: 0:01:56  loss: 1.1840 (1.1840)  acc1: 62.5000 (62.5000)  acc5: 93.7500 (93.7500)  time: 1.7126  data: 1.6956  max mem: 4666\n",
      "[20:45:57.206626] [20:45:57.206615] [20:45:57.206728] Test:  [10/68]  eta: 0:00:16  loss: 1.2969 (1.2415)  acc1: 53.1250 (57.1023)  acc5: 100.0000 (97.4432)  time: 0.2798  data: 0.2634  max mem: 4666\n",
      "[20:45:58.623967] [20:45:58.623957] [20:45:58.624028] Test:  [20/68]  eta: 0:00:10  loss: 0.8889 (1.0037)  acc1: 68.7500 (68.6012)  acc5: 100.0000 (98.3631)  time: 0.1391  data: 0.1230  max mem: 4666\n",
      "[20:46:00.053922] [20:46:00.053910] [20:46:00.054190] Test:  [30/68]  eta: 0:00:07  loss: 0.8889 (1.2096)  acc1: 65.6250 (60.4839)  acc5: 100.0000 (92.8427)  time: 0.1423  data: 0.1263  max mem: 4666\n",
      "[20:46:01.431732] [20:46:01.431720] [20:46:01.431794] Test:  [40/68]  eta: 0:00:04  loss: 1.5718 (1.3280)  acc1: 21.8750 (51.5244)  acc5: 93.7500 (92.9878)  time: 0.1403  data: 0.1243  max mem: 4666\n",
      "[20:46:02.804567] [20:46:02.804557] [20:46:02.804648] Test:  [50/68]  eta: 0:00:03  loss: 1.4879 (1.3049)  acc1: 28.1250 (53.5539)  acc5: 100.0000 (94.3015)  time: 0.1374  data: 0.1215  max mem: 4666\n",
      "[20:46:04.184792] [20:46:04.184783] [20:46:04.184838] Test:  [60/68]  eta: 0:00:01  loss: 1.3023 (1.3161)  acc1: 59.3750 (53.9959)  acc5: 100.0000 (95.2357)  time: 0.1376  data: 0.1217  max mem: 4666\n",
      "[20:46:04.308824] [20:46:04.308818] [20:46:04.308834] Test:  [67/68]  eta: 0:00:00  loss: 1.2819 (1.3172)  acc1: 59.3750 (54.4993)  acc5: 100.0000 (95.7084)  time: 0.1382  data: 0.1225  max mem: 4666\n",
      "[20:46:04.380404] [20:46:04.380396] [20:46:04.380422] Test: Total time: 0:00:10 (0.1508 s / it)\n",
      "[20:46:04.380482] [20:46:04.380478] [20:46:04.380493] * Acc@1 54.499 Acc@5 95.708 loss 1.317\n",
      "[20:46:04.380588] [20:46:04.380583] [20:46:04.380599] Accuracy of the network on the 2167 test images: 54.5%\n",
      "[20:46:04.380616] [20:46:04.380614] [20:46:04.380626] Max accuracy: 54.50%\n",
      "[20:46:05.972471] [20:46:05.972451] [20:46:05.972615] Epoch: [15]  [  0/316]  eta: 0:08:22  lr: 0.000243  loss: 1.7682 (1.7682)  time: 1.5904  data: 1.5388  max mem: 4666\n",
      "[20:46:08.359079] [20:46:08.359068] [20:46:08.359160] Epoch: [15]  [ 20/316]  eta: 0:00:56  lr: 0.000243  loss: 1.6865 (1.6693)  time: 0.1193  data: 0.0675  max mem: 4666\n",
      "[20:46:10.810084] [20:46:10.810073] [20:46:10.810149] Epoch: [15]  [ 40/316]  eta: 0:00:43  lr: 0.000243  loss: 1.6633 (1.6839)  time: 0.1225  data: 0.0709  max mem: 4666\n",
      "[20:46:13.246979] [20:46:13.246969] [20:46:13.247064] Epoch: [15]  [ 60/316]  eta: 0:00:37  lr: 0.000243  loss: 1.7356 (1.6899)  time: 0.1218  data: 0.0701  max mem: 4666\n",
      "[20:46:15.694813] [20:46:15.694803] [20:46:15.694867] Epoch: [15]  [ 80/316]  eta: 0:00:32  lr: 0.000243  loss: 1.7431 (1.6983)  time: 0.1223  data: 0.0707  max mem: 4666\n",
      "[20:46:18.108606] [20:46:18.108596] [20:46:18.108666] Epoch: [15]  [100/316]  eta: 0:00:29  lr: 0.000243  loss: 1.6867 (1.6979)  time: 0.1206  data: 0.0691  max mem: 4666\n",
      "[20:46:20.600972] [20:46:20.600961] [20:46:20.601046] Epoch: [15]  [120/316]  eta: 0:00:26  lr: 0.000243  loss: 1.6793 (1.6981)  time: 0.1246  data: 0.0729  max mem: 4666\n",
      "[20:46:23.074162] [20:46:23.074151] [20:46:23.074477] Epoch: [15]  [140/316]  eta: 0:00:23  lr: 0.000243  loss: 1.6926 (1.6974)  time: 0.1236  data: 0.0720  max mem: 4666\n",
      "[20:46:25.541485] [20:46:25.541475] [20:46:25.541545] Epoch: [15]  [160/316]  eta: 0:00:20  lr: 0.000243  loss: 1.6687 (1.6914)  time: 0.1233  data: 0.0718  max mem: 4666\n",
      "[20:46:27.946052] [20:46:27.946042] [20:46:27.946186] Epoch: [15]  [180/316]  eta: 0:00:17  lr: 0.000242  loss: 1.6753 (1.6924)  time: 0.1202  data: 0.0688  max mem: 4666\n",
      "[20:46:30.381098] [20:46:30.381088] [20:46:30.381382] Epoch: [15]  [200/316]  eta: 0:00:14  lr: 0.000242  loss: 1.6810 (1.6917)  time: 0.1217  data: 0.0703  max mem: 4666\n",
      "[20:46:32.796454] [20:46:32.796444] [20:46:32.796507] Epoch: [15]  [220/316]  eta: 0:00:12  lr: 0.000242  loss: 1.7130 (1.6914)  time: 0.1207  data: 0.0693  max mem: 4666\n",
      "[20:46:35.207381] [20:46:35.207371] [20:46:35.207435] Epoch: [15]  [240/316]  eta: 0:00:09  lr: 0.000242  loss: 1.7242 (1.6932)  time: 0.1205  data: 0.0691  max mem: 4666\n",
      "[20:46:37.686537] [20:46:37.686527] [20:46:37.686592] Epoch: [15]  [260/316]  eta: 0:00:07  lr: 0.000242  loss: 1.7061 (1.6917)  time: 0.1239  data: 0.0726  max mem: 4666\n",
      "[20:46:40.120749] [20:46:40.120738] [20:46:40.120806] Epoch: [15]  [280/316]  eta: 0:00:04  lr: 0.000242  loss: 1.7008 (1.6924)  time: 0.1217  data: 0.0703  max mem: 4666\n",
      "[20:46:42.587963] [20:46:42.587952] [20:46:42.588017] Epoch: [15]  [300/316]  eta: 0:00:02  lr: 0.000242  loss: 1.6375 (1.6897)  time: 0.1233  data: 0.0720  max mem: 4666\n",
      "[20:46:44.369189] [20:46:44.369179] [20:46:44.369234] Epoch: [15]  [315/316]  eta: 0:00:00  lr: 0.000242  loss: 1.6538 (1.6889)  time: 0.1198  data: 0.0685  max mem: 4666\n",
      "[20:46:44.445597] [20:46:44.445587] [20:46:44.445614] Epoch: [15] Total time: 0:00:40 (0.1268 s / it)\n",
      "[20:46:44.445781] [20:46:44.445776] [20:46:44.445789] Averaged stats: lr: 0.000242  loss: 1.6538 (1.6889)\n",
      "[20:46:47.853979] [20:46:47.853958] [20:46:47.854125] Test:  [ 0/68]  eta: 0:01:59  loss: 0.9879 (0.9879)  acc1: 71.8750 (71.8750)  acc5: 100.0000 (100.0000)  time: 1.7587  data: 1.7416  max mem: 4666\n",
      "[20:46:49.219297] [20:46:49.219288] [20:46:49.219572] Test:  [10/68]  eta: 0:00:16  loss: 1.1155 (1.1040)  acc1: 65.6250 (63.9205)  acc5: 100.0000 (98.2955)  time: 0.2839  data: 0.2675  max mem: 4666\n",
      "[20:46:50.667974] [20:46:50.667963] [20:46:50.668269] Test:  [20/68]  eta: 0:00:10  loss: 0.8512 (0.9398)  acc1: 75.0000 (73.5119)  acc5: 100.0000 (98.6607)  time: 0.1406  data: 0.1244  max mem: 4666\n",
      "[20:46:52.071509] [20:46:52.071497] [20:46:52.071568] Test:  [30/68]  eta: 0:00:07  loss: 0.8937 (1.1868)  acc1: 71.8750 (61.6935)  acc5: 100.0000 (93.3468)  time: 0.1425  data: 0.1265  max mem: 4666\n",
      "[20:46:53.462287] [20:46:53.462276] [20:46:53.462344] Test:  [40/68]  eta: 0:00:05  loss: 1.5836 (1.2863)  acc1: 28.1250 (54.0396)  acc5: 93.7500 (93.6738)  time: 0.1396  data: 0.1237  max mem: 4666\n",
      "[20:46:54.859870] [20:46:54.859860] [20:46:54.859921] Test:  [50/68]  eta: 0:00:03  loss: 1.5836 (1.3014)  acc1: 31.2500 (52.5735)  acc5: 100.0000 (94.7917)  time: 0.1394  data: 0.1234  max mem: 4666\n",
      "[20:46:56.235305] [20:46:56.235289] [20:46:56.235382] Test:  [60/68]  eta: 0:00:01  loss: 1.3921 (1.3132)  acc1: 50.0000 (52.7152)  acc5: 100.0000 (95.6455)  time: 0.1386  data: 0.1224  max mem: 4666\n",
      "[20:46:56.342562] [20:46:56.342557] [20:46:56.342572] Test:  [67/68]  eta: 0:00:00  loss: 1.1800 (1.3055)  acc1: 59.3750 (54.3147)  acc5: 100.0000 (96.0775)  time: 0.1383  data: 0.1224  max mem: 4666\n",
      "[20:46:56.415770] [20:46:56.415761] [20:46:56.415786] Test: Total time: 0:00:10 (0.1518 s / it)\n",
      "[20:46:56.415826] [20:46:56.415823] [20:46:56.415832] * Acc@1 54.315 Acc@5 96.078 loss 1.305\n",
      "[20:46:56.415935] [20:46:56.415933] [20:46:56.415942] Accuracy of the network on the 2167 test images: 54.3%\n",
      "[20:46:56.415951] [20:46:56.415949] [20:46:56.415956] Max accuracy: 54.50%\n",
      "[20:46:57.984012] [20:46:57.983995] [20:46:57.984172] Epoch: [16]  [  0/316]  eta: 0:08:15  lr: 0.000242  loss: 1.6912 (1.6912)  time: 1.5667  data: 1.5157  max mem: 4666\n",
      "[20:47:00.423016] [20:47:00.423006] [20:47:00.423083] Epoch: [16]  [ 20/316]  eta: 0:00:56  lr: 0.000242  loss: 1.6388 (1.6687)  time: 0.1219  data: 0.0705  max mem: 4666\n",
      "[20:47:02.871021] [20:47:02.871010] [20:47:02.871072] Epoch: [16]  [ 40/316]  eta: 0:00:43  lr: 0.000242  loss: 1.6504 (1.6754)  time: 0.1224  data: 0.0710  max mem: 4666\n",
      "[20:47:05.267835] [20:47:05.267824] [20:47:05.267890] Epoch: [16]  [ 60/316]  eta: 0:00:37  lr: 0.000242  loss: 1.6787 (1.6792)  time: 0.1198  data: 0.0684  max mem: 4666\n",
      "[20:47:07.710270] [20:47:07.710259] [20:47:07.710330] Epoch: [16]  [ 80/316]  eta: 0:00:32  lr: 0.000241  loss: 1.7425 (1.6861)  time: 0.1221  data: 0.0707  max mem: 4666\n",
      "[20:47:10.140837] [20:47:10.140826] [20:47:10.140898] Epoch: [16]  [100/316]  eta: 0:00:29  lr: 0.000241  loss: 1.7137 (1.6926)  time: 0.1215  data: 0.0701  max mem: 4666\n",
      "[20:47:12.560640] [20:47:12.560629] [20:47:12.560694] Epoch: [16]  [120/316]  eta: 0:00:26  lr: 0.000241  loss: 1.7131 (1.6975)  time: 0.1210  data: 0.0696  max mem: 4666\n",
      "[20:47:15.007873] [20:47:15.007862] [20:47:15.007933] Epoch: [16]  [140/316]  eta: 0:00:23  lr: 0.000241  loss: 1.6977 (1.7012)  time: 0.1223  data: 0.0711  max mem: 4666\n",
      "[20:47:17.425059] [20:47:17.425048] [20:47:17.425113] Epoch: [16]  [160/316]  eta: 0:00:20  lr: 0.000241  loss: 1.6681 (1.6992)  time: 0.1208  data: 0.0695  max mem: 4666\n",
      "[20:47:19.843821] [20:47:19.843810] [20:47:19.843875] Epoch: [16]  [180/316]  eta: 0:00:17  lr: 0.000241  loss: 1.7071 (1.7008)  time: 0.1209  data: 0.0696  max mem: 4666\n",
      "[20:47:22.279344] [20:47:22.279334] [20:47:22.279400] Epoch: [16]  [200/316]  eta: 0:00:14  lr: 0.000241  loss: 1.6536 (1.6980)  time: 0.1217  data: 0.0704  max mem: 4666\n",
      "[20:47:24.761810] [20:47:24.761799] [20:47:24.761864] Epoch: [16]  [220/316]  eta: 0:00:12  lr: 0.000241  loss: 1.6752 (1.6982)  time: 0.1241  data: 0.0727  max mem: 4666\n",
      "[20:47:27.250872] [20:47:27.250862] [20:47:27.250928] Epoch: [16]  [240/316]  eta: 0:00:09  lr: 0.000241  loss: 1.7527 (1.6987)  time: 0.1244  data: 0.0730  max mem: 4666\n",
      "[20:47:29.656625] [20:47:29.656615] [20:47:29.656679] Epoch: [16]  [260/316]  eta: 0:00:07  lr: 0.000241  loss: 1.6571 (1.6973)  time: 0.1202  data: 0.0689  max mem: 4666\n",
      "[20:47:32.078403] [20:47:32.078393] [20:47:32.078456] Epoch: [16]  [280/316]  eta: 0:00:04  lr: 0.000241  loss: 1.6797 (1.6965)  time: 0.1210  data: 0.0697  max mem: 4666\n",
      "[20:47:34.518984] [20:47:34.518973] [20:47:34.519042] Epoch: [16]  [300/316]  eta: 0:00:02  lr: 0.000240  loss: 1.7116 (1.6951)  time: 0.1220  data: 0.0706  max mem: 4666\n",
      "[20:47:36.174433] [20:47:36.174423] [20:47:36.174480] Epoch: [16]  [315/316]  eta: 0:00:00  lr: 0.000240  loss: 1.6590 (1.6934)  time: 0.1183  data: 0.0671  max mem: 4666\n",
      "[20:47:36.251190] [20:47:36.251181] [20:47:36.251207] Epoch: [16] Total time: 0:00:39 (0.1261 s / it)\n",
      "[20:47:36.251266] [20:47:36.251263] [20:47:36.251272] Averaged stats: lr: 0.000240  loss: 1.6590 (1.6934)\n",
      "[20:47:39.687968] [20:47:39.687946] [20:47:39.688134] Test:  [ 0/68]  eta: 0:01:58  loss: 1.0783 (1.0783)  acc1: 68.7500 (68.7500)  acc5: 100.0000 (100.0000)  time: 1.7437  data: 1.7264  max mem: 4666\n",
      "[20:47:41.047630] [20:47:41.047619] [20:47:41.047835] Test:  [10/68]  eta: 0:00:16  loss: 1.2425 (1.2045)  acc1: 59.3750 (58.8068)  acc5: 100.0000 (97.7273)  time: 0.2820  data: 0.2656  max mem: 4666\n",
      "[20:47:42.447936] [20:47:42.447926] [20:47:42.448002] Test:  [20/68]  eta: 0:00:10  loss: 0.8188 (0.9599)  acc1: 71.8750 (70.6845)  acc5: 100.0000 (98.6607)  time: 0.1379  data: 0.1217  max mem: 4666\n",
      "[20:47:43.855486] [20:47:43.855475] [20:47:43.855631] Test:  [30/68]  eta: 0:00:07  loss: 0.8188 (1.1772)  acc1: 68.7500 (61.4919)  acc5: 100.0000 (94.2540)  time: 0.1403  data: 0.1244  max mem: 4666\n",
      "[20:47:45.253219] [20:47:45.253207] [20:47:45.253287] Test:  [40/68]  eta: 0:00:04  loss: 1.4927 (1.2914)  acc1: 31.2500 (53.5061)  acc5: 93.7500 (94.0549)  time: 0.1402  data: 0.1242  max mem: 4666\n",
      "[20:47:46.630895] [20:47:46.630885] [20:47:46.630963] Test:  [50/68]  eta: 0:00:03  loss: 1.4391 (1.2619)  acc1: 34.3750 (55.3309)  acc5: 100.0000 (95.2206)  time: 0.1387  data: 0.1228  max mem: 4666\n",
      "[20:47:48.035176] [20:47:48.035161] [20:47:48.035259] Test:  [60/68]  eta: 0:00:01  loss: 1.2363 (1.2751)  acc1: 59.3750 (55.5328)  acc5: 100.0000 (96.0041)  time: 0.1390  data: 0.1231  max mem: 4666\n",
      "[20:47:48.145478] [20:47:48.145473] [20:47:48.145489] Test:  [67/68]  eta: 0:00:00  loss: 1.1970 (1.2767)  acc1: 59.3750 (55.9760)  acc5: 100.0000 (96.4006)  time: 0.1389  data: 0.1230  max mem: 4666\n",
      "[20:47:48.215127] [20:47:48.215121] [20:47:48.215141] Test: Total time: 0:00:10 (0.1511 s / it)\n",
      "[20:47:48.215176] [20:47:48.215174] [20:47:48.215182] * Acc@1 55.976 Acc@5 96.401 loss 1.277\n",
      "[20:47:48.215256] [20:47:48.215254] [20:47:48.215262] Accuracy of the network on the 2167 test images: 56.0%\n",
      "[20:47:48.215272] [20:47:48.215271] [20:47:48.215277] Max accuracy: 55.98%\n",
      "[20:47:49.834705] [20:47:49.834680] [20:47:49.834894] Epoch: [17]  [  0/316]  eta: 0:08:31  lr: 0.000240  loss: 1.6484 (1.6484)  time: 1.6182  data: 1.5671  max mem: 4666\n",
      "[20:47:52.298986] [20:47:52.298976] [20:47:52.299044] Epoch: [17]  [ 20/316]  eta: 0:00:57  lr: 0.000240  loss: 1.6672 (1.6749)  time: 0.1232  data: 0.0717  max mem: 4666\n",
      "[20:47:54.764218] [20:47:54.764207] [20:47:54.764277] Epoch: [17]  [ 40/316]  eta: 0:00:44  lr: 0.000240  loss: 1.6916 (1.6940)  time: 0.1232  data: 0.0719  max mem: 4666\n",
      "[20:47:57.204627] [20:47:57.204617] [20:47:57.204774] Epoch: [17]  [ 60/316]  eta: 0:00:37  lr: 0.000240  loss: 1.6501 (1.6888)  time: 0.1220  data: 0.0706  max mem: 4666\n",
      "[20:47:59.618327] [20:47:59.618317] [20:47:59.618393] Epoch: [17]  [ 80/316]  eta: 0:00:33  lr: 0.000240  loss: 1.7120 (1.6899)  time: 0.1206  data: 0.0693  max mem: 4666\n",
      "[20:48:02.104699] [20:48:02.104689] [20:48:02.104754] Epoch: [17]  [100/316]  eta: 0:00:29  lr: 0.000240  loss: 1.7072 (1.6941)  time: 0.1243  data: 0.0728  max mem: 4666\n",
      "[20:48:04.591263] [20:48:04.591252] [20:48:04.591562] Epoch: [17]  [120/316]  eta: 0:00:26  lr: 0.000240  loss: 1.6687 (1.6962)  time: 0.1243  data: 0.0729  max mem: 4666\n",
      "[20:48:07.023457] [20:48:07.023448] [20:48:07.023510] Epoch: [17]  [140/316]  eta: 0:00:23  lr: 0.000240  loss: 1.7266 (1.6992)  time: 0.1216  data: 0.0702  max mem: 4666\n",
      "[20:48:09.479110] [20:48:09.479098] [20:48:09.479167] Epoch: [17]  [160/316]  eta: 0:00:20  lr: 0.000240  loss: 1.6135 (1.6928)  time: 0.1227  data: 0.0713  max mem: 4666\n",
      "[20:48:11.941407] [20:48:11.941395] [20:48:11.941464] Epoch: [17]  [180/316]  eta: 0:00:17  lr: 0.000239  loss: 1.6781 (1.6919)  time: 0.1231  data: 0.0717  max mem: 4666\n",
      "[20:48:14.405401] [20:48:14.405391] [20:48:14.405565] Epoch: [17]  [200/316]  eta: 0:00:15  lr: 0.000239  loss: 1.6519 (1.6875)  time: 0.1232  data: 0.0717  max mem: 4666\n",
      "[20:48:16.872632] [20:48:16.872622] [20:48:16.872690] Epoch: [17]  [220/316]  eta: 0:00:12  lr: 0.000239  loss: 1.6380 (1.6867)  time: 0.1233  data: 0.0719  max mem: 4666\n",
      "[20:48:19.361959] [20:48:19.361948] [20:48:19.362046] Epoch: [17]  [240/316]  eta: 0:00:09  lr: 0.000239  loss: 1.6583 (1.6845)  time: 0.1244  data: 0.0730  max mem: 4666\n",
      "[20:48:21.816683] [20:48:21.816673] [20:48:21.816744] Epoch: [17]  [260/316]  eta: 0:00:07  lr: 0.000239  loss: 1.6711 (1.6855)  time: 0.1227  data: 0.0713  max mem: 4666\n",
      "[20:48:24.302199] [20:48:24.302188] [20:48:24.302253] Epoch: [17]  [280/316]  eta: 0:00:04  lr: 0.000239  loss: 1.6643 (1.6852)  time: 0.1242  data: 0.0728  max mem: 4666\n",
      "[20:48:26.764933] [20:48:26.764923] [20:48:26.765021] Epoch: [17]  [300/316]  eta: 0:00:02  lr: 0.000239  loss: 1.6743 (1.6846)  time: 0.1231  data: 0.0717  max mem: 4666\n",
      "[20:48:28.249579] [20:48:28.249569] [20:48:28.249625] Epoch: [17]  [315/316]  eta: 0:00:00  lr: 0.000239  loss: 1.6941 (1.6861)  time: 0.1232  data: 0.0721  max mem: 4666\n",
      "[20:48:28.328217] [20:48:28.328209] [20:48:28.328233] Epoch: [17] Total time: 0:00:40 (0.1269 s / it)\n",
      "[20:48:28.328287] [20:48:28.328285] [20:48:28.328294] Averaged stats: lr: 0.000239  loss: 1.6941 (1.6861)\n",
      "[20:48:31.723977] [20:48:31.723951] [20:48:31.724145] Test:  [ 0/68]  eta: 0:01:58  loss: 1.0629 (1.0629)  acc1: 71.8750 (71.8750)  acc5: 100.0000 (100.0000)  time: 1.7443  data: 1.7271  max mem: 4666\n",
      "[20:48:33.097539] [20:48:33.097528] [20:48:33.097666] Test:  [10/68]  eta: 0:00:16  loss: 1.2219 (1.1613)  acc1: 65.6250 (60.5114)  acc5: 100.0000 (99.1477)  time: 0.2834  data: 0.2669  max mem: 4666\n",
      "[20:48:34.497646] [20:48:34.497634] [20:48:34.497702] Test:  [20/68]  eta: 0:00:10  loss: 0.9197 (1.0166)  acc1: 68.7500 (67.8571)  acc5: 100.0000 (99.1071)  time: 0.1386  data: 0.1224  max mem: 4666\n",
      "[20:48:35.910483] [20:48:35.910472] [20:48:35.910549] Test:  [30/68]  eta: 0:00:07  loss: 1.0112 (1.2341)  acc1: 59.3750 (59.9798)  acc5: 100.0000 (92.7419)  time: 0.1406  data: 0.1246  max mem: 4666\n",
      "[20:48:37.298919] [20:48:37.298908] [20:48:37.298994] Test:  [40/68]  eta: 0:00:04  loss: 1.4791 (1.3260)  acc1: 28.1250 (52.3628)  acc5: 93.7500 (93.0640)  time: 0.1400  data: 0.1240  max mem: 4666\n",
      "[20:48:38.679424] [20:48:38.679412] [20:48:38.679478] Test:  [50/68]  eta: 0:00:03  loss: 1.4791 (1.3070)  acc1: 37.5000 (54.3505)  acc5: 100.0000 (94.3627)  time: 0.1384  data: 0.1224  max mem: 4666\n",
      "[20:48:40.048568] [20:48:40.048557] [20:48:40.048623] Test:  [60/68]  eta: 0:00:01  loss: 1.2162 (1.2991)  acc1: 62.5000 (55.4816)  acc5: 100.0000 (95.2869)  time: 0.1374  data: 0.1215  max mem: 4666\n",
      "[20:48:40.202951] [20:48:40.202944] [20:48:40.202968] Test:  [67/68]  eta: 0:00:00  loss: 1.1711 (1.2920)  acc1: 65.6250 (56.3913)  acc5: 100.0000 (95.7545)  time: 0.1379  data: 0.1222  max mem: 4666\n",
      "[20:48:40.277677] [20:48:40.277671] [20:48:40.277704] Test: Total time: 0:00:10 (0.1515 s / it)\n",
      "[20:48:40.277737] [20:48:40.277735] [20:48:40.277744] * Acc@1 56.391 Acc@5 95.754 loss 1.292\n",
      "[20:48:40.277897] [20:48:40.277894] [20:48:40.277904] Accuracy of the network on the 2167 test images: 56.4%\n",
      "[20:48:40.277914] [20:48:40.277912] [20:48:40.277920] Max accuracy: 56.39%\n",
      "[20:48:41.892351] [20:48:41.892324] [20:48:41.892553] Epoch: [18]  [  0/316]  eta: 0:08:29  lr: 0.000239  loss: 1.6763 (1.6763)  time: 1.6132  data: 1.5619  max mem: 4666\n",
      "[20:48:44.347371] [20:48:44.347361] [20:48:44.347424] Epoch: [18]  [ 20/316]  eta: 0:00:57  lr: 0.000239  loss: 1.5979 (1.6471)  time: 0.1227  data: 0.0712  max mem: 4666\n",
      "[20:48:46.826620] [20:48:46.826611] [20:48:46.826677] Epoch: [18]  [ 40/316]  eta: 0:00:44  lr: 0.000238  loss: 1.6936 (1.6643)  time: 0.1239  data: 0.0725  max mem: 4666\n",
      "[20:48:49.329276] [20:48:49.329266] [20:48:49.329333] Epoch: [18]  [ 60/316]  eta: 0:00:37  lr: 0.000238  loss: 1.6998 (1.6736)  time: 0.1251  data: 0.0737  max mem: 4666\n",
      "[20:48:51.801613] [20:48:51.801603] [20:48:51.801670] Epoch: [18]  [ 80/316]  eta: 0:00:33  lr: 0.000238  loss: 1.6865 (1.6798)  time: 0.1236  data: 0.0722  max mem: 4666\n",
      "[20:48:54.265458] [20:48:54.265447] [20:48:54.265517] Epoch: [18]  [100/316]  eta: 0:00:29  lr: 0.000238  loss: 1.6855 (1.6823)  time: 0.1232  data: 0.0717  max mem: 4666\n",
      "[20:48:56.724854] [20:48:56.724844] [20:48:56.724976] Epoch: [18]  [120/316]  eta: 0:00:26  lr: 0.000238  loss: 1.6899 (1.6872)  time: 0.1229  data: 0.0715  max mem: 4666\n",
      "[20:48:59.242097] [20:48:59.242080] [20:48:59.242155] Epoch: [18]  [140/316]  eta: 0:00:23  lr: 0.000238  loss: 1.7280 (1.6910)  time: 0.1258  data: 0.0744  max mem: 4666\n",
      "[20:49:01.694924] [20:49:01.694914] [20:49:01.694975] Epoch: [18]  [160/316]  eta: 0:00:20  lr: 0.000238  loss: 1.6244 (1.6886)  time: 0.1226  data: 0.0713  max mem: 4666\n",
      "[20:49:04.183610] [20:49:04.183599] [20:49:04.183668] Epoch: [18]  [180/316]  eta: 0:00:17  lr: 0.000238  loss: 1.6987 (1.6890)  time: 0.1244  data: 0.0730  max mem: 4666\n",
      "[20:49:06.612312] [20:49:06.612300] [20:49:06.612369] Epoch: [18]  [200/316]  eta: 0:00:15  lr: 0.000238  loss: 1.6552 (1.6863)  time: 0.1214  data: 0.0700  max mem: 4666\n",
      "[20:49:09.116404] [20:49:09.116394] [20:49:09.116462] Epoch: [18]  [220/316]  eta: 0:00:12  lr: 0.000237  loss: 1.6544 (1.6836)  time: 0.1252  data: 0.0738  max mem: 4666\n",
      "[20:49:11.588896] [20:49:11.588885] [20:49:11.588954] Epoch: [18]  [240/316]  eta: 0:00:09  lr: 0.000237  loss: 1.6953 (1.6844)  time: 0.1236  data: 0.0722  max mem: 4666\n",
      "[20:49:14.071870] [20:49:14.071859] [20:49:14.071929] Epoch: [18]  [260/316]  eta: 0:00:07  lr: 0.000237  loss: 1.6933 (1.6839)  time: 0.1241  data: 0.0727  max mem: 4666\n",
      "[20:49:16.552605] [20:49:16.552594] [20:49:16.552663] Epoch: [18]  [280/316]  eta: 0:00:04  lr: 0.000237  loss: 1.6732 (1.6840)  time: 0.1240  data: 0.0726  max mem: 4666\n",
      "[20:49:18.982776] [20:49:18.982764] [20:49:18.982829] Epoch: [18]  [300/316]  eta: 0:00:02  lr: 0.000237  loss: 1.6527 (1.6825)  time: 0.1215  data: 0.0701  max mem: 4666\n",
      "[20:49:20.426960] [20:49:20.426949] [20:49:20.427007] Epoch: [18]  [315/316]  eta: 0:00:00  lr: 0.000237  loss: 1.6549 (1.6823)  time: 0.1201  data: 0.0688  max mem: 4666\n",
      "[20:49:20.505407] [20:49:20.505395] [20:49:20.505435] Epoch: [18] Total time: 0:00:40 (0.1273 s / it)\n",
      "[20:49:20.505511] [20:49:20.505506] [20:49:20.505525] Averaged stats: lr: 0.000237  loss: 1.6549 (1.6823)\n",
      "[20:49:23.836168] [20:49:23.836143] [20:49:23.836352] Test:  [ 0/68]  eta: 0:01:54  loss: 1.0894 (1.0894)  acc1: 68.7500 (68.7500)  acc5: 96.8750 (96.8750)  time: 1.6768  data: 1.6596  max mem: 4666\n",
      "[20:49:25.212999] [20:49:25.212988] [20:49:25.213064] Test:  [10/68]  eta: 0:00:16  loss: 1.2696 (1.2102)  acc1: 62.5000 (58.8068)  acc5: 96.8750 (95.7386)  time: 0.2775  data: 0.2610  max mem: 4666\n",
      "[20:49:26.660707] [20:49:26.660697] [20:49:26.660805] Test:  [20/68]  eta: 0:00:10  loss: 0.8941 (0.9986)  acc1: 68.7500 (68.3036)  acc5: 100.0000 (97.3214)  time: 0.1411  data: 0.1249  max mem: 4666\n",
      "[20:49:28.072308] [20:49:28.072297] [20:49:28.072369] Test:  [30/68]  eta: 0:00:07  loss: 0.8941 (1.2248)  acc1: 62.5000 (58.9718)  acc5: 100.0000 (91.5323)  time: 0.1429  data: 0.1269  max mem: 4666\n",
      "[20:49:29.462415] [20:49:29.462405] [20:49:29.462519] Test:  [40/68]  eta: 0:00:04  loss: 1.4555 (1.2982)  acc1: 31.2500 (51.9817)  acc5: 96.8750 (92.4543)  time: 0.1400  data: 0.1240  max mem: 4666\n",
      "[20:49:30.842521] [20:49:30.842509] [20:49:30.842573] Test:  [50/68]  eta: 0:00:03  loss: 1.4023 (1.2846)  acc1: 31.2500 (52.8186)  acc5: 100.0000 (93.8113)  time: 0.1384  data: 0.1224  max mem: 4666\n",
      "[20:49:32.217951] [20:49:32.217941] [20:49:32.218045] Test:  [60/68]  eta: 0:00:01  loss: 1.2117 (1.2671)  acc1: 59.3750 (54.4057)  acc5: 100.0000 (94.8258)  time: 0.1377  data: 0.1217  max mem: 4666\n",
      "[20:49:32.333166] [20:49:32.333161] [20:49:32.333178] Test:  [67/68]  eta: 0:00:00  loss: 1.0652 (1.2510)  acc1: 65.6250 (55.9299)  acc5: 100.0000 (95.3392)  time: 0.1374  data: 0.1216  max mem: 4666\n",
      "[20:49:32.410770] [20:49:32.410765] [20:49:32.410780] Test: Total time: 0:00:10 (0.1508 s / it)\n",
      "[20:49:32.410811] [20:49:32.410809] [20:49:32.410818] * Acc@1 55.930 Acc@5 95.339 loss 1.251\n",
      "[20:49:32.410882] [20:49:32.410880] [20:49:32.410888] Accuracy of the network on the 2167 test images: 55.9%\n",
      "[20:49:32.410897] [20:49:32.410896] [20:49:32.410902] Max accuracy: 56.39%\n",
      "[20:49:34.005250] [20:49:34.005231] [20:49:34.005909] Epoch: [19]  [  0/316]  eta: 0:08:23  lr: 0.000237  loss: 1.7140 (1.7140)  time: 1.5932  data: 1.5406  max mem: 4666\n",
      "[20:49:36.520551] [20:49:36.520539] [20:49:36.520608] Epoch: [19]  [ 20/316]  eta: 0:00:57  lr: 0.000237  loss: 1.6107 (1.6540)  time: 0.1257  data: 0.0738  max mem: 4666\n",
      "[20:49:39.025896] [20:49:39.025884] [20:49:39.026211] Epoch: [19]  [ 40/316]  eta: 0:00:44  lr: 0.000237  loss: 1.6375 (1.6648)  time: 0.1252  data: 0.0735  max mem: 4666\n",
      "[20:49:41.462846] [20:49:41.462835] [20:49:41.462901] Epoch: [19]  [ 60/316]  eta: 0:00:37  lr: 0.000237  loss: 1.6949 (1.6664)  time: 0.1218  data: 0.0701  max mem: 4666\n",
      "[20:49:43.930309] [20:49:43.930298] [20:49:43.930366] Epoch: [19]  [ 80/316]  eta: 0:00:33  lr: 0.000236  loss: 1.7165 (1.6740)  time: 0.1233  data: 0.0716  max mem: 4666\n",
      "[20:49:46.363833] [20:49:46.363822] [20:49:46.363962] Epoch: [19]  [100/316]  eta: 0:00:29  lr: 0.000236  loss: 1.6973 (1.6755)  time: 0.1216  data: 0.0699  max mem: 4666\n",
      "[20:49:48.862211] [20:49:48.862200] [20:49:48.862265] Epoch: [19]  [120/316]  eta: 0:00:26  lr: 0.000236  loss: 1.6508 (1.6712)  time: 0.1249  data: 0.0735  max mem: 4666\n",
      "[20:49:51.284740] [20:49:51.284729] [20:49:51.284797] Epoch: [19]  [140/316]  eta: 0:00:23  lr: 0.000236  loss: 1.6587 (1.6724)  time: 0.1211  data: 0.0697  max mem: 4666\n",
      "[20:49:53.740718] [20:49:53.740708] [20:49:53.740793] Epoch: [19]  [160/316]  eta: 0:00:20  lr: 0.000236  loss: 1.6209 (1.6692)  time: 0.1228  data: 0.0714  max mem: 4666\n",
      "[20:49:56.199452] [20:49:56.199441] [20:49:56.199508] Epoch: [19]  [180/316]  eta: 0:00:17  lr: 0.000236  loss: 1.6762 (1.6720)  time: 0.1229  data: 0.0715  max mem: 4666\n",
      "[20:49:58.653648] [20:49:58.653638] [20:49:58.653701] Epoch: [19]  [200/316]  eta: 0:00:15  lr: 0.000236  loss: 1.6486 (1.6718)  time: 0.1227  data: 0.0713  max mem: 4666\n",
      "[20:50:01.147824] [20:50:01.147814] [20:50:01.147879] Epoch: [19]  [220/316]  eta: 0:00:12  lr: 0.000236  loss: 1.6401 (1.6697)  time: 0.1247  data: 0.0733  max mem: 4666\n",
      "[20:50:03.588292] [20:50:03.588280] [20:50:03.588362] Epoch: [19]  [240/316]  eta: 0:00:09  lr: 0.000235  loss: 1.6722 (1.6693)  time: 0.1220  data: 0.0706  max mem: 4666\n",
      "[20:50:06.067012] [20:50:06.067000] [20:50:06.067072] Epoch: [19]  [260/316]  eta: 0:00:07  lr: 0.000235  loss: 1.6535 (1.6696)  time: 0.1239  data: 0.0725  max mem: 4666\n",
      "[20:50:08.501529] [20:50:08.501517] [20:50:08.501586] Epoch: [19]  [280/316]  eta: 0:00:04  lr: 0.000235  loss: 1.6695 (1.6709)  time: 0.1217  data: 0.0703  max mem: 4666\n",
      "[20:50:10.954109] [20:50:10.954099] [20:50:10.954166] Epoch: [19]  [300/316]  eta: 0:00:02  lr: 0.000235  loss: 1.6975 (1.6731)  time: 0.1226  data: 0.0712  max mem: 4666\n",
      "[20:50:12.387851] [20:50:12.387842] [20:50:12.387897] Epoch: [19]  [315/316]  eta: 0:00:00  lr: 0.000235  loss: 1.6529 (1.6754)  time: 0.1199  data: 0.0687  max mem: 4666\n",
      "[20:50:12.462301] [20:50:12.462292] [20:50:12.462318] Epoch: [19] Total time: 0:00:40 (0.1267 s / it)\n",
      "[20:50:12.462374] [20:50:12.462372] [20:50:12.462381] Averaged stats: lr: 0.000235  loss: 1.6529 (1.6754)\n",
      "[20:50:15.845861] [20:50:15.845834] [20:50:15.846038] Test:  [ 0/68]  eta: 0:01:54  loss: 1.0956 (1.0956)  acc1: 78.1250 (78.1250)  acc5: 100.0000 (100.0000)  time: 1.6815  data: 1.6644  max mem: 4666\n",
      "[20:50:17.223781] [20:50:17.223770] [20:50:17.223840] Test:  [10/68]  eta: 0:00:16  loss: 1.2136 (1.1443)  acc1: 71.8750 (65.6250)  acc5: 100.0000 (99.7159)  time: 0.2780  data: 0.2615  max mem: 4666\n",
      "[20:50:18.623726] [20:50:18.623715] [20:50:18.623779] Test:  [20/68]  eta: 0:00:10  loss: 0.8724 (0.9671)  acc1: 75.0000 (73.6607)  acc5: 100.0000 (99.7024)  time: 0.1388  data: 0.1226  max mem: 4666\n",
      "[20:50:20.014962] [20:50:20.014951] [20:50:20.015018] Test:  [30/68]  eta: 0:00:07  loss: 0.8949 (1.1862)  acc1: 68.7500 (64.9194)  acc5: 100.0000 (93.1452)  time: 0.1395  data: 0.1235  max mem: 4666\n",
      "[20:50:21.404026] [20:50:21.404014] [20:50:21.404084] Test:  [40/68]  eta: 0:00:04  loss: 1.5181 (1.2945)  acc1: 37.5000 (56.8598)  acc5: 96.8750 (93.5213)  time: 0.1390  data: 0.1229  max mem: 4666\n",
      "[20:50:22.781205] [20:50:22.781195] [20:50:22.781325] Test:  [50/68]  eta: 0:00:03  loss: 1.4928 (1.2900)  acc1: 37.5000 (58.0882)  acc5: 100.0000 (94.7917)  time: 0.1382  data: 0.1222  max mem: 4666\n",
      "[20:50:24.161328] [20:50:24.161318] [20:50:24.161383] Test:  [60/68]  eta: 0:00:01  loss: 1.2439 (1.2924)  acc1: 65.6250 (58.8115)  acc5: 100.0000 (95.6455)  time: 0.1378  data: 0.1218  max mem: 4666\n",
      "[20:50:24.308897] [20:50:24.308888] [20:50:24.308914] Test:  [67/68]  eta: 0:00:00  loss: 1.2628 (1.2974)  acc1: 65.6250 (58.8832)  acc5: 100.0000 (96.0775)  time: 0.1380  data: 0.1223  max mem: 4666\n",
      "[20:50:24.377684] [20:50:24.377675] [20:50:24.377716] Test: Total time: 0:00:10 (0.1502 s / it)\n",
      "[20:50:24.377756] [20:50:24.377753] [20:50:24.377762] * Acc@1 58.883 Acc@5 96.078 loss 1.297\n",
      "[20:50:24.377931] [20:50:24.377927] [20:50:24.377938] Accuracy of the network on the 2167 test images: 58.9%\n",
      "[20:50:24.377948] [20:50:24.377947] [20:50:24.377954] Max accuracy: 58.88%\n",
      "[20:50:25.991792] [20:50:25.991766] [20:50:25.991993] Epoch: [20]  [  0/316]  eta: 0:08:29  lr: 0.000235  loss: 1.5522 (1.5522)  time: 1.6124  data: 1.5601  max mem: 4666\n",
      "[20:50:28.385152] [20:50:28.385142] [20:50:28.385203] Epoch: [20]  [ 20/316]  eta: 0:00:56  lr: 0.000235  loss: 1.6554 (1.6630)  time: 0.1196  data: 0.0679  max mem: 4666\n",
      "[20:50:30.905441] [20:50:30.905429] [20:50:30.905495] Epoch: [20]  [ 40/316]  eta: 0:00:43  lr: 0.000235  loss: 1.6790 (1.6710)  time: 0.1260  data: 0.0745  max mem: 4666\n",
      "[20:50:33.323055] [20:50:33.323044] [20:50:33.323206] Epoch: [20]  [ 60/316]  eta: 0:00:37  lr: 0.000235  loss: 1.6924 (1.6762)  time: 0.1208  data: 0.0694  max mem: 4666\n",
      "[20:50:35.758405] [20:50:35.758394] [20:50:35.758457] Epoch: [20]  [ 80/316]  eta: 0:00:33  lr: 0.000234  loss: 1.6987 (1.6760)  time: 0.1217  data: 0.0703  max mem: 4666\n",
      "[20:50:38.200853] [20:50:38.200843] [20:50:38.200909] Epoch: [20]  [100/316]  eta: 0:00:29  lr: 0.000234  loss: 1.7026 (1.6776)  time: 0.1221  data: 0.0707  max mem: 4666\n",
      "[20:50:40.707306] [20:50:40.707295] [20:50:40.707361] Epoch: [20]  [120/316]  eta: 0:00:26  lr: 0.000234  loss: 1.6368 (1.6739)  time: 0.1253  data: 0.0738  max mem: 4666\n",
      "[20:50:43.180083] [20:50:43.180073] [20:50:43.180212] Epoch: [20]  [140/316]  eta: 0:00:23  lr: 0.000234  loss: 1.6763 (1.6743)  time: 0.1236  data: 0.0722  max mem: 4666\n",
      "[20:50:45.671772] [20:50:45.671762] [20:50:45.671822] Epoch: [20]  [160/316]  eta: 0:00:20  lr: 0.000234  loss: 1.5987 (1.6664)  time: 0.1245  data: 0.0731  max mem: 4666\n",
      "[20:50:48.112673] [20:50:48.112664] [20:50:48.112796] Epoch: [20]  [180/316]  eta: 0:00:17  lr: 0.000234  loss: 1.6378 (1.6642)  time: 0.1220  data: 0.0706  max mem: 4666\n",
      "[20:50:50.532985] [20:50:50.532972] [20:50:50.533040] Epoch: [20]  [200/316]  eta: 0:00:15  lr: 0.000234  loss: 1.6299 (1.6620)  time: 0.1210  data: 0.0696  max mem: 4666\n",
      "[20:50:52.984390] [20:50:52.984378] [20:50:52.984755] Epoch: [20]  [220/316]  eta: 0:00:12  lr: 0.000234  loss: 1.6416 (1.6637)  time: 0.1225  data: 0.0709  max mem: 4666\n",
      "[20:50:55.443565] [20:50:55.443552] [20:50:55.443699] Epoch: [20]  [240/316]  eta: 0:00:09  lr: 0.000233  loss: 1.6720 (1.6625)  time: 0.1229  data: 0.0712  max mem: 4666\n",
      "[20:50:57.924751] [20:50:57.924740] [20:50:57.924831] Epoch: [20]  [260/316]  eta: 0:00:07  lr: 0.000233  loss: 1.6017 (1.6603)  time: 0.1240  data: 0.0724  max mem: 4666\n",
      "[20:51:00.361226] [20:51:00.361216] [20:51:00.361286] Epoch: [20]  [280/316]  eta: 0:00:04  lr: 0.000233  loss: 1.6748 (1.6617)  time: 0.1218  data: 0.0702  max mem: 4666\n",
      "[20:51:02.779166] [20:51:02.779157] [20:51:02.779218] Epoch: [20]  [300/316]  eta: 0:00:02  lr: 0.000233  loss: 1.5982 (1.6615)  time: 0.1209  data: 0.0692  max mem: 4666\n",
      "[20:51:04.307689] [20:51:04.307679] [20:51:04.307736] Epoch: [20]  [315/316]  eta: 0:00:00  lr: 0.000233  loss: 1.6530 (1.6618)  time: 0.1209  data: 0.0694  max mem: 4666\n",
      "[20:51:04.384709] [20:51:04.384697] [20:51:04.384731] Epoch: [20] Total time: 0:00:40 (0.1266 s / it)\n",
      "[20:51:04.384912] [20:51:04.384906] [20:51:04.384924] Averaged stats: lr: 0.000233  loss: 1.6530 (1.6618)\n",
      "[20:51:07.799997] [20:51:07.799978] [20:51:07.800152] Test:  [ 0/68]  eta: 0:01:59  loss: 0.9878 (0.9878)  acc1: 71.8750 (71.8750)  acc5: 100.0000 (100.0000)  time: 1.7517  data: 1.7346  max mem: 4666\n",
      "[20:51:09.190261] [20:51:09.190248] [20:51:09.190325] Test:  [10/68]  eta: 0:00:16  loss: 1.1384 (1.1081)  acc1: 68.7500 (65.6250)  acc5: 100.0000 (98.2955)  time: 0.2855  data: 0.2690  max mem: 4666\n",
      "[20:51:10.730230] [20:51:10.730220] [20:51:10.730335] Test:  [20/68]  eta: 0:00:10  loss: 0.8314 (0.9447)  acc1: 75.0000 (74.1071)  acc5: 100.0000 (98.6607)  time: 0.1464  data: 0.1302  max mem: 4666\n",
      "[20:51:12.121483] [20:51:12.121472] [20:51:12.121722] Test:  [30/68]  eta: 0:00:07  loss: 0.9147 (1.1524)  acc1: 75.0000 (65.6250)  acc5: 100.0000 (94.4556)  time: 0.1465  data: 0.1305  max mem: 4666\n",
      "[20:51:13.519515] [20:51:13.519503] [20:51:13.519579] Test:  [40/68]  eta: 0:00:05  loss: 1.4466 (1.2379)  acc1: 34.3750 (58.9177)  acc5: 96.8750 (94.5884)  time: 0.1394  data: 0.1235  max mem: 4666\n",
      "[20:51:14.908203] [20:51:14.908192] [20:51:14.908259] Test:  [50/68]  eta: 0:00:03  loss: 1.4466 (1.2407)  acc1: 37.5000 (58.0270)  acc5: 100.0000 (95.6495)  time: 0.1393  data: 0.1234  max mem: 4666\n",
      "[20:51:16.262233] [20:51:16.262216] [20:51:16.262296] Test:  [60/68]  eta: 0:00:01  loss: 1.2240 (1.2479)  acc1: 59.3750 (57.9918)  acc5: 100.0000 (96.3627)  time: 0.1371  data: 0.1212  max mem: 4666\n",
      "[20:51:16.407530] [20:51:16.407524] [20:51:16.407542] Test:  [67/68]  eta: 0:00:00  loss: 1.2309 (1.2534)  acc1: 56.2500 (56.1606)  acc5: 100.0000 (96.7236)  time: 0.1375  data: 0.1218  max mem: 4666\n",
      "[20:51:16.483316] [20:51:16.483309] [20:51:16.483346] Test: Total time: 0:00:10 (0.1535 s / it)\n",
      "[20:51:16.483386] [20:51:16.483383] [20:51:16.483392] * Acc@1 56.161 Acc@5 96.724 loss 1.253\n",
      "[20:51:16.483705] [20:51:16.483700] [20:51:16.483713] Accuracy of the network on the 2167 test images: 56.2%\n",
      "[20:51:16.483723] [20:51:16.483721] [20:51:16.483728] Max accuracy: 58.88%\n",
      "[20:51:18.024751] [20:51:18.024730] [20:51:18.024928] Epoch: [21]  [  0/316]  eta: 0:08:06  lr: 0.000233  loss: 1.5927 (1.5927)  time: 1.5396  data: 1.4890  max mem: 4666\n",
      "[20:51:20.460604] [20:51:20.460594] [20:51:20.460658] Epoch: [21]  [ 20/316]  eta: 0:00:56  lr: 0.000233  loss: 1.6034 (1.6287)  time: 0.1217  data: 0.0700  max mem: 4666\n",
      "[20:51:22.887521] [20:51:22.887510] [20:51:22.887571] Epoch: [21]  [ 40/316]  eta: 0:00:43  lr: 0.000233  loss: 1.6386 (1.6345)  time: 0.1213  data: 0.0696  max mem: 4666\n",
      "[20:51:25.325369] [20:51:25.325359] [20:51:25.325419] Epoch: [21]  [ 60/316]  eta: 0:00:37  lr: 0.000233  loss: 1.6565 (1.6399)  time: 0.1218  data: 0.0702  max mem: 4666\n",
      "[20:51:27.784738] [20:51:27.784727] [20:51:27.784804] Epoch: [21]  [ 80/316]  eta: 0:00:32  lr: 0.000232  loss: 1.6924 (1.6582)  time: 0.1229  data: 0.0713  max mem: 4666\n",
      "[20:51:30.215782] [20:51:30.215771] [20:51:30.215836] Epoch: [21]  [100/316]  eta: 0:00:29  lr: 0.000232  loss: 1.6762 (1.6654)  time: 0.1215  data: 0.0699  max mem: 4666\n",
      "[20:51:32.663197] [20:51:32.663186] [20:51:32.663247] Epoch: [21]  [120/316]  eta: 0:00:26  lr: 0.000232  loss: 1.6059 (1.6595)  time: 0.1223  data: 0.0707  max mem: 4666\n",
      "[20:51:35.110013] [20:51:35.110003] [20:51:35.110064] Epoch: [21]  [140/316]  eta: 0:00:23  lr: 0.000232  loss: 1.6895 (1.6644)  time: 0.1223  data: 0.0707  max mem: 4666\n",
      "[20:51:37.611555] [20:51:37.611545] [20:51:37.611605] Epoch: [21]  [160/316]  eta: 0:00:20  lr: 0.000232  loss: 1.6088 (1.6633)  time: 0.1250  data: 0.0734  max mem: 4666\n",
      "[20:51:40.063325] [20:51:40.063313] [20:51:40.063384] Epoch: [21]  [180/316]  eta: 0:00:17  lr: 0.000232  loss: 1.6774 (1.6634)  time: 0.1225  data: 0.0709  max mem: 4666\n",
      "[20:51:42.549823] [20:51:42.549814] [20:51:42.549874] Epoch: [21]  [200/316]  eta: 0:00:15  lr: 0.000232  loss: 1.6346 (1.6602)  time: 0.1243  data: 0.0726  max mem: 4666\n",
      "[20:51:45.035724] [20:51:45.035714] [20:51:45.035778] Epoch: [21]  [220/316]  eta: 0:00:12  lr: 0.000232  loss: 1.5848 (1.6552)  time: 0.1242  data: 0.0726  max mem: 4666\n",
      "[20:51:47.464830] [20:51:47.464820] [20:51:47.464885] Epoch: [21]  [240/316]  eta: 0:00:09  lr: 0.000231  loss: 1.6633 (1.6560)  time: 0.1214  data: 0.0697  max mem: 4666\n",
      "[20:51:49.928733] [20:51:49.928723] [20:51:49.928797] Epoch: [21]  [260/316]  eta: 0:00:07  lr: 0.000231  loss: 1.6728 (1.6549)  time: 0.1231  data: 0.0715  max mem: 4666\n",
      "[20:51:52.386215] [20:51:52.386204] [20:51:52.386267] Epoch: [21]  [280/316]  eta: 0:00:04  lr: 0.000231  loss: 1.6120 (1.6536)  time: 0.1228  data: 0.0712  max mem: 4666\n",
      "[20:51:54.757614] [20:51:54.757604] [20:51:54.757669] Epoch: [21]  [300/316]  eta: 0:00:02  lr: 0.000231  loss: 1.6842 (1.6531)  time: 0.1185  data: 0.0669  max mem: 4666\n",
      "[20:51:56.457591] [20:51:56.457582] [20:51:56.457640] Epoch: [21]  [315/316]  eta: 0:00:00  lr: 0.000231  loss: 1.6794 (1.6538)  time: 0.1211  data: 0.0697  max mem: 4666\n",
      "[20:51:56.535342] [20:51:56.535337] [20:51:56.535353] Epoch: [21] Total time: 0:00:40 (0.1267 s / it)\n",
      "[20:51:56.535381] [20:51:56.535379] [20:51:56.535387] Averaged stats: lr: 0.000231  loss: 1.6794 (1.6538)\n",
      "[20:51:59.925472] [20:51:59.925447] [20:51:59.925652] Test:  [ 0/68]  eta: 0:01:56  loss: 0.9911 (0.9911)  acc1: 71.8750 (71.8750)  acc5: 100.0000 (100.0000)  time: 1.7140  data: 1.6969  max mem: 4666\n",
      "[20:52:01.284813] [20:52:01.284803] [20:52:01.284892] Test:  [10/68]  eta: 0:00:16  loss: 1.1366 (1.1047)  acc1: 68.7500 (62.5000)  acc5: 100.0000 (98.5795)  time: 0.2793  data: 0.2630  max mem: 4666\n",
      "[20:52:02.721567] [20:52:02.721557] [20:52:02.721632] Test:  [20/68]  eta: 0:00:10  loss: 0.7804 (0.8940)  acc1: 75.0000 (73.5119)  acc5: 100.0000 (98.8095)  time: 0.1397  data: 0.1236  max mem: 4666\n",
      "[20:52:04.125956] [20:52:04.125945] [20:52:04.126010] Test:  [30/68]  eta: 0:00:07  loss: 0.7804 (1.1322)  acc1: 71.8750 (63.9113)  acc5: 100.0000 (92.8427)  time: 0.1420  data: 0.1260  max mem: 4666\n",
      "[20:52:05.528340] [20:52:05.528328] [20:52:05.528397] Test:  [40/68]  eta: 0:00:04  loss: 1.3702 (1.2223)  acc1: 37.5000 (56.7073)  acc5: 96.8750 (93.3689)  time: 0.1403  data: 0.1243  max mem: 4666\n",
      "[20:52:06.905746] [20:52:06.905736] [20:52:06.905798] Test:  [50/68]  eta: 0:00:03  loss: 1.3681 (1.2100)  acc1: 37.5000 (56.8015)  acc5: 100.0000 (94.6691)  time: 0.1389  data: 0.1228  max mem: 4666\n",
      "[20:52:08.288453] [20:52:08.288439] [20:52:08.288514] Test:  [60/68]  eta: 0:00:01  loss: 1.2239 (1.2319)  acc1: 59.3750 (56.7111)  acc5: 100.0000 (95.5430)  time: 0.1379  data: 0.1218  max mem: 4666\n",
      "[20:52:08.415005] [20:52:08.414999] [20:52:08.415016] Test:  [67/68]  eta: 0:00:00  loss: 1.0942 (1.2227)  acc1: 59.3750 (57.5911)  acc5: 100.0000 (95.9852)  time: 0.1380  data: 0.1224  max mem: 4666\n",
      "[20:52:08.479244] [20:52:08.479235] [20:52:08.479261] Test: Total time: 0:00:10 (0.1510 s / it)\n",
      "[20:52:08.479297] [20:52:08.479295] [20:52:08.479303] * Acc@1 57.591 Acc@5 95.985 loss 1.223\n",
      "[20:52:08.479401] [20:52:08.479398] [20:52:08.479407] Accuracy of the network on the 2167 test images: 57.6%\n",
      "[20:52:08.479417] [20:52:08.479415] [20:52:08.479422] Max accuracy: 58.88%\n",
      "[20:52:10.104540] [20:52:10.104522] [20:52:10.104683] Epoch: [22]  [  0/316]  eta: 0:08:33  lr: 0.000231  loss: 1.5361 (1.5361)  time: 1.6238  data: 1.5740  max mem: 4666\n",
      "[20:52:12.561469] [20:52:12.561458] [20:52:12.561534] Epoch: [22]  [ 20/316]  eta: 0:00:57  lr: 0.000231  loss: 1.6366 (1.6306)  time: 0.1228  data: 0.0714  max mem: 4666\n",
      "[20:52:14.982412] [20:52:14.982400] [20:52:14.982650] Epoch: [22]  [ 40/316]  eta: 0:00:43  lr: 0.000231  loss: 1.6241 (1.6315)  time: 0.1210  data: 0.0696  max mem: 4666\n",
      "[20:52:17.431177] [20:52:17.431167] [20:52:17.431243] Epoch: [22]  [ 60/316]  eta: 0:00:37  lr: 0.000230  loss: 1.6742 (1.6365)  time: 0.1224  data: 0.0709  max mem: 4666\n",
      "[20:52:19.877165] [20:52:19.877155] [20:52:19.877234] Epoch: [22]  [ 80/316]  eta: 0:00:33  lr: 0.000230  loss: 1.6969 (1.6499)  time: 0.1222  data: 0.0708  max mem: 4666\n",
      "[20:52:22.343023] [20:52:22.343011] [20:52:22.343153] Epoch: [22]  [100/316]  eta: 0:00:29  lr: 0.000230  loss: 1.6657 (1.6524)  time: 0.1232  data: 0.0718  max mem: 4666\n",
      "[20:52:24.864901] [20:52:24.864891] [20:52:24.864953] Epoch: [22]  [120/316]  eta: 0:00:26  lr: 0.000230  loss: 1.6517 (1.6570)  time: 0.1260  data: 0.0746  max mem: 4666\n",
      "[20:52:27.343653] [20:52:27.343642] [20:52:27.343704] Epoch: [22]  [140/316]  eta: 0:00:23  lr: 0.000230  loss: 1.6532 (1.6609)  time: 0.1239  data: 0.0725  max mem: 4666\n",
      "[20:52:29.835619] [20:52:29.835608] [20:52:29.835671] Epoch: [22]  [160/316]  eta: 0:00:20  lr: 0.000230  loss: 1.6144 (1.6540)  time: 0.1246  data: 0.0731  max mem: 4666\n",
      "[20:52:32.339731] [20:52:32.339720] [20:52:32.339787] Epoch: [22]  [180/316]  eta: 0:00:17  lr: 0.000230  loss: 1.6732 (1.6554)  time: 0.1252  data: 0.0738  max mem: 4666\n",
      "[20:52:34.790558] [20:52:34.790548] [20:52:34.790611] Epoch: [22]  [200/316]  eta: 0:00:15  lr: 0.000229  loss: 1.6356 (1.6533)  time: 0.1225  data: 0.0711  max mem: 4666\n",
      "[20:52:37.300003] [20:52:37.299993] [20:52:37.300054] Epoch: [22]  [220/316]  eta: 0:00:12  lr: 0.000229  loss: 1.7076 (1.6571)  time: 0.1254  data: 0.0741  max mem: 4666\n",
      "[20:52:39.749870] [20:52:39.749858] [20:52:39.749936] Epoch: [22]  [240/316]  eta: 0:00:09  lr: 0.000229  loss: 1.6786 (1.6558)  time: 0.1225  data: 0.0711  max mem: 4666\n",
      "[20:52:42.229068] [20:52:42.229058] [20:52:42.229122] Epoch: [22]  [260/316]  eta: 0:00:07  lr: 0.000229  loss: 1.6790 (1.6548)  time: 0.1239  data: 0.0726  max mem: 4666\n",
      "[20:52:44.674247] [20:52:44.674237] [20:52:44.674297] Epoch: [22]  [280/316]  eta: 0:00:04  lr: 0.000229  loss: 1.6484 (1.6551)  time: 0.1222  data: 0.0708  max mem: 4666\n",
      "[20:52:47.119406] [20:52:47.119396] [20:52:47.119455] Epoch: [22]  [300/316]  eta: 0:00:02  lr: 0.000229  loss: 1.6898 (1.6550)  time: 0.1222  data: 0.0709  max mem: 4666\n",
      "[20:52:48.570038] [20:52:48.570029] [20:52:48.570082] Epoch: [22]  [315/316]  eta: 0:00:00  lr: 0.000229  loss: 1.6898 (1.6580)  time: 0.1204  data: 0.0692  max mem: 4666\n",
      "[20:52:48.641662] [20:52:48.641648] [20:52:48.641689] Epoch: [22] Total time: 0:00:40 (0.1271 s / it)\n",
      "[20:52:48.641770] [20:52:48.641764] [20:52:48.641783] Averaged stats: lr: 0.000229  loss: 1.6898 (1.6580)\n",
      "[20:52:52.066726] [20:52:52.066703] [20:52:52.066896] Test:  [ 0/68]  eta: 0:01:56  loss: 1.0601 (1.0601)  acc1: 68.7500 (68.7500)  acc5: 100.0000 (100.0000)  time: 1.7115  data: 1.6942  max mem: 4666\n",
      "[20:52:53.445000] [20:52:53.444988] [20:52:53.445055] Test:  [10/68]  eta: 0:00:16  loss: 1.2452 (1.1727)  acc1: 68.7500 (61.0795)  acc5: 100.0000 (98.0114)  time: 0.2808  data: 0.2643  max mem: 4666\n",
      "[20:52:54.847967] [20:52:54.847957] [20:52:54.848036] Test:  [20/68]  eta: 0:00:10  loss: 0.7598 (0.9321)  acc1: 78.1250 (73.0655)  acc5: 100.0000 (98.6607)  time: 0.1390  data: 0.1228  max mem: 4666\n",
      "[20:52:56.237409] [20:52:56.237398] [20:52:56.237464] Test:  [30/68]  eta: 0:00:07  loss: 0.7983 (1.1346)  acc1: 78.1250 (65.0202)  acc5: 100.0000 (94.1532)  time: 0.1396  data: 0.1236  max mem: 4666\n",
      "[20:52:57.629774] [20:52:57.629764] [20:52:57.629842] Test:  [40/68]  eta: 0:00:04  loss: 1.4650 (1.2405)  acc1: 34.3750 (57.0884)  acc5: 93.7500 (94.3598)  time: 0.1390  data: 0.1230  max mem: 4666\n",
      "[20:52:59.016368] [20:52:59.016357] [20:52:59.016424] Test:  [50/68]  eta: 0:00:03  loss: 1.4403 (1.2208)  acc1: 31.2500 (57.4142)  acc5: 100.0000 (95.4657)  time: 0.1389  data: 0.1229  max mem: 4666\n",
      "[20:53:00.372546] [20:53:00.372536] [20:53:00.372635] Test:  [60/68]  eta: 0:00:01  loss: 1.1380 (1.2149)  acc1: 62.5000 (58.5553)  acc5: 100.0000 (96.1578)  time: 0.1371  data: 0.1211  max mem: 4666\n",
      "[20:53:00.488162] [20:53:00.488152] [20:53:00.488180] Test:  [67/68]  eta: 0:00:00  loss: 1.0880 (1.2145)  acc1: 62.5000 (58.6064)  acc5: 100.0000 (96.5390)  time: 0.1372  data: 0.1215  max mem: 4666\n",
      "[20:53:00.557494] [20:53:00.557484] [20:53:00.557512] Test: Total time: 0:00:10 (0.1500 s / it)\n",
      "[20:53:00.557547] [20:53:00.557545] [20:53:00.557553] * Acc@1 58.606 Acc@5 96.539 loss 1.214\n",
      "[20:53:00.557756] [20:53:00.557752] [20:53:00.557764] Accuracy of the network on the 2167 test images: 58.6%\n",
      "[20:53:00.557774] [20:53:00.557772] [20:53:00.557779] Max accuracy: 58.88%\n",
      "[20:53:02.174252] [20:53:02.174236] [20:53:02.174324] Epoch: [23]  [  0/316]  eta: 0:08:30  lr: 0.000229  loss: 1.4715 (1.4715)  time: 1.6152  data: 1.5656  max mem: 4666\n",
      "[20:53:04.588636] [20:53:04.588625] [20:53:04.588722] Epoch: [23]  [ 20/316]  eta: 0:00:56  lr: 0.000228  loss: 1.6792 (1.6126)  time: 0.1207  data: 0.0692  max mem: 4666\n",
      "[20:53:06.988322] [20:53:06.988312] [20:53:06.988377] Epoch: [23]  [ 40/316]  eta: 0:00:43  lr: 0.000228  loss: 1.6665 (1.6423)  time: 0.1199  data: 0.0685  max mem: 4666\n",
      "[20:53:09.447651] [20:53:09.447641] [20:53:09.447707] Epoch: [23]  [ 60/316]  eta: 0:00:37  lr: 0.000228  loss: 1.6589 (1.6426)  time: 0.1229  data: 0.0715  max mem: 4666\n",
      "[20:53:11.893611] [20:53:11.893601] [20:53:11.893666] Epoch: [23]  [ 80/316]  eta: 0:00:33  lr: 0.000228  loss: 1.6892 (1.6492)  time: 0.1223  data: 0.0709  max mem: 4666\n",
      "[20:53:14.369566] [20:53:14.369556] [20:53:14.369626] Epoch: [23]  [100/316]  eta: 0:00:29  lr: 0.000228  loss: 1.6554 (1.6517)  time: 0.1237  data: 0.0723  max mem: 4666\n",
      "[20:53:16.838111] [20:53:16.838100] [20:53:16.838247] Epoch: [23]  [120/316]  eta: 0:00:26  lr: 0.000228  loss: 1.6316 (1.6507)  time: 0.1234  data: 0.0719  max mem: 4666\n",
      "[20:53:19.316566] [20:53:19.316557] [20:53:19.316806] Epoch: [23]  [140/316]  eta: 0:00:23  lr: 0.000228  loss: 1.6417 (1.6524)  time: 0.1239  data: 0.0724  max mem: 4666\n",
      "[20:53:21.733140] [20:53:21.733130] [20:53:21.733198] Epoch: [23]  [160/316]  eta: 0:00:20  lr: 0.000227  loss: 1.6783 (1.6564)  time: 0.1208  data: 0.0693  max mem: 4666\n",
      "[20:53:24.197010] [20:53:24.196998] [20:53:24.197167] Epoch: [23]  [180/316]  eta: 0:00:17  lr: 0.000227  loss: 1.6395 (1.6556)  time: 0.1231  data: 0.0717  max mem: 4666\n",
      "[20:53:26.603050] [20:53:26.603039] [20:53:26.603104] Epoch: [23]  [200/316]  eta: 0:00:15  lr: 0.000227  loss: 1.5910 (1.6537)  time: 0.1203  data: 0.0689  max mem: 4666\n",
      "[20:53:29.042683] [20:53:29.042672] [20:53:29.042736] Epoch: [23]  [220/316]  eta: 0:00:12  lr: 0.000227  loss: 1.6597 (1.6526)  time: 0.1219  data: 0.0705  max mem: 4666\n",
      "[20:53:31.494148] [20:53:31.494137] [20:53:31.494210] Epoch: [23]  [240/316]  eta: 0:00:09  lr: 0.000227  loss: 1.6411 (1.6518)  time: 0.1225  data: 0.0709  max mem: 4666\n",
      "[20:53:33.977994] [20:53:33.977983] [20:53:33.978048] Epoch: [23]  [260/316]  eta: 0:00:07  lr: 0.000227  loss: 1.6088 (1.6505)  time: 0.1242  data: 0.0722  max mem: 4666\n",
      "[20:53:36.431690] [20:53:36.431680] [20:53:36.431742] Epoch: [23]  [280/316]  eta: 0:00:04  lr: 0.000226  loss: 1.6368 (1.6526)  time: 0.1226  data: 0.0713  max mem: 4666\n",
      "[20:53:38.851303] [20:53:38.851293] [20:53:38.851350] Epoch: [23]  [300/316]  eta: 0:00:02  lr: 0.000226  loss: 1.5920 (1.6494)  time: 0.1209  data: 0.0695  max mem: 4666\n",
      "[20:53:40.639640] [20:53:40.639631] [20:53:40.639693] Epoch: [23]  [315/316]  eta: 0:00:00  lr: 0.000226  loss: 1.6319 (1.6499)  time: 0.1224  data: 0.0713  max mem: 4666\n",
      "[20:53:40.715780] [20:53:40.715770] [20:53:40.715798] Epoch: [23] Total time: 0:00:40 (0.1271 s / it)\n",
      "[20:53:40.715984] [20:53:40.715979] [20:53:40.715992] Averaged stats: lr: 0.000226  loss: 1.6319 (1.6499)\n",
      "[20:53:44.150074] [20:53:44.150048] [20:53:44.150288] Test:  [ 0/68]  eta: 0:01:57  loss: 0.9032 (0.9032)  acc1: 75.0000 (75.0000)  acc5: 100.0000 (100.0000)  time: 1.7208  data: 1.7034  max mem: 4666\n",
      "[20:53:45.524154] [20:53:45.524145] [20:53:45.524202] Test:  [10/68]  eta: 0:00:16  loss: 1.0854 (1.0458)  acc1: 68.7500 (65.6250)  acc5: 100.0000 (98.0114)  time: 0.2813  data: 0.2647  max mem: 4666\n",
      "[20:53:46.964239] [20:53:46.964229] [20:53:46.964462] Test:  [20/68]  eta: 0:00:10  loss: 0.8085 (0.8917)  acc1: 81.2500 (75.4464)  acc5: 100.0000 (98.5119)  time: 0.1406  data: 0.1244  max mem: 4666\n",
      "[20:53:48.380351] [20:53:48.380340] [20:53:48.380409] Test:  [30/68]  eta: 0:00:07  loss: 0.8565 (1.1437)  acc1: 75.0000 (64.4153)  acc5: 100.0000 (92.0363)  time: 0.1427  data: 0.1267  max mem: 4666\n",
      "[20:53:49.783516] [20:53:49.783505] [20:53:49.783596] Test:  [40/68]  eta: 0:00:05  loss: 1.4694 (1.2228)  acc1: 37.5000 (57.9268)  acc5: 96.8750 (92.6829)  time: 0.1409  data: 0.1249  max mem: 4666\n",
      "[20:53:51.145613] [20:53:51.145603] [20:53:51.145669] Test:  [50/68]  eta: 0:00:03  loss: 1.4464 (1.2331)  acc1: 40.6250 (56.5564)  acc5: 100.0000 (94.0564)  time: 0.1382  data: 0.1222  max mem: 4666\n",
      "[20:53:52.522410] [20:53:52.522400] [20:53:52.522469] Test:  [60/68]  eta: 0:00:01  loss: 1.2950 (1.2377)  acc1: 50.0000 (56.9672)  acc5: 100.0000 (95.0307)  time: 0.1369  data: 0.1209  max mem: 4666\n",
      "[20:53:52.659378] [20:53:52.659372] [20:53:52.659390] Test:  [67/68]  eta: 0:00:00  loss: 1.0460 (1.2230)  acc1: 65.6250 (57.8219)  acc5: 100.0000 (95.5238)  time: 0.1377  data: 0.1220  max mem: 4666\n",
      "[20:53:52.741705] [20:53:52.741692] [20:53:52.741767] Test: Total time: 0:00:10 (0.1517 s / it)\n",
      "[20:53:52.741851] [20:53:52.741846] [20:53:52.741866] * Acc@1 57.822 Acc@5 95.524 loss 1.223\n",
      "[20:53:52.742222] [20:53:52.742210] [20:53:52.742242] Accuracy of the network on the 2167 test images: 57.8%\n",
      "[20:53:52.742265] [20:53:52.742262] [20:53:52.742278] Max accuracy: 58.88%\n",
      "[20:53:54.379427] [20:53:54.379408] [20:53:54.379589] Epoch: [24]  [  0/316]  eta: 0:08:36  lr: 0.000226  loss: 1.5943 (1.5943)  time: 1.6351  data: 1.5841  max mem: 4666\n",
      "[20:53:56.807855] [20:53:56.807844] [20:53:56.807905] Epoch: [24]  [ 20/316]  eta: 0:00:57  lr: 0.000226  loss: 1.6109 (1.6400)  time: 0.1214  data: 0.0699  max mem: 4666\n",
      "[20:53:59.246838] [20:53:59.246828] [20:53:59.246893] Epoch: [24]  [ 40/316]  eta: 0:00:43  lr: 0.000226  loss: 1.6468 (1.6468)  time: 0.1219  data: 0.0704  max mem: 4666\n",
      "[20:54:01.640066] [20:54:01.640056] [20:54:01.640121] Epoch: [24]  [ 60/316]  eta: 0:00:37  lr: 0.000226  loss: 1.6318 (1.6467)  time: 0.1196  data: 0.0682  max mem: 4666\n",
      "[20:54:04.075783] [20:54:04.075772] [20:54:04.075838] Epoch: [24]  [ 80/316]  eta: 0:00:33  lr: 0.000226  loss: 1.6672 (1.6579)  time: 0.1217  data: 0.0703  max mem: 4666\n",
      "[20:54:06.549921] [20:54:06.549910] [20:54:06.549977] Epoch: [24]  [100/316]  eta: 0:00:29  lr: 0.000225  loss: 1.6060 (1.6552)  time: 0.1237  data: 0.0723  max mem: 4666\n",
      "[20:54:09.013425] [20:54:09.013415] [20:54:09.013481] Epoch: [24]  [120/316]  eta: 0:00:26  lr: 0.000225  loss: 1.6549 (1.6563)  time: 0.1231  data: 0.0717  max mem: 4666\n",
      "[20:54:11.452976] [20:54:11.452966] [20:54:11.453029] Epoch: [24]  [140/316]  eta: 0:00:23  lr: 0.000225  loss: 1.6363 (1.6520)  time: 0.1219  data: 0.0705  max mem: 4666\n",
      "[20:54:13.892529] [20:54:13.892518] [20:54:13.892587] Epoch: [24]  [160/316]  eta: 0:00:20  lr: 0.000225  loss: 1.5962 (1.6464)  time: 0.1219  data: 0.0705  max mem: 4666\n",
      "[20:54:16.374483] [20:54:16.374471] [20:54:16.374538] Epoch: [24]  [180/316]  eta: 0:00:17  lr: 0.000225  loss: 1.6264 (1.6471)  time: 0.1241  data: 0.0726  max mem: 4666\n",
      "[20:54:18.815995] [20:54:18.815984] [20:54:18.816053] Epoch: [24]  [200/316]  eta: 0:00:15  lr: 0.000225  loss: 1.5853 (1.6414)  time: 0.1220  data: 0.0706  max mem: 4666\n",
      "[20:54:21.269296] [20:54:21.269285] [20:54:21.269351] Epoch: [24]  [220/316]  eta: 0:00:12  lr: 0.000225  loss: 1.6450 (1.6413)  time: 0.1226  data: 0.0712  max mem: 4666\n",
      "[20:54:23.751866] [20:54:23.751855] [20:54:23.751984] Epoch: [24]  [240/316]  eta: 0:00:09  lr: 0.000224  loss: 1.6213 (1.6418)  time: 0.1241  data: 0.0726  max mem: 4666\n",
      "[20:54:26.173545] [20:54:26.173535] [20:54:26.173598] Epoch: [24]  [260/316]  eta: 0:00:07  lr: 0.000224  loss: 1.6355 (1.6421)  time: 0.1210  data: 0.0696  max mem: 4666\n",
      "[20:54:28.630839] [20:54:28.630829] [20:54:28.630898] Epoch: [24]  [280/316]  eta: 0:00:04  lr: 0.000224  loss: 1.6267 (1.6424)  time: 0.1228  data: 0.0714  max mem: 4666\n",
      "[20:54:31.082630] [20:54:31.082620] [20:54:31.082682] Epoch: [24]  [300/316]  eta: 0:00:02  lr: 0.000224  loss: 1.6877 (1.6449)  time: 0.1225  data: 0.0712  max mem: 4666\n",
      "[20:54:32.765282] [20:54:32.765272] [20:54:32.765326] Epoch: [24]  [315/316]  eta: 0:00:00  lr: 0.000224  loss: 1.6631 (1.6464)  time: 0.1218  data: 0.0706  max mem: 4666\n",
      "[20:54:32.846064] [20:54:32.846058] [20:54:32.846079] Epoch: [24] Total time: 0:00:40 (0.1269 s / it)\n",
      "[20:54:32.846132] [20:54:32.846130] [20:54:32.846139] Averaged stats: lr: 0.000224  loss: 1.6631 (1.6464)\n",
      "[20:54:36.351394] [20:54:36.351369] [20:54:36.351587] Test:  [ 0/68]  eta: 0:01:56  loss: 1.0704 (1.0704)  acc1: 75.0000 (75.0000)  acc5: 100.0000 (100.0000)  time: 1.7159  data: 1.6988  max mem: 4666\n",
      "[20:54:37.733852] [20:54:37.733841] [20:54:37.733906] Test:  [10/68]  eta: 0:00:16  loss: 1.2451 (1.1635)  acc1: 65.6250 (62.2159)  acc5: 96.8750 (98.2955)  time: 0.2816  data: 0.2651  max mem: 4666\n",
      "[20:54:39.123606] [20:54:39.123595] [20:54:39.123691] Test:  [20/68]  eta: 0:00:10  loss: 0.7341 (0.9220)  acc1: 81.2500 (74.5536)  acc5: 100.0000 (98.6607)  time: 0.1385  data: 0.1223  max mem: 4666\n",
      "[20:54:40.518080] [20:54:40.518069] [20:54:40.518303] Test:  [30/68]  eta: 0:00:07  loss: 0.8275 (1.1382)  acc1: 81.2500 (64.7177)  acc5: 100.0000 (93.9516)  time: 0.1391  data: 0.1232  max mem: 4666\n",
      "[20:54:41.934417] [20:54:41.934400] [20:54:41.934482] Test:  [40/68]  eta: 0:00:04  loss: 1.5720 (1.2465)  acc1: 31.2500 (56.8598)  acc5: 93.7500 (94.2073)  time: 0.1404  data: 0.1245  max mem: 4666\n",
      "[20:54:43.315120] [20:54:43.315110] [20:54:43.315197] Test:  [50/68]  eta: 0:00:03  loss: 1.5465 (1.2534)  acc1: 31.2500 (55.9436)  acc5: 100.0000 (95.3431)  time: 0.1398  data: 0.1238  max mem: 4666\n",
      "[20:54:44.714529] [20:54:44.714515] [20:54:44.714589] Test:  [60/68]  eta: 0:00:01  loss: 1.2760 (1.2342)  acc1: 62.5000 (57.7869)  acc5: 100.0000 (96.1066)  time: 0.1389  data: 0.1230  max mem: 4666\n",
      "[20:54:44.822295] [20:54:44.822288] [20:54:44.822321] Test:  [67/68]  eta: 0:00:00  loss: 1.0440 (1.2246)  acc1: 62.5000 (57.5450)  acc5: 100.0000 (96.4467)  time: 0.1387  data: 0.1230  max mem: 4666\n",
      "[20:54:44.886964] [20:54:44.886954] [20:54:44.886982] Test: Total time: 0:00:10 (0.1508 s / it)\n",
      "[20:54:44.887018] [20:54:44.887016] [20:54:44.887024] * Acc@1 57.545 Acc@5 96.447 loss 1.225\n",
      "[20:54:44.887128] [20:54:44.887126] [20:54:44.887135] Accuracy of the network on the 2167 test images: 57.5%\n",
      "[20:54:44.887144] [20:54:44.887142] [20:54:44.887149] Max accuracy: 58.88%\n",
      "[20:54:46.483051] [20:54:46.483035] [20:54:46.483130] Epoch: [25]  [  0/316]  eta: 0:08:23  lr: 0.000224  loss: 1.6770 (1.6770)  time: 1.5946  data: 1.5417  max mem: 4666\n",
      "[20:54:48.946826] [20:54:48.946817] [20:54:48.946878] Epoch: [25]  [ 20/316]  eta: 0:00:57  lr: 0.000224  loss: 1.6073 (1.6197)  time: 0.1231  data: 0.0716  max mem: 4666\n",
      "[20:54:51.445713] [20:54:51.445700] [20:54:51.445802] Epoch: [25]  [ 40/316]  eta: 0:00:44  lr: 0.000223  loss: 1.6457 (1.6370)  time: 0.1249  data: 0.0735  max mem: 4666\n",
      "[20:54:53.897506] [20:54:53.897496] [20:54:53.897560] Epoch: [25]  [ 60/316]  eta: 0:00:37  lr: 0.000223  loss: 1.6709 (1.6385)  time: 0.1226  data: 0.0712  max mem: 4666\n",
      "[20:54:56.330519] [20:54:56.330510] [20:54:56.330571] Epoch: [25]  [ 80/316]  eta: 0:00:33  lr: 0.000223  loss: 1.6551 (1.6378)  time: 0.1216  data: 0.0702  max mem: 4666\n",
      "[20:54:58.805621] [20:54:58.805611] [20:54:58.805675] Epoch: [25]  [100/316]  eta: 0:00:29  lr: 0.000223  loss: 1.6148 (1.6337)  time: 0.1237  data: 0.0723  max mem: 4666\n",
      "[20:55:01.259113] [20:55:01.259100] [20:55:01.259242] Epoch: [25]  [120/316]  eta: 0:00:26  lr: 0.000223  loss: 1.6281 (1.6368)  time: 0.1226  data: 0.0712  max mem: 4666\n",
      "[20:55:03.745483] [20:55:03.745473] [20:55:03.745539] Epoch: [25]  [140/316]  eta: 0:00:23  lr: 0.000223  loss: 1.6826 (1.6420)  time: 0.1243  data: 0.0729  max mem: 4666\n",
      "[20:55:06.230919] [20:55:06.230908] [20:55:06.230980] Epoch: [25]  [160/316]  eta: 0:00:20  lr: 0.000222  loss: 1.5842 (1.6366)  time: 0.1242  data: 0.0731  max mem: 4666\n",
      "[20:55:08.640681] [20:55:08.640671] [20:55:08.640733] Epoch: [25]  [180/316]  eta: 0:00:17  lr: 0.000222  loss: 1.6618 (1.6398)  time: 0.1205  data: 0.0691  max mem: 4666\n",
      "[20:55:11.103902] [20:55:11.103891] [20:55:11.103957] Epoch: [25]  [200/316]  eta: 0:00:15  lr: 0.000222  loss: 1.6021 (1.6387)  time: 0.1231  data: 0.0717  max mem: 4666\n",
      "[20:55:13.570443] [20:55:13.570432] [20:55:13.570495] Epoch: [25]  [220/316]  eta: 0:00:12  lr: 0.000222  loss: 1.6090 (1.6379)  time: 0.1233  data: 0.0719  max mem: 4666\n",
      "[20:55:16.060157] [20:55:16.060147] [20:55:16.060212] Epoch: [25]  [240/316]  eta: 0:00:09  lr: 0.000222  loss: 1.6434 (1.6380)  time: 0.1244  data: 0.0730  max mem: 4666\n",
      "[20:55:18.474380] [20:55:18.474369] [20:55:18.474432] Epoch: [25]  [260/316]  eta: 0:00:07  lr: 0.000222  loss: 1.6449 (1.6366)  time: 0.1207  data: 0.0692  max mem: 4666\n",
      "[20:55:20.930013] [20:55:20.930002] [20:55:20.930071] Epoch: [25]  [280/316]  eta: 0:00:04  lr: 0.000221  loss: 1.6312 (1.6377)  time: 0.1227  data: 0.0714  max mem: 4666\n",
      "[20:55:23.331413] [20:55:23.331403] [20:55:23.331464] Epoch: [25]  [300/316]  eta: 0:00:02  lr: 0.000221  loss: 1.6401 (1.6370)  time: 0.1200  data: 0.0687  max mem: 4666\n",
      "[20:55:24.899701] [20:55:24.899692] [20:55:24.899745] Epoch: [25]  [315/316]  eta: 0:00:00  lr: 0.000221  loss: 1.5708 (1.6361)  time: 0.1212  data: 0.0699  max mem: 4666\n",
      "[20:55:24.967773] [20:55:24.967762] [20:55:24.967793] Epoch: [25] Total time: 0:00:40 (0.1268 s / it)\n",
      "[20:55:24.967853] [20:55:24.967850] [20:55:24.967860] Averaged stats: lr: 0.000221  loss: 1.5708 (1.6361)\n",
      "[20:55:28.403257] [20:55:28.403239] [20:55:28.403327] Test:  [ 0/68]  eta: 0:01:56  loss: 0.9396 (0.9396)  acc1: 78.1250 (78.1250)  acc5: 100.0000 (100.0000)  time: 1.7144  data: 1.6973  max mem: 4666\n",
      "[20:55:29.793575] [20:55:29.793565] [20:55:29.793634] Test:  [10/68]  eta: 0:00:16  loss: 1.1577 (1.0780)  acc1: 71.8750 (65.6250)  acc5: 100.0000 (98.5795)  time: 0.2822  data: 0.2657  max mem: 4666\n",
      "[20:55:31.230117] [20:55:31.230106] [20:55:31.230346] Test:  [20/68]  eta: 0:00:10  loss: 0.6087 (0.8029)  acc1: 84.3750 (78.5714)  acc5: 100.0000 (99.1071)  time: 0.1413  data: 0.1251  max mem: 4666\n",
      "[20:55:32.624443] [20:55:32.624432] [20:55:32.624494] Test:  [30/68]  eta: 0:00:07  loss: 0.6412 (1.0411)  acc1: 84.3750 (67.7419)  acc5: 100.0000 (95.3629)  time: 0.1415  data: 0.1255  max mem: 4666\n",
      "[20:55:34.037295] [20:55:34.037284] [20:55:34.037432] Test:  [40/68]  eta: 0:00:05  loss: 1.6274 (1.1755)  acc1: 31.2500 (58.7652)  acc5: 96.8750 (95.3506)  time: 0.1403  data: 0.1243  max mem: 4666\n",
      "[20:55:35.425050] [20:55:35.425040] [20:55:35.425101] Test:  [50/68]  eta: 0:00:03  loss: 1.6119 (1.1955)  acc1: 37.5000 (57.5980)  acc5: 100.0000 (96.2010)  time: 0.1400  data: 0.1240  max mem: 4666\n",
      "[20:55:36.810086] [20:55:36.810076] [20:55:36.810149] Test:  [60/68]  eta: 0:00:01  loss: 1.2259 (1.1987)  acc1: 56.2500 (57.4795)  acc5: 100.0000 (96.8238)  time: 0.1386  data: 0.1227  max mem: 4666\n",
      "[20:55:36.930599] [20:55:36.930589] [20:55:36.930616] Test:  [67/68]  eta: 0:00:00  loss: 1.1295 (1.1962)  acc1: 59.3750 (57.1297)  acc5: 100.0000 (97.0928)  time: 0.1373  data: 0.1216  max mem: 4666\n",
      "[20:55:36.998397] [20:55:36.998388] [20:55:36.998414] Test: Total time: 0:00:10 (0.1516 s / it)\n",
      "[20:55:36.998453] [20:55:36.998450] [20:55:36.998460] * Acc@1 57.130 Acc@5 97.093 loss 1.196\n",
      "[20:55:36.998562] [20:55:36.998559] [20:55:36.998569] Accuracy of the network on the 2167 test images: 57.1%\n",
      "[20:55:36.998578] [20:55:36.998577] [20:55:36.998584] Max accuracy: 58.88%\n",
      "[20:55:38.583474] [20:55:38.583454] [20:55:38.583614] Epoch: [26]  [  0/316]  eta: 0:08:20  lr: 0.000221  loss: 1.6075 (1.6075)  time: 1.5835  data: 1.5324  max mem: 4666\n",
      "[20:55:41.051445] [20:55:41.051436] [20:55:41.051497] Epoch: [26]  [ 20/316]  eta: 0:00:57  lr: 0.000221  loss: 1.5871 (1.5861)  time: 0.1234  data: 0.0717  max mem: 4666\n",
      "[20:55:43.496384] [20:55:43.496375] [20:55:43.496432] Epoch: [26]  [ 40/316]  eta: 0:00:43  lr: 0.000221  loss: 1.6413 (1.6185)  time: 0.1222  data: 0.0707  max mem: 4666\n",
      "[20:55:46.003771] [20:55:46.003761] [20:55:46.003822] Epoch: [26]  [ 60/316]  eta: 0:00:37  lr: 0.000221  loss: 1.7158 (1.6394)  time: 0.1253  data: 0.0735  max mem: 4666\n",
      "[20:55:48.399178] [20:55:48.399168] [20:55:48.399228] Epoch: [26]  [ 80/316]  eta: 0:00:33  lr: 0.000220  loss: 1.6524 (1.6451)  time: 0.1197  data: 0.0683  max mem: 4666\n",
      "[20:55:50.872256] [20:55:50.872245] [20:55:50.872323] Epoch: [26]  [100/316]  eta: 0:00:29  lr: 0.000220  loss: 1.5983 (1.6428)  time: 0.1236  data: 0.0722  max mem: 4666\n",
      "[20:55:53.322046] [20:55:53.322035] [20:55:53.322177] Epoch: [26]  [120/316]  eta: 0:00:26  lr: 0.000220  loss: 1.6424 (1.6393)  time: 0.1224  data: 0.0710  max mem: 4666\n",
      "[20:55:55.811095] [20:55:55.811084] [20:55:55.811161] Epoch: [26]  [140/316]  eta: 0:00:23  lr: 0.000220  loss: 1.6461 (1.6453)  time: 0.1244  data: 0.0729  max mem: 4666\n",
      "[20:55:58.284111] [20:55:58.284100] [20:55:58.284165] Epoch: [26]  [160/316]  eta: 0:00:20  lr: 0.000220  loss: 1.6053 (1.6465)  time: 0.1236  data: 0.0722  max mem: 4666\n",
      "[20:56:00.780431] [20:56:00.780419] [20:56:00.780483] Epoch: [26]  [180/316]  eta: 0:00:17  lr: 0.000220  loss: 1.6869 (1.6511)  time: 0.1248  data: 0.0734  max mem: 4666\n",
      "[20:56:03.218112] [20:56:03.218102] [20:56:03.218165] Epoch: [26]  [200/316]  eta: 0:00:15  lr: 0.000219  loss: 1.6008 (1.6452)  time: 0.1218  data: 0.0705  max mem: 4666\n",
      "[20:56:05.681092] [20:56:05.681081] [20:56:05.681146] Epoch: [26]  [220/316]  eta: 0:00:12  lr: 0.000219  loss: 1.6226 (1.6450)  time: 0.1231  data: 0.0717  max mem: 4666\n",
      "[20:56:08.092166] [20:56:08.092156] [20:56:08.092220] Epoch: [26]  [240/316]  eta: 0:00:09  lr: 0.000219  loss: 1.7015 (1.6484)  time: 0.1205  data: 0.0691  max mem: 4666\n",
      "[20:56:10.532755] [20:56:10.532743] [20:56:10.532886] Epoch: [26]  [260/316]  eta: 0:00:07  lr: 0.000219  loss: 1.6465 (1.6463)  time: 0.1220  data: 0.0706  max mem: 4666\n",
      "[20:56:13.034970] [20:56:13.034960] [20:56:13.035025] Epoch: [26]  [280/316]  eta: 0:00:04  lr: 0.000219  loss: 1.6552 (1.6480)  time: 0.1251  data: 0.0736  max mem: 4666\n",
      "[20:56:15.445320] [20:56:15.445310] [20:56:15.445371] Epoch: [26]  [300/316]  eta: 0:00:02  lr: 0.000219  loss: 1.6472 (1.6465)  time: 0.1205  data: 0.0691  max mem: 4666\n",
      "[20:56:16.963526] [20:56:16.963517] [20:56:16.963569] Epoch: [26]  [315/316]  eta: 0:00:00  lr: 0.000219  loss: 1.6472 (1.6479)  time: 0.1229  data: 0.0718  max mem: 4666\n",
      "[20:56:17.051367] [20:56:17.051360] [20:56:17.051382] Epoch: [26] Total time: 0:00:40 (0.1267 s / it)\n",
      "[20:56:17.051431] [20:56:17.051429] [20:56:17.051437] Averaged stats: lr: 0.000219  loss: 1.6472 (1.6479)\n",
      "[20:56:20.445744] [20:56:20.445720] [20:56:20.445933] Test:  [ 0/68]  eta: 0:01:55  loss: 0.9941 (0.9941)  acc1: 75.0000 (75.0000)  acc5: 100.0000 (100.0000)  time: 1.7049  data: 1.6879  max mem: 4666\n",
      "[20:56:21.818716] [20:56:21.818706] [20:56:21.818769] Test:  [10/68]  eta: 0:00:16  loss: 1.2334 (1.1404)  acc1: 65.6250 (63.6364)  acc5: 100.0000 (98.5795)  time: 0.2797  data: 0.2634  max mem: 4666\n",
      "[20:56:23.252122] [20:56:23.252112] [20:56:23.252174] Test:  [20/68]  eta: 0:00:10  loss: 0.7295 (0.8934)  acc1: 81.2500 (76.6369)  acc5: 100.0000 (99.1071)  time: 0.1402  data: 0.1241  max mem: 4666\n",
      "[20:56:24.649363] [20:56:24.649353] [20:56:24.649463] Test:  [30/68]  eta: 0:00:07  loss: 0.7317 (1.1029)  acc1: 81.2500 (67.3387)  acc5: 100.0000 (94.5565)  time: 0.1415  data: 0.1255  max mem: 4666\n",
      "[20:56:26.055557] [20:56:26.055547] [20:56:26.055629] Test:  [40/68]  eta: 0:00:04  loss: 1.4782 (1.2058)  acc1: 43.7500 (60.2134)  acc5: 93.7500 (94.4360)  time: 0.1401  data: 0.1241  max mem: 4666\n",
      "[20:56:27.433074] [20:56:27.433063] [20:56:27.433143] Test:  [50/68]  eta: 0:00:03  loss: 1.4451 (1.2063)  acc1: 43.7500 (59.6201)  acc5: 100.0000 (95.5270)  time: 0.1391  data: 0.1231  max mem: 4666\n",
      "[20:56:28.822365] [20:56:28.822355] [20:56:28.822542] Test:  [60/68]  eta: 0:00:01  loss: 1.2016 (1.2111)  acc1: 59.3750 (59.2725)  acc5: 100.0000 (96.2602)  time: 0.1383  data: 0.1223  max mem: 4666\n",
      "[20:56:28.937792] [20:56:28.937787] [20:56:28.937802] Test:  [67/68]  eta: 0:00:00  loss: 1.1599 (1.2096)  acc1: 59.3750 (58.3295)  acc5: 100.0000 (96.6313)  time: 0.1380  data: 0.1223  max mem: 4666\n",
      "[20:56:29.005099] [20:56:29.005089] [20:56:29.005117] Test: Total time: 0:00:10 (0.1510 s / it)\n",
      "[20:56:29.005154] [20:56:29.005152] [20:56:29.005161] * Acc@1 58.329 Acc@5 96.631 loss 1.210\n",
      "[20:56:29.005338] [20:56:29.005334] [20:56:29.005347] Accuracy of the network on the 2167 test images: 58.3%\n",
      "[20:56:29.005357] [20:56:29.005355] [20:56:29.005362] Max accuracy: 58.88%\n",
      "[20:56:30.606103] [20:56:30.606076] [20:56:30.606243] Epoch: [27]  [  0/316]  eta: 0:08:25  lr: 0.000218  loss: 1.6057 (1.6057)  time: 1.5993  data: 1.5483  max mem: 4666\n",
      "[20:56:33.046013] [20:56:33.046002] [20:56:33.046176] Epoch: [27]  [ 20/316]  eta: 0:00:56  lr: 0.000218  loss: 1.6510 (1.6439)  time: 0.1219  data: 0.0705  max mem: 4666\n",
      "[20:56:35.545073] [20:56:35.545061] [20:56:35.545130] Epoch: [27]  [ 40/316]  eta: 0:00:43  lr: 0.000218  loss: 1.6305 (1.6389)  time: 0.1249  data: 0.0735  max mem: 4666\n",
      "[20:56:37.973403] [20:56:37.973393] [20:56:37.973459] Epoch: [27]  [ 60/316]  eta: 0:00:37  lr: 0.000218  loss: 1.6643 (1.6431)  time: 0.1214  data: 0.0700  max mem: 4666\n",
      "[20:56:40.426606] [20:56:40.426594] [20:56:40.426660] Epoch: [27]  [ 80/316]  eta: 0:00:33  lr: 0.000218  loss: 1.6585 (1.6516)  time: 0.1226  data: 0.0712  max mem: 4666\n",
      "[20:56:42.907656] [20:56:42.907646] [20:56:42.907712] Epoch: [27]  [100/316]  eta: 0:00:29  lr: 0.000218  loss: 1.6399 (1.6521)  time: 0.1240  data: 0.0726  max mem: 4666\n",
      "[20:56:45.335810] [20:56:45.335800] [20:56:45.335864] Epoch: [27]  [120/316]  eta: 0:00:26  lr: 0.000217  loss: 1.6187 (1.6495)  time: 0.1214  data: 0.0700  max mem: 4666\n",
      "[20:56:47.781113] [20:56:47.781103] [20:56:47.781166] Epoch: [27]  [140/316]  eta: 0:00:23  lr: 0.000217  loss: 1.6417 (1.6501)  time: 0.1222  data: 0.0707  max mem: 4666\n",
      "[20:56:50.258502] [20:56:50.258492] [20:56:50.258556] Epoch: [27]  [160/316]  eta: 0:00:20  lr: 0.000217  loss: 1.6164 (1.6444)  time: 0.1238  data: 0.0722  max mem: 4666\n",
      "[20:56:52.773998] [20:56:52.773989] [20:56:52.774051] Epoch: [27]  [180/316]  eta: 0:00:17  lr: 0.000217  loss: 1.6394 (1.6435)  time: 0.1257  data: 0.0741  max mem: 4666\n",
      "[20:56:55.236183] [20:56:55.236173] [20:56:55.236235] Epoch: [27]  [200/316]  eta: 0:00:15  lr: 0.000217  loss: 1.6080 (1.6412)  time: 0.1231  data: 0.0714  max mem: 4666\n",
      "[20:56:57.693337] [20:56:57.693327] [20:56:57.693389] Epoch: [27]  [220/316]  eta: 0:00:12  lr: 0.000217  loss: 1.6445 (1.6425)  time: 0.1228  data: 0.0711  max mem: 4666\n",
      "[20:57:00.142226] [20:57:00.142216] [20:57:00.142280] Epoch: [27]  [240/316]  eta: 0:00:09  lr: 0.000216  loss: 1.6281 (1.6427)  time: 0.1224  data: 0.0707  max mem: 4666\n",
      "[20:57:02.644512] [20:57:02.644502] [20:57:02.644565] Epoch: [27]  [260/316]  eta: 0:00:07  lr: 0.000216  loss: 1.6578 (1.6417)  time: 0.1251  data: 0.0734  max mem: 4666\n",
      "[20:57:05.135003] [20:57:05.134992] [20:57:05.135055] Epoch: [27]  [280/316]  eta: 0:00:04  lr: 0.000216  loss: 1.6256 (1.6413)  time: 0.1245  data: 0.0730  max mem: 4666\n",
      "[20:57:07.583147] [20:57:07.583134] [20:57:07.583212] Epoch: [27]  [300/316]  eta: 0:00:02  lr: 0.000216  loss: 1.6854 (1.6435)  time: 0.1224  data: 0.0708  max mem: 4666\n",
      "[20:57:09.035793] [20:57:09.035784] [20:57:09.035841] Epoch: [27]  [315/316]  eta: 0:00:00  lr: 0.000216  loss: 1.6098 (1.6447)  time: 0.1212  data: 0.0697  max mem: 4666\n",
      "[20:57:09.108534] [20:57:09.108525] [20:57:09.108552] Epoch: [27] Total time: 0:00:40 (0.1269 s / it)\n",
      "[20:57:09.108609] [20:57:09.108607] [20:57:09.108615] Averaged stats: lr: 0.000216  loss: 1.6098 (1.6447)\n",
      "[20:57:12.504208] [20:57:12.504183] [20:57:12.504389] Test:  [ 0/68]  eta: 0:01:58  loss: 1.0960 (1.0960)  acc1: 68.7500 (68.7500)  acc5: 96.8750 (96.8750)  time: 1.7387  data: 1.7217  max mem: 4666\n",
      "[20:57:13.849524] [20:57:13.849513] [20:57:13.849577] Test:  [10/68]  eta: 0:00:16  loss: 1.2745 (1.2347)  acc1: 65.6250 (61.3636)  acc5: 100.0000 (98.5795)  time: 0.2803  data: 0.2638  max mem: 4666\n",
      "[20:57:15.228296] [20:57:15.228280] [20:57:15.228371] Test:  [20/68]  eta: 0:00:10  loss: 0.8213 (0.9897)  acc1: 75.0000 (72.9167)  acc5: 100.0000 (99.1071)  time: 0.1361  data: 0.1199  max mem: 4666\n",
      "[20:57:16.649731] [20:57:16.649720] [20:57:16.649790] Test:  [30/68]  eta: 0:00:07  loss: 0.8793 (1.1928)  acc1: 71.8750 (62.9032)  acc5: 100.0000 (94.9597)  time: 0.1399  data: 0.1239  max mem: 4666\n",
      "[20:57:18.037959] [20:57:18.037949] [20:57:18.038042] Test:  [40/68]  eta: 0:00:04  loss: 1.4915 (1.2756)  acc1: 34.3750 (55.7165)  acc5: 96.8750 (95.0457)  time: 0.1404  data: 0.1242  max mem: 4666\n",
      "[20:57:19.424551] [20:57:19.424541] [20:57:19.424611] Test:  [50/68]  eta: 0:00:03  loss: 1.3429 (1.2442)  acc1: 37.5000 (56.4338)  acc5: 100.0000 (96.0172)  time: 0.1387  data: 0.1225  max mem: 4666\n",
      "[20:57:20.790310] [20:57:20.790301] [20:57:20.790361] Test:  [60/68]  eta: 0:00:01  loss: 1.1444 (1.2189)  acc1: 65.6250 (58.2480)  acc5: 100.0000 (96.5676)  time: 0.1376  data: 0.1217  max mem: 4666\n",
      "[20:57:20.969688] [20:57:20.969678] [20:57:20.969705] Test:  [67/68]  eta: 0:00:00  loss: 0.9568 (1.1991)  acc1: 71.8750 (59.8062)  acc5: 100.0000 (96.9082)  time: 0.1380  data: 0.1223  max mem: 4666\n",
      "[20:57:21.039543] [20:57:21.039535] [20:57:21.039586] Test: Total time: 0:00:10 (0.1511 s / it)\n",
      "[20:57:21.039623] [20:57:21.039620] [20:57:21.039629] * Acc@1 59.806 Acc@5 96.908 loss 1.199\n",
      "[20:57:21.039905] [20:57:21.039896] [20:57:21.039923] Accuracy of the network on the 2167 test images: 59.8%\n",
      "[20:57:21.039947] [20:57:21.039943] [20:57:21.039960] Max accuracy: 59.81%\n",
      "[20:57:22.581315] [20:57:22.581296] [20:57:22.581481] Epoch: [28]  [  0/316]  eta: 0:08:06  lr: 0.000216  loss: 1.4780 (1.4780)  time: 1.5397  data: 1.4875  max mem: 4666\n",
      "[20:57:25.021863] [20:57:25.021853] [20:57:25.022172] Epoch: [28]  [ 20/316]  eta: 0:00:56  lr: 0.000216  loss: 1.6223 (1.6252)  time: 0.1220  data: 0.0702  max mem: 4666\n",
      "[20:57:27.531597] [20:57:27.531587] [20:57:27.531653] Epoch: [28]  [ 40/316]  eta: 0:00:43  lr: 0.000215  loss: 1.6433 (1.6330)  time: 0.1254  data: 0.0740  max mem: 4666\n",
      "[20:57:30.000246] [20:57:30.000235] [20:57:30.000303] Epoch: [28]  [ 60/316]  eta: 0:00:37  lr: 0.000215  loss: 1.6220 (1.6320)  time: 0.1234  data: 0.0719  max mem: 4666\n",
      "[20:57:32.393193] [20:57:32.393183] [20:57:32.393280] Epoch: [28]  [ 80/316]  eta: 0:00:33  lr: 0.000215  loss: 1.6793 (1.6405)  time: 0.1196  data: 0.0682  max mem: 4666\n",
      "[20:57:34.843821] [20:57:34.843811] [20:57:34.843891] Epoch: [28]  [100/316]  eta: 0:00:29  lr: 0.000215  loss: 1.6588 (1.6459)  time: 0.1225  data: 0.0711  max mem: 4666\n",
      "[20:57:37.280927] [20:57:37.280917] [20:57:37.280981] Epoch: [28]  [120/316]  eta: 0:00:26  lr: 0.000215  loss: 1.6816 (1.6539)  time: 0.1218  data: 0.0704  max mem: 4666\n",
      "[20:57:39.728295] [20:57:39.728285] [20:57:39.728416] Epoch: [28]  [140/316]  eta: 0:00:23  lr: 0.000214  loss: 1.6599 (1.6549)  time: 0.1223  data: 0.0708  max mem: 4666\n",
      "[20:57:42.180969] [20:57:42.180959] [20:57:42.181038] Epoch: [28]  [160/316]  eta: 0:00:20  lr: 0.000214  loss: 1.5807 (1.6447)  time: 0.1226  data: 0.0712  max mem: 4666\n",
      "[20:57:44.643454] [20:57:44.643444] [20:57:44.643512] Epoch: [28]  [180/316]  eta: 0:00:17  lr: 0.000214  loss: 1.6529 (1.6486)  time: 0.1231  data: 0.0717  max mem: 4666\n",
      "[20:57:47.040773] [20:57:47.040763] [20:57:47.040829] Epoch: [28]  [200/316]  eta: 0:00:14  lr: 0.000214  loss: 1.5576 (1.6426)  time: 0.1198  data: 0.0685  max mem: 4666\n",
      "[20:57:49.515148] [20:57:49.515138] [20:57:49.515201] Epoch: [28]  [220/316]  eta: 0:00:12  lr: 0.000214  loss: 1.6513 (1.6408)  time: 0.1237  data: 0.0723  max mem: 4666\n",
      "[20:57:51.924580] [20:57:51.924569] [20:57:51.924636] Epoch: [28]  [240/316]  eta: 0:00:09  lr: 0.000214  loss: 1.6406 (1.6411)  time: 0.1204  data: 0.0691  max mem: 4666\n",
      "[20:57:54.383898] [20:57:54.383889] [20:57:54.383951] Epoch: [28]  [260/316]  eta: 0:00:07  lr: 0.000213  loss: 1.5974 (1.6390)  time: 0.1229  data: 0.0716  max mem: 4666\n",
      "[20:57:56.860161] [20:57:56.860149] [20:57:56.860218] Epoch: [28]  [280/316]  eta: 0:00:04  lr: 0.000213  loss: 1.6680 (1.6389)  time: 0.1238  data: 0.0723  max mem: 4666\n",
      "[20:57:59.323441] [20:57:59.323430] [20:57:59.323499] Epoch: [28]  [300/316]  eta: 0:00:02  lr: 0.000213  loss: 1.6269 (1.6382)  time: 0.1231  data: 0.0717  max mem: 4666\n",
      "[20:58:01.077694] [20:58:01.077685] [20:58:01.077740] Epoch: [28]  [315/316]  eta: 0:00:00  lr: 0.000213  loss: 1.6068 (1.6376)  time: 0.1208  data: 0.0693  max mem: 4666\n",
      "[20:58:01.152603] [20:58:01.152594] [20:58:01.152621] Epoch: [28] Total time: 0:00:40 (0.1269 s / it)\n",
      "[20:58:01.152681] [20:58:01.152678] [20:58:01.152687] Averaged stats: lr: 0.000213  loss: 1.6068 (1.6376)\n",
      "[20:58:04.558329] [20:58:04.558307] [20:58:04.558491] Test:  [ 0/68]  eta: 0:01:57  loss: 0.9323 (0.9323)  acc1: 78.1250 (78.1250)  acc5: 100.0000 (100.0000)  time: 1.7224  data: 1.7054  max mem: 4666\n",
      "[20:58:05.929004] [20:58:05.928994] [20:58:05.929064] Test:  [10/68]  eta: 0:00:16  loss: 1.1377 (1.0346)  acc1: 68.7500 (66.7614)  acc5: 100.0000 (99.7159)  time: 0.2811  data: 0.2647  max mem: 4666\n",
      "[20:58:07.320042] [20:58:07.320032] [20:58:07.320180] Test:  [20/68]  eta: 0:00:10  loss: 0.5956 (0.7889)  acc1: 84.3750 (78.2738)  acc5: 100.0000 (99.5536)  time: 0.1380  data: 0.1219  max mem: 4666\n",
      "[20:58:08.721425] [20:58:08.721415] [20:58:08.721510] Test:  [30/68]  eta: 0:00:07  loss: 0.6407 (1.0454)  acc1: 81.2500 (66.5323)  acc5: 100.0000 (94.1532)  time: 0.1395  data: 0.1236  max mem: 4666\n",
      "[20:58:10.115636] [20:58:10.115625] [20:58:10.115707] Test:  [40/68]  eta: 0:00:04  loss: 1.5638 (1.1790)  acc1: 34.3750 (57.6220)  acc5: 96.8750 (94.3598)  time: 0.1397  data: 0.1237  max mem: 4666\n",
      "[20:58:11.501655] [20:58:11.501644] [20:58:11.501705] Test:  [50/68]  eta: 0:00:03  loss: 1.4624 (1.1783)  acc1: 34.3750 (57.5980)  acc5: 100.0000 (95.4657)  time: 0.1389  data: 0.1230  max mem: 4666\n",
      "[20:58:12.877628] [20:58:12.877612] [20:58:12.877707] Test:  [60/68]  eta: 0:00:01  loss: 1.1940 (1.1879)  acc1: 59.3750 (57.3258)  acc5: 100.0000 (96.2090)  time: 0.1380  data: 0.1221  max mem: 4666\n",
      "[20:58:12.985051] [20:58:12.985046] [20:58:12.985062] Test:  [67/68]  eta: 0:00:00  loss: 1.0617 (1.1800)  acc1: 65.6250 (58.7448)  acc5: 100.0000 (96.5851)  time: 0.1374  data: 0.1217  max mem: 4666\n",
      "[20:58:13.053735] [20:58:13.053728] [20:58:13.053749] Test: Total time: 0:00:10 (0.1503 s / it)\n",
      "[20:58:13.053784] [20:58:13.053782] [20:58:13.053791] * Acc@1 58.745 Acc@5 96.585 loss 1.180\n",
      "[20:58:13.053899] [20:58:13.053893] [20:58:13.053913] Accuracy of the network on the 2167 test images: 58.7%\n",
      "[20:58:13.053933] [20:58:13.053929] [20:58:13.053944] Max accuracy: 59.81%\n",
      "[20:58:14.704205] [20:58:14.704186] [20:58:14.704351] Epoch: [29]  [  0/316]  eta: 0:08:40  lr: 0.000213  loss: 1.6508 (1.6508)  time: 1.6485  data: 1.5983  max mem: 4666\n",
      "[20:58:17.141228] [20:58:17.141218] [20:58:17.141298] Epoch: [29]  [ 20/316]  eta: 0:00:57  lr: 0.000213  loss: 1.5894 (1.5988)  time: 0.1218  data: 0.0701  max mem: 4666\n",
      "[20:58:19.597985] [20:58:19.597974] [20:58:19.598045] Epoch: [29]  [ 40/316]  eta: 0:00:44  lr: 0.000212  loss: 1.5980 (1.6028)  time: 0.1228  data: 0.0712  max mem: 4666\n",
      "[20:58:22.012004] [20:58:22.011992] [20:58:22.012059] Epoch: [29]  [ 60/316]  eta: 0:00:37  lr: 0.000212  loss: 1.6985 (1.6218)  time: 0.1207  data: 0.0690  max mem: 4666\n",
      "[20:58:24.443540] [20:58:24.443529] [20:58:24.443594] Epoch: [29]  [ 80/316]  eta: 0:00:33  lr: 0.000212  loss: 1.5999 (1.6225)  time: 0.1215  data: 0.0698  max mem: 4666\n",
      "[20:58:26.848532] [20:58:26.848521] [20:58:26.848589] Epoch: [29]  [100/316]  eta: 0:00:29  lr: 0.000212  loss: 1.6287 (1.6288)  time: 0.1202  data: 0.0684  max mem: 4666\n",
      "[20:58:29.356307] [20:58:29.356296] [20:58:29.356496] Epoch: [29]  [120/316]  eta: 0:00:26  lr: 0.000212  loss: 1.6355 (1.6329)  time: 0.1253  data: 0.0736  max mem: 4666\n",
      "[20:58:31.816293] [20:58:31.816282] [20:58:31.816367] Epoch: [29]  [140/316]  eta: 0:00:23  lr: 0.000211  loss: 1.5927 (1.6336)  time: 0.1229  data: 0.0713  max mem: 4666\n",
      "[20:58:34.256579] [20:58:34.256568] [20:58:34.256634] Epoch: [29]  [160/316]  eta: 0:00:20  lr: 0.000211  loss: 1.5947 (1.6290)  time: 0.1220  data: 0.0703  max mem: 4666\n",
      "[20:58:36.697189] [20:58:36.697178] [20:58:36.697277] Epoch: [29]  [180/316]  eta: 0:00:17  lr: 0.000211  loss: 1.6699 (1.6297)  time: 0.1220  data: 0.0704  max mem: 4666\n",
      "[20:58:39.167197] [20:58:39.167187] [20:58:39.167253] Epoch: [29]  [200/316]  eta: 0:00:15  lr: 0.000211  loss: 1.5995 (1.6289)  time: 0.1235  data: 0.0718  max mem: 4666\n",
      "[20:58:41.611629] [20:58:41.611619] [20:58:41.611682] Epoch: [29]  [220/316]  eta: 0:00:12  lr: 0.000211  loss: 1.5983 (1.6274)  time: 0.1222  data: 0.0707  max mem: 4666\n",
      "[20:58:44.069625] [20:58:44.069614] [20:58:44.069694] Epoch: [29]  [240/316]  eta: 0:00:09  lr: 0.000211  loss: 1.6408 (1.6273)  time: 0.1229  data: 0.0714  max mem: 4666\n",
      "[20:58:46.578722] [20:58:46.578712] [20:58:46.578777] Epoch: [29]  [260/316]  eta: 0:00:07  lr: 0.000210  loss: 1.5824 (1.6296)  time: 0.1254  data: 0.0740  max mem: 4666\n",
      "[20:58:49.047360] [20:58:49.047349] [20:58:49.047413] Epoch: [29]  [280/316]  eta: 0:00:04  lr: 0.000210  loss: 1.6569 (1.6307)  time: 0.1234  data: 0.0719  max mem: 4666\n",
      "[20:58:51.527859] [20:58:51.527849] [20:58:51.527974] Epoch: [29]  [300/316]  eta: 0:00:02  lr: 0.000210  loss: 1.6312 (1.6322)  time: 0.1240  data: 0.0726  max mem: 4666\n",
      "[20:58:53.309487] [20:58:53.309478] [20:58:53.309534] Epoch: [29]  [315/316]  eta: 0:00:00  lr: 0.000210  loss: 1.6656 (1.6335)  time: 0.1255  data: 0.0742  max mem: 4666\n",
      "[20:58:53.386863] [20:58:53.386857] [20:58:53.386874] Epoch: [29] Total time: 0:00:40 (0.1276 s / it)\n",
      "[20:58:53.386909] [20:58:53.386906] [20:58:53.386915] Averaged stats: lr: 0.000210  loss: 1.6656 (1.6335)\n",
      "[20:58:56.749534] [20:58:56.749510] [20:58:56.749729] Test:  [ 0/68]  eta: 0:01:53  loss: 0.9884 (0.9884)  acc1: 71.8750 (71.8750)  acc5: 100.0000 (100.0000)  time: 1.6711  data: 1.6541  max mem: 4666\n",
      "[20:58:58.127948] [20:58:58.127938] [20:58:58.128004] Test:  [10/68]  eta: 0:00:16  loss: 1.2399 (1.1679)  acc1: 65.6250 (60.7955)  acc5: 100.0000 (97.4432)  time: 0.2771  data: 0.2607  max mem: 4666\n",
      "[20:58:59.519941] [20:58:59.519930] [20:58:59.519991] Test:  [20/68]  eta: 0:00:10  loss: 0.7340 (0.9165)  acc1: 78.1250 (72.7679)  acc5: 100.0000 (98.5119)  time: 0.1384  data: 0.1223  max mem: 4666\n",
      "[20:59:00.913609] [20:59:00.913598] [20:59:00.913660] Test:  [30/68]  eta: 0:00:07  loss: 0.7441 (1.1317)  acc1: 71.8750 (63.4073)  acc5: 100.0000 (95.3629)  time: 0.1392  data: 0.1232  max mem: 4666\n",
      "[20:59:02.301560] [20:59:02.301549] [20:59:02.301609] Test:  [40/68]  eta: 0:00:04  loss: 1.4165 (1.2274)  acc1: 37.5000 (56.8598)  acc5: 93.7500 (95.1220)  time: 0.1390  data: 0.1229  max mem: 4666\n",
      "[20:59:03.685136] [20:59:03.685126] [20:59:03.685186] Test:  [50/68]  eta: 0:00:03  loss: 1.2354 (1.1740)  acc1: 43.7500 (59.0686)  acc5: 100.0000 (96.0784)  time: 0.1385  data: 0.1225  max mem: 4666\n",
      "[20:59:05.070003] [20:59:05.069993] [20:59:05.070050] Test:  [60/68]  eta: 0:00:01  loss: 1.0779 (1.1790)  acc1: 56.2500 (58.9652)  acc5: 100.0000 (96.7213)  time: 0.1384  data: 0.1224  max mem: 4666\n",
      "[20:59:05.208504] [20:59:05.208494] [20:59:05.208521] Test:  [67/68]  eta: 0:00:00  loss: 1.0628 (1.1753)  acc1: 62.5000 (59.7600)  acc5: 100.0000 (97.0466)  time: 0.1378  data: 0.1221  max mem: 4666\n",
      "[20:59:05.277809] [20:59:05.277801] [20:59:05.277838] Test: Total time: 0:00:10 (0.1500 s / it)\n",
      "[20:59:05.277875] [20:59:05.277873] [20:59:05.277881] * Acc@1 59.760 Acc@5 97.047 loss 1.175\n",
      "[20:59:05.278193] [20:59:05.278189] [20:59:05.278201] Accuracy of the network on the 2167 test images: 59.8%\n",
      "[20:59:05.278210] [20:59:05.278209] [20:59:05.278217] Max accuracy: 59.81%\n",
      "[20:59:06.812015] [20:59:06.811997] [20:59:06.812172] Epoch: [30]  [  0/316]  eta: 0:08:04  lr: 0.000210  loss: 1.6357 (1.6357)  time: 1.5324  data: 1.4821  max mem: 4666\n",
      "[20:59:09.264437] [20:59:09.264425] [20:59:09.264531] Epoch: [30]  [ 20/316]  eta: 0:00:56  lr: 0.000210  loss: 1.6050 (1.6153)  time: 0.1226  data: 0.0710  max mem: 4666\n",
      "[20:59:11.695760] [20:59:11.695751] [20:59:11.695812] Epoch: [30]  [ 40/316]  eta: 0:00:43  lr: 0.000209  loss: 1.5862 (1.5949)  time: 0.1215  data: 0.0700  max mem: 4666\n",
      "[20:59:14.110467] [20:59:14.110456] [20:59:14.110521] Epoch: [30]  [ 60/316]  eta: 0:00:37  lr: 0.000209  loss: 1.6689 (1.6185)  time: 0.1207  data: 0.0691  max mem: 4666\n",
      "[20:59:16.536421] [20:59:16.536411] [20:59:16.536477] Epoch: [30]  [ 80/316]  eta: 0:00:32  lr: 0.000209  loss: 1.6366 (1.6217)  time: 0.1213  data: 0.0698  max mem: 4666\n",
      "[20:59:19.050823] [20:59:19.050812] [20:59:19.050880] Epoch: [30]  [100/316]  eta: 0:00:29  lr: 0.000209  loss: 1.6535 (1.6233)  time: 0.1257  data: 0.0743  max mem: 4666\n",
      "[20:59:21.553041] [20:59:21.553031] [20:59:21.553093] Epoch: [30]  [120/316]  eta: 0:00:26  lr: 0.000209  loss: 1.6407 (1.6231)  time: 0.1251  data: 0.0736  max mem: 4666\n",
      "[20:59:24.015565] [20:59:24.015555] [20:59:24.015619] Epoch: [30]  [140/316]  eta: 0:00:23  lr: 0.000208  loss: 1.6503 (1.6249)  time: 0.1231  data: 0.0716  max mem: 4666\n",
      "[20:59:26.493635] [20:59:26.493625] [20:59:26.493689] Epoch: [30]  [160/316]  eta: 0:00:20  lr: 0.000208  loss: 1.6034 (1.6238)  time: 0.1239  data: 0.0724  max mem: 4666\n",
      "[20:59:28.988634] [20:59:28.988622] [20:59:28.988702] Epoch: [30]  [180/316]  eta: 0:00:17  lr: 0.000208  loss: 1.6931 (1.6261)  time: 0.1247  data: 0.0732  max mem: 4666\n",
      "[20:59:31.487544] [20:59:31.487534] [20:59:31.487595] Epoch: [30]  [200/316]  eta: 0:00:15  lr: 0.000208  loss: 1.6351 (1.6274)  time: 0.1249  data: 0.0731  max mem: 4666\n",
      "[20:59:34.001324] [20:59:34.001313] [20:59:34.001387] Epoch: [30]  [220/316]  eta: 0:00:12  lr: 0.000208  loss: 1.6403 (1.6292)  time: 0.1256  data: 0.0739  max mem: 4666\n",
      "[20:59:36.518233] [20:59:36.518223] [20:59:36.518293] Epoch: [30]  [240/316]  eta: 0:00:09  lr: 0.000207  loss: 1.6411 (1.6296)  time: 0.1258  data: 0.0740  max mem: 4666\n",
      "[20:59:38.962792] [20:59:38.962782] [20:59:38.962849] Epoch: [30]  [260/316]  eta: 0:00:07  lr: 0.000207  loss: 1.5797 (1.6280)  time: 0.1222  data: 0.0706  max mem: 4666\n",
      "[20:59:41.439776] [20:59:41.439766] [20:59:41.439830] Epoch: [30]  [280/316]  eta: 0:00:04  lr: 0.000207  loss: 1.5988 (1.6270)  time: 0.1238  data: 0.0723  max mem: 4666\n",
      "[20:59:43.898378] [20:59:43.898368] [20:59:43.898436] Epoch: [30]  [300/316]  eta: 0:00:02  lr: 0.000207  loss: 1.6074 (1.6256)  time: 0.1229  data: 0.0715  max mem: 4666\n",
      "[20:59:45.379148] [20:59:45.379138] [20:59:45.379193] Epoch: [30]  [315/316]  eta: 0:00:00  lr: 0.000207  loss: 1.6167 (1.6260)  time: 0.1216  data: 0.0703  max mem: 4666\n",
      "[20:59:45.452348] [20:59:45.452342] [20:59:45.452361] Epoch: [30] Total time: 0:00:40 (0.1271 s / it)\n",
      "[20:59:45.452405] [20:59:45.452402] [20:59:45.452411] Averaged stats: lr: 0.000207  loss: 1.6167 (1.6260)\n",
      "[20:59:48.861916] [20:59:48.861890] [20:59:48.862112] Test:  [ 0/68]  eta: 0:01:54  loss: 0.8859 (0.8859)  acc1: 78.1250 (78.1250)  acc5: 100.0000 (100.0000)  time: 1.6865  data: 1.6693  max mem: 4666\n",
      "[20:59:50.253023] [20:59:50.253013] [20:59:50.253100] Test:  [10/68]  eta: 0:00:16  loss: 1.0799 (1.0310)  acc1: 68.7500 (65.6250)  acc5: 100.0000 (99.4318)  time: 0.2797  data: 0.2631  max mem: 4666\n",
      "[20:59:51.641223] [20:59:51.641213] [20:59:51.641279] Test:  [20/68]  eta: 0:00:10  loss: 0.7704 (0.8468)  acc1: 78.1250 (75.7440)  acc5: 100.0000 (99.4048)  time: 0.1389  data: 0.1227  max mem: 4666\n",
      "[20:59:53.044761] [20:59:53.044751] [20:59:53.044813] Test:  [30/68]  eta: 0:00:07  loss: 0.8493 (1.0612)  acc1: 78.1250 (66.5323)  acc5: 100.0000 (95.6653)  time: 0.1395  data: 0.1235  max mem: 4666\n",
      "[20:59:54.443154] [20:59:54.443144] [20:59:54.443205] Test:  [40/68]  eta: 0:00:04  loss: 1.4477 (1.1554)  acc1: 40.6250 (60.2134)  acc5: 96.8750 (95.5793)  time: 0.1400  data: 0.1240  max mem: 4666\n",
      "[20:59:55.828195] [20:59:55.828185] [20:59:55.828249] Test:  [50/68]  eta: 0:00:03  loss: 1.3266 (1.1443)  acc1: 37.5000 (60.5392)  acc5: 100.0000 (96.4461)  time: 0.1391  data: 0.1232  max mem: 4666\n",
      "[20:59:57.229028] [20:59:57.229018] [20:59:57.229079] Test:  [60/68]  eta: 0:00:01  loss: 1.0807 (1.1460)  acc1: 65.6250 (60.6557)  acc5: 100.0000 (96.9775)  time: 0.1392  data: 0.1233  max mem: 4666\n",
      "[20:59:57.377348] [20:59:57.377338] [20:59:57.377365] Test:  [67/68]  eta: 0:00:00  loss: 1.0900 (1.1477)  acc1: 65.6250 (60.5907)  acc5: 100.0000 (97.2773)  time: 0.1384  data: 0.1227  max mem: 4666\n",
      "[20:59:57.441575] [20:59:57.441567] [20:59:57.441608] Test: Total time: 0:00:10 (0.1510 s / it)\n",
      "[20:59:57.441656] [20:59:57.441653] [20:59:57.441663] * Acc@1 60.591 Acc@5 97.277 loss 1.148\n",
      "[20:59:57.441850] [20:59:57.441846] [20:59:57.441859] Accuracy of the network on the 2167 test images: 60.6%\n",
      "[20:59:57.441872] [20:59:57.441870] [20:59:57.441879] Max accuracy: 60.59%\n",
      "[20:59:59.030305] [20:59:59.030287] [20:59:59.030442] Epoch: [31]  [  0/316]  eta: 0:08:21  lr: 0.000207  loss: 1.5799 (1.5799)  time: 1.5870  data: 1.5364  max mem: 4666\n",
      "[21:00:01.503303] [21:00:01.503293] [21:00:01.503354] Epoch: [31]  [ 20/316]  eta: 0:00:57  lr: 0.000207  loss: 1.6040 (1.5948)  time: 0.1236  data: 0.0722  max mem: 4666\n",
      "[21:00:03.933279] [21:00:03.933269] [21:00:03.933580] Epoch: [31]  [ 40/316]  eta: 0:00:43  lr: 0.000206  loss: 1.6449 (1.6053)  time: 0.1214  data: 0.0701  max mem: 4666\n",
      "[21:00:06.403621] [21:00:06.403611] [21:00:06.403679] Epoch: [31]  [ 60/316]  eta: 0:00:37  lr: 0.000206  loss: 1.6363 (1.6062)  time: 0.1235  data: 0.0720  max mem: 4666\n",
      "[21:00:08.901639] [21:00:08.901628] [21:00:08.901692] Epoch: [31]  [ 80/316]  eta: 0:00:33  lr: 0.000206  loss: 1.5933 (1.6126)  time: 0.1248  data: 0.0734  max mem: 4666\n",
      "[21:00:11.424116] [21:00:11.424106] [21:00:11.424170] Epoch: [31]  [100/316]  eta: 0:00:29  lr: 0.000206  loss: 1.6465 (1.6213)  time: 0.1261  data: 0.0747  max mem: 4666\n",
      "[21:00:13.894836] [21:00:13.894826] [21:00:13.894888] Epoch: [31]  [120/316]  eta: 0:00:26  lr: 0.000206  loss: 1.6250 (1.6238)  time: 0.1235  data: 0.0722  max mem: 4666\n",
      "[21:00:16.361393] [21:00:16.361382] [21:00:16.361445] Epoch: [31]  [140/316]  eta: 0:00:23  lr: 0.000205  loss: 1.6550 (1.6298)  time: 0.1233  data: 0.0719  max mem: 4666\n",
      "[21:00:18.844982] [21:00:18.844971] [21:00:18.845032] Epoch: [31]  [160/316]  eta: 0:00:20  lr: 0.000205  loss: 1.5875 (1.6268)  time: 0.1241  data: 0.0728  max mem: 4666\n",
      "[21:00:21.366637] [21:00:21.366627] [21:00:21.366695] Epoch: [31]  [180/316]  eta: 0:00:17  lr: 0.000205  loss: 1.6320 (1.6269)  time: 0.1260  data: 0.0747  max mem: 4666\n",
      "[21:00:23.812250] [21:00:23.812240] [21:00:23.812308] Epoch: [31]  [200/316]  eta: 0:00:15  lr: 0.000205  loss: 1.5321 (1.6214)  time: 0.1222  data: 0.0709  max mem: 4666\n",
      "[21:00:26.258324] [21:00:26.258314] [21:00:26.258380] Epoch: [31]  [220/316]  eta: 0:00:12  lr: 0.000205  loss: 1.5672 (1.6224)  time: 0.1223  data: 0.0709  max mem: 4666\n",
      "[21:00:28.723399] [21:00:28.723386] [21:00:28.723457] Epoch: [31]  [240/316]  eta: 0:00:09  lr: 0.000204  loss: 1.6555 (1.6252)  time: 0.1232  data: 0.0719  max mem: 4666\n",
      "[21:00:31.198359] [21:00:31.198349] [21:00:31.198416] Epoch: [31]  [260/316]  eta: 0:00:07  lr: 0.000204  loss: 1.6038 (1.6230)  time: 0.1237  data: 0.0724  max mem: 4666\n",
      "[21:00:33.699038] [21:00:33.699028] [21:00:33.699092] Epoch: [31]  [280/316]  eta: 0:00:04  lr: 0.000204  loss: 1.6547 (1.6254)  time: 0.1250  data: 0.0737  max mem: 4666\n",
      "[21:00:36.136395] [21:00:36.136385] [21:00:36.136452] Epoch: [31]  [300/316]  eta: 0:00:02  lr: 0.000204  loss: 1.6297 (1.6271)  time: 0.1218  data: 0.0705  max mem: 4666\n",
      "[21:00:37.572504] [21:00:37.572494] [21:00:37.572548] Epoch: [31]  [315/316]  eta: 0:00:00  lr: 0.000204  loss: 1.6377 (1.6271)  time: 0.1202  data: 0.0691  max mem: 4666\n",
      "[21:00:37.650424] [21:00:37.650415] [21:00:37.650441] Epoch: [31] Total time: 0:00:40 (0.1272 s / it)\n",
      "[21:00:37.650495] [21:00:37.650492] [21:00:37.650505] Averaged stats: lr: 0.000204  loss: 1.6377 (1.6271)\n",
      "[21:00:41.057783] [21:00:41.057760] [21:00:41.057938] Test:  [ 0/68]  eta: 0:01:56  loss: 0.9611 (0.9611)  acc1: 71.8750 (71.8750)  acc5: 100.0000 (100.0000)  time: 1.7126  data: 1.6957  max mem: 4666\n",
      "[21:00:42.431960] [21:00:42.431948] [21:00:42.432040] Test:  [10/68]  eta: 0:00:16  loss: 1.2215 (1.1195)  acc1: 65.6250 (64.2045)  acc5: 100.0000 (97.7273)  time: 0.2805  data: 0.2641  max mem: 4666\n",
      "[21:00:43.844813] [21:00:43.844803] [21:00:43.845049] Test:  [20/68]  eta: 0:00:10  loss: 0.6925 (0.8612)  acc1: 84.3750 (76.9345)  acc5: 100.0000 (98.8095)  time: 0.1393  data: 0.1231  max mem: 4666\n",
      "[21:00:45.305279] [21:00:45.305269] [21:00:45.305342] Test:  [30/68]  eta: 0:00:07  loss: 0.7394 (1.0934)  acc1: 81.2500 (65.8266)  acc5: 100.0000 (94.1532)  time: 0.1436  data: 0.1276  max mem: 4666\n",
      "[21:00:46.700912] [21:00:46.700900] [21:00:46.700968] Test:  [40/68]  eta: 0:00:05  loss: 1.5358 (1.2147)  acc1: 31.2500 (57.5457)  acc5: 93.7500 (94.1311)  time: 0.1427  data: 0.1268  max mem: 4666\n",
      "[21:00:48.106337] [21:00:48.106326] [21:00:48.106482] Test:  [50/68]  eta: 0:00:03  loss: 1.4114 (1.1963)  acc1: 37.5000 (58.2108)  acc5: 100.0000 (95.2819)  time: 0.1400  data: 0.1241  max mem: 4666\n",
      "[21:00:49.505754] [21:00:49.505745] [21:00:49.505802] Test:  [60/68]  eta: 0:00:01  loss: 1.1411 (1.1919)  acc1: 62.5000 (58.9652)  acc5: 100.0000 (95.9016)  time: 0.1402  data: 0.1243  max mem: 4666\n",
      "[21:00:49.612566] [21:00:49.612560] [21:00:49.612575] Test:  [67/68]  eta: 0:00:00  loss: 1.1003 (1.1891)  acc1: 65.6250 (59.6677)  acc5: 100.0000 (96.2621)  time: 0.1399  data: 0.1243  max mem: 4666\n",
      "[21:00:49.677592] [21:00:49.677587] [21:00:49.677602] Test: Total time: 0:00:10 (0.1520 s / it)\n",
      "[21:00:49.677633] [21:00:49.677631] [21:00:49.677640] * Acc@1 59.668 Acc@5 96.262 loss 1.189\n",
      "[21:00:49.677702] [21:00:49.677699] [21:00:49.677708] Accuracy of the network on the 2167 test images: 59.7%\n",
      "[21:00:49.677717] [21:00:49.677715] [21:00:49.677721] Max accuracy: 60.59%\n",
      "[21:00:51.213763] [21:00:51.213727] [21:00:51.213954] Epoch: [32]  [  0/316]  eta: 0:08:04  lr: 0.000204  loss: 1.5244 (1.5244)  time: 1.5346  data: 1.4837  max mem: 4666\n",
      "[21:00:53.655004] [21:00:53.654994] [21:00:53.655058] Epoch: [32]  [ 20/316]  eta: 0:00:56  lr: 0.000203  loss: 1.5976 (1.6076)  time: 0.1220  data: 0.0706  max mem: 4666\n",
      "[21:00:56.130628] [21:00:56.130617] [21:00:56.130743] Epoch: [32]  [ 40/316]  eta: 0:00:43  lr: 0.000203  loss: 1.6174 (1.6140)  time: 0.1237  data: 0.0723  max mem: 4666\n",
      "[21:00:58.579195] [21:00:58.579184] [21:00:58.579423] Epoch: [32]  [ 60/316]  eta: 0:00:37  lr: 0.000203  loss: 1.6094 (1.6173)  time: 0.1224  data: 0.0710  max mem: 4666\n",
      "[21:01:01.061961] [21:01:01.061950] [21:01:01.062017] Epoch: [32]  [ 80/316]  eta: 0:00:33  lr: 0.000203  loss: 1.6616 (1.6288)  time: 0.1241  data: 0.0727  max mem: 4666\n",
      "[21:01:03.535844] [21:01:03.535832] [21:01:03.535899] Epoch: [32]  [100/316]  eta: 0:00:29  lr: 0.000203  loss: 1.6247 (1.6320)  time: 0.1237  data: 0.0723  max mem: 4666\n",
      "[21:01:06.007821] [21:01:06.007810] [21:01:06.007878] Epoch: [32]  [120/316]  eta: 0:00:26  lr: 0.000202  loss: 1.5732 (1.6258)  time: 0.1236  data: 0.0722  max mem: 4666\n",
      "[21:01:08.497135] [21:01:08.497123] [21:01:08.497196] Epoch: [32]  [140/316]  eta: 0:00:23  lr: 0.000202  loss: 1.6573 (1.6353)  time: 0.1244  data: 0.0730  max mem: 4666\n",
      "[21:01:10.957166] [21:01:10.957156] [21:01:10.957221] Epoch: [32]  [160/316]  eta: 0:00:20  lr: 0.000202  loss: 1.6273 (1.6337)  time: 0.1230  data: 0.0716  max mem: 4666\n",
      "[21:01:13.421596] [21:01:13.421583] [21:01:13.421661] Epoch: [32]  [180/316]  eta: 0:00:17  lr: 0.000202  loss: 1.6508 (1.6380)  time: 0.1232  data: 0.0718  max mem: 4666\n",
      "[21:01:15.870195] [21:01:15.870184] [21:01:15.870318] Epoch: [32]  [200/316]  eta: 0:00:15  lr: 0.000202  loss: 1.6144 (1.6336)  time: 0.1224  data: 0.0710  max mem: 4666\n",
      "[21:01:18.359991] [21:01:18.359979] [21:01:18.360047] Epoch: [32]  [220/316]  eta: 0:00:12  lr: 0.000201  loss: 1.5775 (1.6301)  time: 0.1244  data: 0.0731  max mem: 4666\n",
      "[21:01:20.816913] [21:01:20.816903] [21:01:20.816969] Epoch: [32]  [240/316]  eta: 0:00:09  lr: 0.000201  loss: 1.6728 (1.6306)  time: 0.1228  data: 0.0714  max mem: 4666\n",
      "[21:01:23.271274] [21:01:23.271263] [21:01:23.271329] Epoch: [32]  [260/316]  eta: 0:00:07  lr: 0.000201  loss: 1.5763 (1.6277)  time: 0.1227  data: 0.0713  max mem: 4666\n",
      "[21:01:25.704581] [21:01:25.704570] [21:01:25.704637] Epoch: [32]  [280/316]  eta: 0:00:04  lr: 0.000201  loss: 1.6197 (1.6271)  time: 0.1216  data: 0.0703  max mem: 4666\n",
      "[21:01:28.118018] [21:01:28.118007] [21:01:28.118079] Epoch: [32]  [300/316]  eta: 0:00:02  lr: 0.000201  loss: 1.5556 (1.6255)  time: 0.1206  data: 0.0693  max mem: 4666\n",
      "[21:01:29.847948] [21:01:29.847939] [21:01:29.847994] Epoch: [32]  [315/316]  eta: 0:00:00  lr: 0.000200  loss: 1.4920 (1.6218)  time: 0.1227  data: 0.0715  max mem: 4666\n",
      "[21:01:29.922624] [21:01:29.922617] [21:01:29.922639] Epoch: [32] Total time: 0:00:40 (0.1274 s / it)\n",
      "[21:01:29.922807] [21:01:29.922803] [21:01:29.922815] Averaged stats: lr: 0.000200  loss: 1.4920 (1.6218)\n",
      "[21:01:33.345465] [21:01:33.345434] [21:01:33.345654] Test:  [ 0/68]  eta: 0:01:55  loss: 1.0222 (1.0222)  acc1: 65.6250 (65.6250)  acc5: 96.8750 (96.8750)  time: 1.6916  data: 1.6744  max mem: 4666\n",
      "[21:01:34.729194] [21:01:34.729184] [21:01:34.729301] Test:  [10/68]  eta: 0:00:16  loss: 1.2549 (1.2174)  acc1: 65.6250 (58.2386)  acc5: 96.8750 (96.3068)  time: 0.2795  data: 0.2630  max mem: 4666\n",
      "[21:01:36.116695] [21:01:36.116684] [21:01:36.116749] Test:  [20/68]  eta: 0:00:10  loss: 0.7222 (0.9316)  acc1: 75.0000 (72.0238)  acc5: 100.0000 (97.7679)  time: 0.1385  data: 0.1223  max mem: 4666\n",
      "[21:01:37.523016] [21:01:37.523006] [21:01:37.523065] Test:  [30/68]  eta: 0:00:07  loss: 0.8582 (1.0844)  acc1: 78.1250 (67.0363)  acc5: 100.0000 (95.6653)  time: 0.1396  data: 0.1236  max mem: 4666\n",
      "[21:01:38.920088] [21:01:38.920078] [21:01:38.920212] Test:  [40/68]  eta: 0:00:04  loss: 1.3121 (1.1695)  acc1: 43.7500 (61.8140)  acc5: 93.7500 (95.1220)  time: 0.1401  data: 0.1241  max mem: 4666\n",
      "[21:01:40.297025] [21:01:40.297016] [21:01:40.297074] Test:  [50/68]  eta: 0:00:03  loss: 1.2814 (1.1347)  acc1: 43.7500 (62.8676)  acc5: 100.0000 (96.0784)  time: 0.1386  data: 0.1227  max mem: 4666\n",
      "[21:01:41.690855] [21:01:41.690845] [21:01:41.690958] Test:  [60/68]  eta: 0:00:01  loss: 1.0079 (1.1248)  acc1: 68.7500 (63.3709)  acc5: 100.0000 (96.6189)  time: 0.1385  data: 0.1225  max mem: 4666\n",
      "[21:01:41.813726] [21:01:41.813721] [21:01:41.813737] Test:  [67/68]  eta: 0:00:00  loss: 1.0091 (1.1250)  acc1: 65.6250 (62.0674)  acc5: 100.0000 (96.9082)  time: 0.1378  data: 0.1221  max mem: 4666\n",
      "[21:01:41.878788] [21:01:41.878779] [21:01:41.878804] Test: Total time: 0:00:10 (0.1504 s / it)\n",
      "[21:01:41.878841] [21:01:41.878839] [21:01:41.878848] * Acc@1 62.067 Acc@5 96.908 loss 1.125\n",
      "[21:01:41.878949] [21:01:41.878946] [21:01:41.878955] Accuracy of the network on the 2167 test images: 62.1%\n",
      "[21:01:41.878966] [21:01:41.878964] [21:01:41.878971] Max accuracy: 62.07%\n",
      "[21:01:43.460370] [21:01:43.460347] [21:01:43.460539] Epoch: [33]  [  0/316]  eta: 0:08:19  lr: 0.000200  loss: 1.4564 (1.4564)  time: 1.5800  data: 1.5293  max mem: 4666\n",
      "[21:01:45.893543] [21:01:45.893532] [21:01:45.893594] Epoch: [33]  [ 20/316]  eta: 0:00:56  lr: 0.000200  loss: 1.5538 (1.5496)  time: 0.1216  data: 0.0698  max mem: 4666\n",
      "[21:01:48.321449] [21:01:48.321438] [21:01:48.321507] Epoch: [33]  [ 40/316]  eta: 0:00:43  lr: 0.000200  loss: 1.5846 (1.5743)  time: 0.1214  data: 0.0697  max mem: 4666\n",
      "[21:01:50.760763] [21:01:50.760753] [21:01:50.760819] Epoch: [33]  [ 60/316]  eta: 0:00:37  lr: 0.000200  loss: 1.5744 (1.5855)  time: 0.1219  data: 0.0705  max mem: 4666\n",
      "[21:01:53.221242] [21:01:53.221231] [21:01:53.221303] Epoch: [33]  [ 80/316]  eta: 0:00:33  lr: 0.000199  loss: 1.6729 (1.6009)  time: 0.1230  data: 0.0716  max mem: 4666\n",
      "[21:01:55.654614] [21:01:55.654602] [21:01:55.654700] Epoch: [33]  [100/316]  eta: 0:00:29  lr: 0.000199  loss: 1.6407 (1.6054)  time: 0.1216  data: 0.0703  max mem: 4666\n",
      "[21:01:58.117846] [21:01:58.117835] [21:01:58.117903] Epoch: [33]  [120/316]  eta: 0:00:26  lr: 0.000199  loss: 1.5819 (1.6059)  time: 0.1231  data: 0.0718  max mem: 4666\n",
      "[21:02:00.547751] [21:02:00.547741] [21:02:00.547806] Epoch: [33]  [140/316]  eta: 0:00:23  lr: 0.000199  loss: 1.6283 (1.6059)  time: 0.1215  data: 0.0701  max mem: 4666\n",
      "[21:02:03.016696] [21:02:03.016686] [21:02:03.016750] Epoch: [33]  [160/316]  eta: 0:00:20  lr: 0.000199  loss: 1.5974 (1.6047)  time: 0.1234  data: 0.0721  max mem: 4666\n",
      "[21:02:05.401697] [21:02:05.401686] [21:02:05.401753] Epoch: [33]  [180/316]  eta: 0:00:17  lr: 0.000198  loss: 1.6228 (1.6038)  time: 0.1192  data: 0.0678  max mem: 4666\n",
      "[21:02:07.841123] [21:02:07.841112] [21:02:07.841177] Epoch: [33]  [200/316]  eta: 0:00:14  lr: 0.000198  loss: 1.5791 (1.6022)  time: 0.1219  data: 0.0705  max mem: 4666\n",
      "[21:02:10.327634] [21:02:10.327623] [21:02:10.327694] Epoch: [33]  [220/316]  eta: 0:00:12  lr: 0.000198  loss: 1.5971 (1.6032)  time: 0.1243  data: 0.0730  max mem: 4666\n",
      "[21:02:12.782330] [21:02:12.782319] [21:02:12.782381] Epoch: [33]  [240/316]  eta: 0:00:09  lr: 0.000198  loss: 1.6201 (1.6046)  time: 0.1227  data: 0.0713  max mem: 4666\n",
      "[21:02:15.191031] [21:02:15.191020] [21:02:15.191088] Epoch: [33]  [260/316]  eta: 0:00:07  lr: 0.000198  loss: 1.5891 (1.6047)  time: 0.1204  data: 0.0690  max mem: 4666\n",
      "[21:02:17.652901] [21:02:17.652891] [21:02:17.652953] Epoch: [33]  [280/316]  eta: 0:00:04  lr: 0.000197  loss: 1.6142 (1.6057)  time: 0.1231  data: 0.0717  max mem: 4666\n",
      "[21:02:20.070736] [21:02:20.070725] [21:02:20.070787] Epoch: [33]  [300/316]  eta: 0:00:02  lr: 0.000197  loss: 1.5817 (1.6067)  time: 0.1209  data: 0.0695  max mem: 4666\n",
      "[21:02:22.073909] [21:02:22.073899] [21:02:22.073951] Epoch: [33]  [315/316]  eta: 0:00:00  lr: 0.000197  loss: 1.5591 (1.6071)  time: 0.1190  data: 0.0678  max mem: 4666\n",
      "[21:02:22.153526] [21:02:22.153518] [21:02:22.153540] Epoch: [33] Total time: 0:00:40 (0.1274 s / it)\n",
      "[21:02:22.153589] [21:02:22.153587] [21:02:22.153595] Averaged stats: lr: 0.000197  loss: 1.5591 (1.6071)\n",
      "[21:02:26.026233] [21:02:26.026205] [21:02:26.026406] Test:  [ 0/68]  eta: 0:01:57  loss: 0.8793 (0.8793)  acc1: 78.1250 (78.1250)  acc5: 100.0000 (100.0000)  time: 1.7292  data: 1.7122  max mem: 4666\n",
      "[21:02:27.367485] [21:02:27.367473] [21:02:27.367552] Test:  [10/68]  eta: 0:00:16  loss: 1.0898 (1.0220)  acc1: 71.8750 (66.7614)  acc5: 100.0000 (99.4318)  time: 0.2790  data: 0.2626  max mem: 4666\n",
      "[21:02:28.768049] [21:02:28.768038] [21:02:28.768132] Test:  [20/68]  eta: 0:00:10  loss: 0.7047 (0.8125)  acc1: 78.1250 (77.6786)  acc5: 100.0000 (99.5536)  time: 0.1370  data: 0.1208  max mem: 4666\n",
      "[21:02:30.160933] [21:02:30.160923] [21:02:30.161053] Test:  [30/68]  eta: 0:00:07  loss: 0.7918 (1.0310)  acc1: 78.1250 (69.2540)  acc5: 100.0000 (96.0685)  time: 0.1396  data: 0.1236  max mem: 4666\n",
      "[21:02:31.536012] [21:02:31.536001] [21:02:31.536155] Test:  [40/68]  eta: 0:00:04  loss: 1.4219 (1.1271)  acc1: 46.8750 (62.8811)  acc5: 93.7500 (95.5793)  time: 0.1383  data: 0.1223  max mem: 4666\n",
      "[21:02:32.913904] [21:02:32.913895] [21:02:32.913954] Test:  [50/68]  eta: 0:00:03  loss: 1.2835 (1.1062)  acc1: 46.8750 (63.6642)  acc5: 100.0000 (96.4461)  time: 0.1376  data: 0.1216  max mem: 4666\n",
      "[21:02:34.294074] [21:02:34.294064] [21:02:34.294128] Test:  [60/68]  eta: 0:00:01  loss: 1.0397 (1.1209)  acc1: 65.6250 (63.4221)  acc5: 100.0000 (96.9262)  time: 0.1378  data: 0.1219  max mem: 4666\n",
      "[21:02:34.412852] [21:02:34.412844] [21:02:34.412866] Test:  [67/68]  eta: 0:00:00  loss: 1.0275 (1.1273)  acc1: 59.3750 (62.4827)  acc5: 100.0000 (97.1850)  time: 0.1373  data: 0.1216  max mem: 4666\n",
      "[21:02:34.480813] [21:02:34.480799] [21:02:34.480841] Test: Total time: 0:00:10 (0.1498 s / it)\n",
      "[21:02:34.480905] [21:02:34.480900] [21:02:34.480918] * Acc@1 62.483 Acc@5 97.185 loss 1.127\n",
      "[21:02:34.481074] [21:02:34.481068] [21:02:34.481090] Accuracy of the network on the 2167 test images: 62.5%\n",
      "[21:02:34.481112] [21:02:34.481108] [21:02:34.481123] Max accuracy: 62.48%\n",
      "[21:02:36.113784] [21:02:36.113764] [21:02:36.113923] Epoch: [34]  [  0/316]  eta: 0:08:35  lr: 0.000197  loss: 1.6874 (1.6874)  time: 1.6311  data: 1.5804  max mem: 4666\n",
      "[21:02:38.517842] [21:02:38.517831] [21:02:38.517928] Epoch: [34]  [ 20/316]  eta: 0:00:56  lr: 0.000197  loss: 1.5808 (1.5887)  time: 0.1201  data: 0.0685  max mem: 4666\n",
      "[21:02:40.988227] [21:02:40.988217] [21:02:40.988283] Epoch: [34]  [ 40/316]  eta: 0:00:43  lr: 0.000197  loss: 1.6211 (1.5890)  time: 0.1235  data: 0.0718  max mem: 4666\n",
      "[21:02:43.421106] [21:02:43.421094] [21:02:43.421189] Epoch: [34]  [ 60/316]  eta: 0:00:37  lr: 0.000196  loss: 1.5954 (1.5928)  time: 0.1216  data: 0.0699  max mem: 4666\n",
      "[21:02:45.854387] [21:02:45.854377] [21:02:45.854517] Epoch: [34]  [ 80/316]  eta: 0:00:33  lr: 0.000196  loss: 1.6358 (1.5942)  time: 0.1216  data: 0.0700  max mem: 4666\n",
      "[21:02:48.297205] [21:02:48.297194] [21:02:48.297261] Epoch: [34]  [100/316]  eta: 0:00:29  lr: 0.000196  loss: 1.6428 (1.6060)  time: 0.1221  data: 0.0704  max mem: 4666\n",
      "[21:02:50.723316] [21:02:50.723306] [21:02:50.723372] Epoch: [34]  [120/316]  eta: 0:00:26  lr: 0.000196  loss: 1.5923 (1.6074)  time: 0.1213  data: 0.0696  max mem: 4666\n",
      "[21:02:53.193149] [21:02:53.193138] [21:02:53.193209] Epoch: [34]  [140/316]  eta: 0:00:23  lr: 0.000196  loss: 1.6834 (1.6142)  time: 0.1234  data: 0.0718  max mem: 4666\n",
      "[21:02:55.625389] [21:02:55.625379] [21:02:55.625444] Epoch: [34]  [160/316]  eta: 0:00:20  lr: 0.000195  loss: 1.5146 (1.6076)  time: 0.1216  data: 0.0701  max mem: 4666\n",
      "[21:02:58.062202] [21:02:58.062191] [21:02:58.062261] Epoch: [34]  [180/316]  eta: 0:00:17  lr: 0.000195  loss: 1.5954 (1.6073)  time: 0.1218  data: 0.0704  max mem: 4666\n",
      "[21:03:00.455066] [21:03:00.455054] [21:03:00.455124] Epoch: [34]  [200/316]  eta: 0:00:14  lr: 0.000195  loss: 1.5817 (1.6054)  time: 0.1196  data: 0.0682  max mem: 4666\n",
      "[21:03:02.911086] [21:03:02.911076] [21:03:02.911141] Epoch: [34]  [220/316]  eta: 0:00:12  lr: 0.000195  loss: 1.6764 (1.6090)  time: 0.1228  data: 0.0714  max mem: 4666\n",
      "[21:03:05.378728] [21:03:05.378718] [21:03:05.378781] Epoch: [34]  [240/316]  eta: 0:00:09  lr: 0.000194  loss: 1.6425 (1.6116)  time: 0.1233  data: 0.0719  max mem: 4666\n",
      "[21:03:07.828639] [21:03:07.828629] [21:03:07.828707] Epoch: [34]  [260/316]  eta: 0:00:07  lr: 0.000194  loss: 1.6474 (1.6134)  time: 0.1224  data: 0.0710  max mem: 4666\n",
      "[21:03:10.268880] [21:03:10.268870] [21:03:10.268932] Epoch: [34]  [280/316]  eta: 0:00:04  lr: 0.000194  loss: 1.6525 (1.6179)  time: 0.1220  data: 0.0706  max mem: 4666\n",
      "[21:03:12.724511] [21:03:12.724501] [21:03:12.724562] Epoch: [34]  [300/316]  eta: 0:00:02  lr: 0.000194  loss: 1.6321 (1.6176)  time: 0.1227  data: 0.0714  max mem: 4666\n",
      "[21:03:14.438717] [21:03:14.438708] [21:03:14.438764] Epoch: [34]  [315/316]  eta: 0:00:00  lr: 0.000194  loss: 1.6140 (1.6172)  time: 0.1210  data: 0.0698  max mem: 4666\n",
      "[21:03:14.506160] [21:03:14.506147] [21:03:14.506180] Epoch: [34] Total time: 0:00:40 (0.1267 s / it)\n",
      "[21:03:14.506237] [21:03:14.506234] [21:03:14.506244] Averaged stats: lr: 0.000194  loss: 1.6140 (1.6172)\n",
      "[21:03:17.834777] [21:03:17.834749] [21:03:17.834990] Test:  [ 0/68]  eta: 0:01:56  loss: 1.0536 (1.0536)  acc1: 65.6250 (65.6250)  acc5: 100.0000 (100.0000)  time: 1.7117  data: 1.6945  max mem: 4666\n",
      "[21:03:19.240974] [21:03:19.240962] [21:03:19.241028] Test:  [10/68]  eta: 0:00:16  loss: 1.3624 (1.1977)  acc1: 62.5000 (61.0795)  acc5: 100.0000 (98.2955)  time: 0.2834  data: 0.2669  max mem: 4666\n",
      "[21:03:20.613327] [21:03:20.613309] [21:03:20.614140] Test:  [20/68]  eta: 0:00:10  loss: 0.5635 (0.8525)  acc1: 84.3750 (75.5952)  acc5: 100.0000 (98.9583)  time: 0.1384  data: 0.1222  max mem: 4666\n",
      "[21:03:22.055257] [21:03:22.055244] [21:03:22.055382] Test:  [30/68]  eta: 0:00:07  loss: 0.6375 (1.0631)  acc1: 81.2500 (67.4395)  acc5: 100.0000 (94.9597)  time: 0.1401  data: 0.1231  max mem: 4666\n",
      "[21:03:23.443966] [21:03:23.443953] [21:03:23.444036] Test:  [40/68]  eta: 0:00:04  loss: 1.5146 (1.1812)  acc1: 37.5000 (59.8323)  acc5: 93.7500 (94.5884)  time: 0.1414  data: 0.1241  max mem: 4666\n",
      "[21:03:24.827505] [21:03:24.827494] [21:03:24.827572] Test:  [50/68]  eta: 0:00:03  loss: 1.3630 (1.1453)  acc1: 43.7500 (61.7647)  acc5: 100.0000 (95.6495)  time: 0.1385  data: 0.1223  max mem: 4666\n",
      "[21:03:26.210584] [21:03:26.210573] [21:03:26.210634] Test:  [60/68]  eta: 0:00:01  loss: 1.0450 (1.1599)  acc1: 56.2500 (60.8607)  acc5: 100.0000 (96.2602)  time: 0.1383  data: 0.1223  max mem: 4666\n",
      "[21:03:26.318324] [21:03:26.318317] [21:03:26.318341] Test:  [67/68]  eta: 0:00:00  loss: 1.0824 (1.1602)  acc1: 62.5000 (60.7291)  acc5: 100.0000 (96.6313)  time: 0.1376  data: 0.1219  max mem: 4666\n",
      "[21:03:26.388027] [21:03:26.388019] [21:03:26.388042] Test: Total time: 0:00:10 (0.1510 s / it)\n",
      "[21:03:26.388078] [21:03:26.388076] [21:03:26.388085] * Acc@1 60.729 Acc@5 96.631 loss 1.160\n",
      "[21:03:26.388176] [21:03:26.388173] [21:03:26.388182] Accuracy of the network on the 2167 test images: 60.7%\n",
      "[21:03:26.388191] [21:03:26.388190] [21:03:26.388196] Max accuracy: 62.48%\n",
      "[21:03:27.977020] [21:03:27.976993] [21:03:27.977218] Epoch: [35]  [  0/316]  eta: 0:08:21  lr: 0.000194  loss: 1.6393 (1.6393)  time: 1.5874  data: 1.5357  max mem: 4666\n",
      "[21:03:30.416222] [21:03:30.416211] [21:03:30.416280] Epoch: [35]  [ 20/316]  eta: 0:00:56  lr: 0.000193  loss: 1.5926 (1.6065)  time: 0.1219  data: 0.0701  max mem: 4666\n",
      "[21:03:32.860101] [21:03:32.860091] [21:03:32.860157] Epoch: [35]  [ 40/316]  eta: 0:00:43  lr: 0.000193  loss: 1.6009 (1.6150)  time: 0.1221  data: 0.0704  max mem: 4666\n",
      "[21:03:35.333392] [21:03:35.333380] [21:03:35.333459] Epoch: [35]  [ 60/316]  eta: 0:00:37  lr: 0.000193  loss: 1.6536 (1.6112)  time: 0.1236  data: 0.0719  max mem: 4666\n",
      "[21:03:37.766255] [21:03:37.766244] [21:03:37.766311] Epoch: [35]  [ 80/316]  eta: 0:00:33  lr: 0.000193  loss: 1.6689 (1.6198)  time: 0.1216  data: 0.0700  max mem: 4666\n",
      "[21:03:40.274486] [21:03:40.274475] [21:03:40.274540] Epoch: [35]  [100/316]  eta: 0:00:29  lr: 0.000193  loss: 1.6159 (1.6243)  time: 0.1254  data: 0.0737  max mem: 4666\n",
      "[21:03:42.710256] [21:03:42.710243] [21:03:42.710313] Epoch: [35]  [120/316]  eta: 0:00:26  lr: 0.000192  loss: 1.6049 (1.6230)  time: 0.1217  data: 0.0701  max mem: 4666\n",
      "[21:03:45.128836] [21:03:45.128825] [21:03:45.128902] Epoch: [35]  [140/316]  eta: 0:00:23  lr: 0.000192  loss: 1.6386 (1.6232)  time: 0.1209  data: 0.0693  max mem: 4666\n",
      "[21:03:47.586509] [21:03:47.586497] [21:03:47.586567] Epoch: [35]  [160/316]  eta: 0:00:20  lr: 0.000192  loss: 1.5733 (1.6203)  time: 0.1228  data: 0.0712  max mem: 4666\n",
      "[21:03:50.047925] [21:03:50.047914] [21:03:50.047978] Epoch: [35]  [180/316]  eta: 0:00:17  lr: 0.000192  loss: 1.5819 (1.6140)  time: 0.1230  data: 0.0714  max mem: 4666\n",
      "[21:03:52.514587] [21:03:52.514576] [21:03:52.514644] Epoch: [35]  [200/316]  eta: 0:00:15  lr: 0.000191  loss: 1.5842 (1.6114)  time: 0.1233  data: 0.0717  max mem: 4666\n",
      "[21:03:54.969201] [21:03:54.969191] [21:03:54.969253] Epoch: [35]  [220/316]  eta: 0:00:12  lr: 0.000191  loss: 1.5471 (1.6064)  time: 0.1227  data: 0.0711  max mem: 4666\n",
      "[21:03:57.366261] [21:03:57.366250] [21:03:57.366316] Epoch: [35]  [240/316]  eta: 0:00:09  lr: 0.000191  loss: 1.6363 (1.6097)  time: 0.1198  data: 0.0682  max mem: 4666\n",
      "[21:03:59.808441] [21:03:59.808430] [21:03:59.808503] Epoch: [35]  [260/316]  eta: 0:00:07  lr: 0.000191  loss: 1.5769 (1.6098)  time: 0.1221  data: 0.0704  max mem: 4666\n",
      "[21:04:02.223394] [21:04:02.223383] [21:04:02.223463] Epoch: [35]  [280/316]  eta: 0:00:04  lr: 0.000191  loss: 1.6121 (1.6116)  time: 0.1207  data: 0.0691  max mem: 4666\n",
      "[21:04:04.699583] [21:04:04.699572] [21:04:04.699723] Epoch: [35]  [300/316]  eta: 0:00:02  lr: 0.000190  loss: 1.7014 (1.6138)  time: 0.1238  data: 0.0722  max mem: 4666\n",
      "[21:04:06.563174] [21:04:06.563164] [21:04:06.563221] Epoch: [35]  [315/316]  eta: 0:00:00  lr: 0.000190  loss: 1.5724 (1.6141)  time: 0.1204  data: 0.0691  max mem: 4666\n",
      "[21:04:06.639188] [21:04:06.639179] [21:04:06.639205] Epoch: [35] Total time: 0:00:40 (0.1274 s / it)\n",
      "[21:04:06.639379] [21:04:06.639375] [21:04:06.639387] Averaged stats: lr: 0.000190  loss: 1.5724 (1.6141)\n",
      "[21:04:10.596260] [21:04:10.596237] [21:04:10.596423] Test:  [ 0/68]  eta: 0:01:57  loss: 1.0333 (1.0333)  acc1: 71.8750 (71.8750)  acc5: 100.0000 (100.0000)  time: 1.7307  data: 1.7138  max mem: 4666\n",
      "[21:04:11.931190] [21:04:11.931181] [21:04:11.931291] Test:  [10/68]  eta: 0:00:16  loss: 1.2773 (1.1848)  acc1: 68.7500 (63.0682)  acc5: 100.0000 (98.5795)  time: 0.2786  data: 0.2621  max mem: 4666\n",
      "[21:04:13.337776] [21:04:13.337767] [21:04:13.337832] Test:  [20/68]  eta: 0:00:10  loss: 0.7513 (0.9174)  acc1: 84.3750 (75.2976)  acc5: 100.0000 (99.1071)  time: 0.1370  data: 0.1208  max mem: 4666\n",
      "[21:04:14.735648] [21:04:14.735638] [21:04:14.735778] Test:  [30/68]  eta: 0:00:07  loss: 0.7650 (1.0917)  acc1: 75.0000 (67.3387)  acc5: 100.0000 (96.8750)  time: 0.1401  data: 0.1242  max mem: 4666\n",
      "[21:04:16.128475] [21:04:16.128465] [21:04:16.128547] Test:  [40/68]  eta: 0:00:04  loss: 1.3426 (1.1708)  acc1: 43.7500 (61.4329)  acc5: 93.7500 (96.1128)  time: 0.1395  data: 0.1235  max mem: 4666\n",
      "[21:04:17.512794] [21:04:17.512784] [21:04:17.512903] Test:  [50/68]  eta: 0:00:03  loss: 1.1654 (1.1197)  acc1: 53.1250 (64.2157)  acc5: 100.0000 (96.8750)  time: 0.1388  data: 0.1228  max mem: 4666\n",
      "[21:04:18.963395] [21:04:18.963385] [21:04:18.963442] Test:  [60/68]  eta: 0:00:01  loss: 1.0816 (1.1556)  acc1: 62.5000 (62.0902)  acc5: 100.0000 (97.1824)  time: 0.1417  data: 0.1257  max mem: 4666\n",
      "[21:04:19.070807] [21:04:19.070798] [21:04:19.070821] Test:  [67/68]  eta: 0:00:00  loss: 1.2009 (1.1693)  acc1: 56.2500 (61.1444)  acc5: 100.0000 (97.4619)  time: 0.1395  data: 0.1239  max mem: 4666\n",
      "[21:04:19.148359] [21:04:19.148350] [21:04:19.148378] Test: Total time: 0:00:10 (0.1512 s / it)\n",
      "[21:04:19.148416] [21:04:19.148414] [21:04:19.148423] * Acc@1 61.144 Acc@5 97.462 loss 1.169\n",
      "[21:04:19.148526] [21:04:19.148523] [21:04:19.148532] Accuracy of the network on the 2167 test images: 61.1%\n",
      "[21:04:19.148543] [21:04:19.148541] [21:04:19.148548] Max accuracy: 62.48%\n",
      "[21:04:20.787715] [21:04:20.787697] [21:04:20.787857] Epoch: [36]  [  0/316]  eta: 0:08:37  lr: 0.000190  loss: 1.6375 (1.6375)  time: 1.6378  data: 1.5874  max mem: 4666\n",
      "[21:04:23.261117] [21:04:23.261106] [21:04:23.261176] Epoch: [36]  [ 20/316]  eta: 0:00:57  lr: 0.000190  loss: 1.5673 (1.5898)  time: 0.1236  data: 0.0721  max mem: 4666\n",
      "[21:04:25.719823] [21:04:25.719813] [21:04:25.719873] Epoch: [36]  [ 40/316]  eta: 0:00:44  lr: 0.000190  loss: 1.6054 (1.6043)  time: 0.1229  data: 0.0714  max mem: 4666\n",
      "[21:04:28.208647] [21:04:28.208636] [21:04:28.208702] Epoch: [36]  [ 60/316]  eta: 0:00:38  lr: 0.000189  loss: 1.6051 (1.6049)  time: 0.1244  data: 0.0728  max mem: 4666\n",
      "[21:04:30.651214] [21:04:30.651205] [21:04:30.651271] Epoch: [36]  [ 80/316]  eta: 0:00:33  lr: 0.000189  loss: 1.5997 (1.6160)  time: 0.1221  data: 0.0704  max mem: 4666\n",
      "[21:04:33.113853] [21:04:33.113843] [21:04:33.113903] Epoch: [36]  [100/316]  eta: 0:00:29  lr: 0.000189  loss: 1.6199 (1.6202)  time: 0.1231  data: 0.0714  max mem: 4666\n",
      "[21:04:35.548306] [21:04:35.548295] [21:04:35.548449] Epoch: [36]  [120/316]  eta: 0:00:26  lr: 0.000189  loss: 1.6294 (1.6266)  time: 0.1217  data: 0.0700  max mem: 4666\n",
      "[21:04:38.012613] [21:04:38.012600] [21:04:38.012688] Epoch: [36]  [140/316]  eta: 0:00:23  lr: 0.000189  loss: 1.6260 (1.6336)  time: 0.1231  data: 0.0715  max mem: 4666\n",
      "[21:04:40.525894] [21:04:40.525884] [21:04:40.525945] Epoch: [36]  [160/316]  eta: 0:00:20  lr: 0.000188  loss: 1.6335 (1.6305)  time: 0.1256  data: 0.0739  max mem: 4666\n",
      "[21:04:42.989727] [21:04:42.989716] [21:04:42.989782] Epoch: [36]  [180/316]  eta: 0:00:17  lr: 0.000188  loss: 1.6530 (1.6326)  time: 0.1231  data: 0.0715  max mem: 4666\n",
      "[21:04:45.443868] [21:04:45.443858] [21:04:45.443927] Epoch: [36]  [200/316]  eta: 0:00:15  lr: 0.000188  loss: 1.5496 (1.6262)  time: 0.1227  data: 0.0710  max mem: 4666\n",
      "[21:04:47.899631] [21:04:47.899622] [21:04:47.899683] Epoch: [36]  [220/316]  eta: 0:00:12  lr: 0.000188  loss: 1.5558 (1.6203)  time: 0.1227  data: 0.0710  max mem: 4666\n",
      "[21:04:50.385105] [21:04:50.385095] [21:04:50.385156] Epoch: [36]  [240/316]  eta: 0:00:09  lr: 0.000187  loss: 1.5762 (1.6190)  time: 0.1242  data: 0.0726  max mem: 4666\n",
      "[21:04:52.803862] [21:04:52.803851] [21:04:52.803914] Epoch: [36]  [260/316]  eta: 0:00:07  lr: 0.000187  loss: 1.5771 (1.6130)  time: 0.1209  data: 0.0692  max mem: 4666\n",
      "[21:04:55.263677] [21:04:55.263667] [21:04:55.263737] Epoch: [36]  [280/316]  eta: 0:00:04  lr: 0.000187  loss: 1.6425 (1.6139)  time: 0.1229  data: 0.0713  max mem: 4666\n",
      "[21:04:57.726966] [21:04:57.726956] [21:04:57.727022] Epoch: [36]  [300/316]  eta: 0:00:02  lr: 0.000187  loss: 1.6484 (1.6156)  time: 0.1231  data: 0.0714  max mem: 4666\n",
      "[21:04:59.198684] [21:04:59.198674] [21:04:59.198732] Epoch: [36]  [315/316]  eta: 0:00:00  lr: 0.000187  loss: 1.5589 (1.6160)  time: 0.1225  data: 0.0712  max mem: 4666\n",
      "[21:04:59.271027] [21:04:59.271016] [21:04:59.271050] Epoch: [36] Total time: 0:00:40 (0.1270 s / it)\n",
      "[21:04:59.271106] [21:04:59.271101] [21:04:59.271121] Averaged stats: lr: 0.000187  loss: 1.5589 (1.6160)\n",
      "[21:05:02.725036] [21:05:02.725012] [21:05:02.725207] Test:  [ 0/68]  eta: 0:01:58  loss: 0.9476 (0.9476)  acc1: 75.0000 (75.0000)  acc5: 100.0000 (100.0000)  time: 1.7475  data: 1.7306  max mem: 4666\n",
      "[21:05:04.107568] [21:05:04.107557] [21:05:04.107639] Test:  [10/68]  eta: 0:00:16  loss: 1.2067 (1.1101)  acc1: 65.6250 (63.6364)  acc5: 100.0000 (99.7159)  time: 0.2845  data: 0.2680  max mem: 4666\n",
      "[21:05:05.524494] [21:05:05.524482] [21:05:05.524552] Test:  [20/68]  eta: 0:00:10  loss: 0.6547 (0.8433)  acc1: 84.3750 (76.7857)  acc5: 100.0000 (99.7024)  time: 0.1399  data: 0.1237  max mem: 4666\n",
      "[21:05:06.903955] [21:05:06.903943] [21:05:06.904005] Test:  [30/68]  eta: 0:00:07  loss: 0.7387 (1.0619)  acc1: 78.1250 (66.7339)  acc5: 100.0000 (95.8669)  time: 0.1398  data: 0.1237  max mem: 4666\n",
      "[21:05:08.288662] [21:05:08.288651] [21:05:08.288752] Test:  [40/68]  eta: 0:00:04  loss: 1.4616 (1.1555)  acc1: 43.7500 (60.2134)  acc5: 96.8750 (95.8841)  time: 0.1381  data: 0.1221  max mem: 4666\n",
      "[21:05:09.670246] [21:05:09.670234] [21:05:09.670301] Test:  [50/68]  eta: 0:00:03  loss: 1.2276 (1.1261)  acc1: 46.8750 (61.9485)  acc5: 100.0000 (96.6912)  time: 0.1382  data: 0.1223  max mem: 4666\n",
      "[21:05:11.071861] [21:05:11.071849] [21:05:11.071920] Test:  [60/68]  eta: 0:00:01  loss: 1.0520 (1.1396)  acc1: 62.5000 (61.3730)  acc5: 100.0000 (97.2336)  time: 0.1391  data: 0.1231  max mem: 4666\n",
      "[21:05:11.207565] [21:05:11.207559] [21:05:11.207578] Test:  [67/68]  eta: 0:00:00  loss: 1.0997 (1.1475)  acc1: 62.5000 (61.6059)  acc5: 100.0000 (97.4619)  time: 0.1386  data: 0.1229  max mem: 4666\n",
      "[21:05:11.274841] [21:05:11.274837] [21:05:11.274863] Test: Total time: 0:00:10 (0.1514 s / it)\n",
      "[21:05:11.274895] [21:05:11.274893] [21:05:11.274901] * Acc@1 61.606 Acc@5 97.462 loss 1.148\n",
      "[21:05:11.275029] [21:05:11.275025] [21:05:11.275036] Accuracy of the network on the 2167 test images: 61.6%\n",
      "[21:05:11.275045] [21:05:11.275043] [21:05:11.275050] Max accuracy: 62.48%\n",
      "[21:05:12.894448] [21:05:12.894430] [21:05:12.894577] Epoch: [37]  [  0/316]  eta: 0:08:31  lr: 0.000187  loss: 1.5894 (1.5894)  time: 1.6182  data: 1.5665  max mem: 4666\n",
      "[21:05:15.290840] [21:05:15.290829] [21:05:15.290896] Epoch: [37]  [ 20/316]  eta: 0:00:56  lr: 0.000186  loss: 1.5926 (1.5958)  time: 0.1198  data: 0.0682  max mem: 4666\n",
      "[21:05:17.776093] [21:05:17.776082] [21:05:17.776150] Epoch: [37]  [ 40/316]  eta: 0:00:43  lr: 0.000186  loss: 1.5276 (1.5712)  time: 0.1242  data: 0.0728  max mem: 4666\n",
      "[21:05:20.201252] [21:05:20.201242] [21:05:20.201308] Epoch: [37]  [ 60/316]  eta: 0:00:37  lr: 0.000186  loss: 1.5756 (1.5776)  time: 0.1212  data: 0.0698  max mem: 4666\n",
      "[21:05:22.653161] [21:05:22.653148] [21:05:22.653232] Epoch: [37]  [ 80/316]  eta: 0:00:33  lr: 0.000186  loss: 1.6094 (1.5900)  time: 0.1225  data: 0.0711  max mem: 4666\n",
      "[21:05:25.071804] [21:05:25.071792] [21:05:25.071939] Epoch: [37]  [100/316]  eta: 0:00:29  lr: 0.000185  loss: 1.6459 (1.5983)  time: 0.1209  data: 0.0695  max mem: 4666\n",
      "[21:05:27.559481] [21:05:27.559471] [21:05:27.559533] Epoch: [37]  [120/316]  eta: 0:00:26  lr: 0.000185  loss: 1.5455 (1.5986)  time: 0.1243  data: 0.0729  max mem: 4666\n",
      "[21:05:30.027295] [21:05:30.027283] [21:05:30.027351] Epoch: [37]  [140/316]  eta: 0:00:23  lr: 0.000185  loss: 1.6198 (1.6012)  time: 0.1233  data: 0.0719  max mem: 4666\n",
      "[21:05:32.471917] [21:05:32.471906] [21:05:32.471971] Epoch: [37]  [160/316]  eta: 0:00:20  lr: 0.000185  loss: 1.6060 (1.6009)  time: 0.1222  data: 0.0708  max mem: 4666\n",
      "[21:05:34.941452] [21:05:34.941441] [21:05:34.941504] Epoch: [37]  [180/316]  eta: 0:00:17  lr: 0.000185  loss: 1.5562 (1.5977)  time: 0.1234  data: 0.0721  max mem: 4666\n",
      "[21:05:37.378432] [21:05:37.378421] [21:05:37.378485] Epoch: [37]  [200/316]  eta: 0:00:15  lr: 0.000184  loss: 1.5676 (1.5930)  time: 0.1218  data: 0.0703  max mem: 4666\n",
      "[21:05:39.826143] [21:05:39.826134] [21:05:39.826201] Epoch: [37]  [220/316]  eta: 0:00:12  lr: 0.000184  loss: 1.6261 (1.5959)  time: 0.1223  data: 0.0710  max mem: 4666\n",
      "[21:05:42.299587] [21:05:42.299577] [21:05:42.299635] Epoch: [37]  [240/316]  eta: 0:00:09  lr: 0.000184  loss: 1.5733 (1.5966)  time: 0.1236  data: 0.0722  max mem: 4666\n",
      "[21:05:44.755342] [21:05:44.755330] [21:05:44.755399] Epoch: [37]  [260/316]  eta: 0:00:07  lr: 0.000184  loss: 1.5700 (1.5962)  time: 0.1227  data: 0.0713  max mem: 4666\n",
      "[21:05:47.205775] [21:05:47.205763] [21:05:47.205832] Epoch: [37]  [280/316]  eta: 0:00:04  lr: 0.000183  loss: 1.6092 (1.5971)  time: 0.1225  data: 0.0711  max mem: 4666\n",
      "[21:05:49.664459] [21:05:49.664449] [21:05:49.664510] Epoch: [37]  [300/316]  eta: 0:00:02  lr: 0.000183  loss: 1.5982 (1.5967)  time: 0.1229  data: 0.0715  max mem: 4666\n",
      "[21:05:51.234302] [21:05:51.234292] [21:05:51.234347] Epoch: [37]  [315/316]  eta: 0:00:00  lr: 0.000183  loss: 1.6230 (1.5987)  time: 0.1187  data: 0.0674  max mem: 4666\n",
      "[21:05:51.307762] [21:05:51.307752] [21:05:51.307779] Epoch: [37] Total time: 0:00:40 (0.1267 s / it)\n",
      "[21:05:51.307834] [21:05:51.307831] [21:05:51.307840] Averaged stats: lr: 0.000183  loss: 1.6230 (1.5987)\n",
      "[21:05:54.714209] [21:05:54.714182] [21:05:54.714399] Test:  [ 0/68]  eta: 0:01:54  loss: 1.0549 (1.0549)  acc1: 71.8750 (71.8750)  acc5: 100.0000 (100.0000)  time: 1.6901  data: 1.6730  max mem: 4666\n",
      "[21:05:56.086419] [21:05:56.086408] [21:05:56.086680] Test:  [10/68]  eta: 0:00:16  loss: 1.2986 (1.2068)  acc1: 65.6250 (61.0795)  acc5: 100.0000 (98.8636)  time: 0.2783  data: 0.2617  max mem: 4666\n",
      "[21:05:57.514276] [21:05:57.514264] [21:05:57.514345] Test:  [20/68]  eta: 0:00:10  loss: 0.7994 (0.9384)  acc1: 78.1250 (73.8095)  acc5: 100.0000 (99.2560)  time: 0.1399  data: 0.1236  max mem: 4666\n",
      "[21:05:58.903026] [21:05:58.903014] [21:05:58.903120] Test:  [30/68]  eta: 0:00:07  loss: 0.8583 (1.1224)  acc1: 68.7500 (65.8266)  acc5: 100.0000 (96.7742)  time: 0.1407  data: 0.1247  max mem: 4666\n",
      "[21:06:00.290899] [21:06:00.290888] [21:06:00.290972] Test:  [40/68]  eta: 0:00:04  loss: 1.3377 (1.1889)  acc1: 46.8750 (60.7470)  acc5: 93.7500 (96.0366)  time: 0.1388  data: 0.1228  max mem: 4666\n",
      "[21:06:01.667532] [21:06:01.667520] [21:06:01.667602] Test:  [50/68]  eta: 0:00:03  loss: 1.0823 (1.1159)  acc1: 50.0000 (63.7255)  acc5: 100.0000 (96.8137)  time: 0.1382  data: 0.1222  max mem: 4666\n",
      "[21:06:03.041072] [21:06:03.041061] [21:06:03.041155] Test:  [60/68]  eta: 0:00:01  loss: 0.9207 (1.1169)  acc1: 65.6250 (63.6270)  acc5: 100.0000 (97.3361)  time: 0.1374  data: 0.1215  max mem: 4666\n",
      "[21:06:03.179182] [21:06:03.179172] [21:06:03.179200] Test:  [67/68]  eta: 0:00:00  loss: 1.0093 (1.1238)  acc1: 65.6250 (62.5750)  acc5: 100.0000 (97.6004)  time: 0.1369  data: 0.1212  max mem: 4666\n",
      "[21:06:03.250330] [21:06:03.250322] [21:06:03.250358] Test: Total time: 0:00:10 (0.1504 s / it)\n",
      "[21:06:03.250395] [21:06:03.250392] [21:06:03.250401] * Acc@1 62.575 Acc@5 97.600 loss 1.124\n",
      "[21:06:03.250566] [21:06:03.250563] [21:06:03.250574] Accuracy of the network on the 2167 test images: 62.6%\n",
      "[21:06:03.250584] [21:06:03.250582] [21:06:03.250590] Max accuracy: 62.57%\n",
      "[21:06:04.794943] [21:06:04.794921] [21:06:04.795111] Epoch: [38]  [  0/316]  eta: 0:08:07  lr: 0.000183  loss: 1.6042 (1.6042)  time: 1.5430  data: 1.4903  max mem: 4666\n",
      "[21:06:07.236338] [21:06:07.236327] [21:06:07.236401] Epoch: [38]  [ 20/316]  eta: 0:00:56  lr: 0.000183  loss: 1.5580 (1.5848)  time: 0.1220  data: 0.0703  max mem: 4666\n",
      "[21:06:09.732920] [21:06:09.732909] [21:06:09.732976] Epoch: [38]  [ 40/316]  eta: 0:00:43  lr: 0.000182  loss: 1.5872 (1.5958)  time: 0.1248  data: 0.0733  max mem: 4666\n",
      "[21:06:12.204525] [21:06:12.204515] [21:06:12.204581] Epoch: [38]  [ 60/316]  eta: 0:00:37  lr: 0.000182  loss: 1.6115 (1.5940)  time: 0.1235  data: 0.0701  max mem: 4666\n",
      "[21:06:14.616759] [21:06:14.616749] [21:06:14.616818] Epoch: [38]  [ 80/316]  eta: 0:00:33  lr: 0.000182  loss: 1.6517 (1.6062)  time: 0.1206  data: 0.0671  max mem: 4666\n",
      "[21:06:17.031694] [21:06:17.031684] [21:06:17.031747] Epoch: [38]  [100/316]  eta: 0:00:29  lr: 0.000182  loss: 1.5761 (1.6001)  time: 0.1207  data: 0.0693  max mem: 4666\n",
      "[21:06:19.467728] [21:06:19.467717] [21:06:19.467784] Epoch: [38]  [120/316]  eta: 0:00:26  lr: 0.000182  loss: 1.5790 (1.5949)  time: 0.1218  data: 0.0700  max mem: 4666\n",
      "[21:06:21.913753] [21:06:21.913743] [21:06:21.913809] Epoch: [38]  [140/316]  eta: 0:00:23  lr: 0.000181  loss: 1.5961 (1.5976)  time: 0.1223  data: 0.0709  max mem: 4666\n",
      "[21:06:24.334258] [21:06:24.334246] [21:06:24.334317] Epoch: [38]  [160/316]  eta: 0:00:20  lr: 0.000181  loss: 1.5893 (1.5995)  time: 0.1210  data: 0.0696  max mem: 4666\n",
      "[21:06:26.780078] [21:06:26.780066] [21:06:26.780142] Epoch: [38]  [180/316]  eta: 0:00:17  lr: 0.000181  loss: 1.5285 (1.5948)  time: 0.1222  data: 0.0709  max mem: 4666\n",
      "[21:06:29.197044] [21:06:29.197034] [21:06:29.197099] Epoch: [38]  [200/316]  eta: 0:00:14  lr: 0.000181  loss: 1.5191 (1.5896)  time: 0.1208  data: 0.0694  max mem: 4666\n",
      "[21:06:31.603973] [21:06:31.603961] [21:06:31.604030] Epoch: [38]  [220/316]  eta: 0:00:12  lr: 0.000180  loss: 1.5348 (1.5881)  time: 0.1203  data: 0.0689  max mem: 4666\n",
      "[21:06:34.036504] [21:06:34.036494] [21:06:34.036558] Epoch: [38]  [240/316]  eta: 0:00:09  lr: 0.000180  loss: 1.6075 (1.5905)  time: 0.1216  data: 0.0699  max mem: 4666\n",
      "[21:06:36.455537] [21:06:36.455526] [21:06:36.455594] Epoch: [38]  [260/316]  eta: 0:00:07  lr: 0.000180  loss: 1.5835 (1.5904)  time: 0.1209  data: 0.0662  max mem: 4666\n",
      "[21:06:38.892381] [21:06:38.892369] [21:06:38.892443] Epoch: [38]  [280/316]  eta: 0:00:04  lr: 0.000180  loss: 1.6028 (1.5912)  time: 0.1218  data: 0.0704  max mem: 4666\n",
      "[21:06:41.344149] [21:06:41.344138] [21:06:41.344207] Epoch: [38]  [300/316]  eta: 0:00:02  lr: 0.000179  loss: 1.5906 (1.5936)  time: 0.1225  data: 0.0712  max mem: 4666\n",
      "[21:06:43.528958] [21:06:43.528949] [21:06:43.529009] Epoch: [38]  [315/316]  eta: 0:00:00  lr: 0.000179  loss: 1.5892 (1.5956)  time: 0.1246  data: 0.0733  max mem: 4666\n",
      "[21:06:43.595002] [21:06:43.594993] [21:06:43.595019] Epoch: [38] Total time: 0:00:40 (0.1277 s / it)\n",
      "[21:06:43.595074] [21:06:43.595071] [21:06:43.595080] Averaged stats: lr: 0.000179  loss: 1.5892 (1.5956)\n",
      "[21:06:47.108989] [21:06:47.108965] [21:06:47.109167] Test:  [ 0/68]  eta: 0:01:59  loss: 0.9686 (0.9686)  acc1: 75.0000 (75.0000)  acc5: 100.0000 (100.0000)  time: 1.7608  data: 1.7437  max mem: 4666\n",
      "[21:06:48.512391] [21:06:48.512380] [21:06:48.512447] Test:  [10/68]  eta: 0:00:16  loss: 1.1921 (1.1217)  acc1: 68.7500 (64.4886)  acc5: 100.0000 (96.8750)  time: 0.2876  data: 0.2711  max mem: 4666\n",
      "[21:06:49.889789] [21:06:49.889779] [21:06:49.889837] Test:  [20/68]  eta: 0:00:10  loss: 0.6055 (0.8291)  acc1: 81.2500 (77.9762)  acc5: 100.0000 (98.2143)  time: 0.1390  data: 0.1228  max mem: 4666\n",
      "[21:06:51.275924] [21:06:51.275913] [21:06:51.275979] Test:  [30/68]  eta: 0:00:07  loss: 0.7131 (1.0347)  acc1: 75.0000 (68.8508)  acc5: 100.0000 (95.0605)  time: 0.1381  data: 0.1222  max mem: 4666\n",
      "[21:06:52.694437] [21:06:52.694426] [21:06:52.694532] Test:  [40/68]  eta: 0:00:05  loss: 1.3714 (1.1362)  acc1: 43.7500 (61.8140)  acc5: 93.7500 (95.2744)  time: 0.1402  data: 0.1242  max mem: 4666\n",
      "[21:06:54.071880] [21:06:54.071870] [21:06:54.071931] Test:  [50/68]  eta: 0:00:03  loss: 1.2089 (1.1222)  acc1: 43.7500 (62.1324)  acc5: 100.0000 (96.2010)  time: 0.1397  data: 0.1238  max mem: 4666\n",
      "[21:06:55.440610] [21:06:55.440599] [21:06:55.440680] Test:  [60/68]  eta: 0:00:01  loss: 1.1323 (1.1391)  acc1: 62.5000 (61.1680)  acc5: 100.0000 (96.6701)  time: 0.1372  data: 0.1213  max mem: 4666\n",
      "[21:06:55.565222] [21:06:55.565216] [21:06:55.565236] Test:  [67/68]  eta: 0:00:00  loss: 1.0468 (1.1372)  acc1: 65.6250 (61.9289)  acc5: 100.0000 (96.9543)  time: 0.1374  data: 0.1217  max mem: 4666\n",
      "[21:06:55.632214] [21:06:55.632205] [21:06:55.632230] Test: Total time: 0:00:10 (0.1512 s / it)\n",
      "[21:06:55.632267] [21:06:55.632264] [21:06:55.632273] * Acc@1 61.929 Acc@5 96.954 loss 1.137\n",
      "[21:06:55.632367] [21:06:55.632365] [21:06:55.632374] Accuracy of the network on the 2167 test images: 61.9%\n",
      "[21:06:55.632383] [21:06:55.632381] [21:06:55.632388] Max accuracy: 62.57%\n",
      "[21:06:57.215309] [21:06:57.215283] [21:06:57.215505] Epoch: [39]  [  0/316]  eta: 0:08:19  lr: 0.000179  loss: 1.7180 (1.7180)  time: 1.5815  data: 1.5287  max mem: 4666\n",
      "[21:06:59.697264] [21:06:59.697252] [21:06:59.697336] Epoch: [39]  [ 20/316]  eta: 0:00:57  lr: 0.000179  loss: 1.5621 (1.5902)  time: 0.1240  data: 0.0725  max mem: 4666\n",
      "[21:07:02.158352] [21:07:02.158342] [21:07:02.158432] Epoch: [39]  [ 40/316]  eta: 0:00:43  lr: 0.000179  loss: 1.5572 (1.5777)  time: 0.1230  data: 0.0717  max mem: 4666\n",
      "[21:07:04.597805] [21:07:04.597795] [21:07:04.597861] Epoch: [39]  [ 60/316]  eta: 0:00:37  lr: 0.000179  loss: 1.6122 (1.5857)  time: 0.1219  data: 0.0708  max mem: 4666\n",
      "[21:07:07.064624] [21:07:07.064614] [21:07:07.064680] Epoch: [39]  [ 80/316]  eta: 0:00:33  lr: 0.000178  loss: 1.6061 (1.5919)  time: 0.1233  data: 0.0719  max mem: 4666\n",
      "[21:07:09.556675] [21:07:09.556665] [21:07:09.556729] Epoch: [39]  [100/316]  eta: 0:00:29  lr: 0.000178  loss: 1.5795 (1.5947)  time: 0.1246  data: 0.0731  max mem: 4666\n",
      "[21:07:11.999264] [21:07:11.999253] [21:07:11.999324] Epoch: [39]  [120/316]  eta: 0:00:26  lr: 0.000178  loss: 1.5960 (1.5870)  time: 0.1221  data: 0.0707  max mem: 4666\n",
      "[21:07:14.441360] [21:07:14.441350] [21:07:14.441424] Epoch: [39]  [140/316]  eta: 0:00:23  lr: 0.000178  loss: 1.6140 (1.5926)  time: 0.1221  data: 0.0707  max mem: 4666\n",
      "[21:07:16.905344] [21:07:16.905333] [21:07:16.905403] Epoch: [39]  [160/316]  eta: 0:00:20  lr: 0.000177  loss: 1.5526 (1.5907)  time: 0.1231  data: 0.0718  max mem: 4666\n",
      "[21:07:19.398334] [21:07:19.398323] [21:07:19.398394] Epoch: [39]  [180/316]  eta: 0:00:17  lr: 0.000177  loss: 1.5961 (1.5943)  time: 0.1246  data: 0.0732  max mem: 4666\n",
      "[21:07:21.840533] [21:07:21.840521] [21:07:21.840594] Epoch: [39]  [200/316]  eta: 0:00:15  lr: 0.000177  loss: 1.6189 (1.5956)  time: 0.1221  data: 0.0707  max mem: 4666\n",
      "[21:07:24.304588] [21:07:24.304576] [21:07:24.304647] Epoch: [39]  [220/316]  eta: 0:00:12  lr: 0.000177  loss: 1.5380 (1.5938)  time: 0.1232  data: 0.0718  max mem: 4666\n",
      "[21:07:26.763221] [21:07:26.763211] [21:07:26.763278] Epoch: [39]  [240/316]  eta: 0:00:09  lr: 0.000176  loss: 1.5821 (1.5942)  time: 0.1229  data: 0.0715  max mem: 4666\n",
      "[21:07:29.220084] [21:07:29.220073] [21:07:29.220145] Epoch: [39]  [260/316]  eta: 0:00:07  lr: 0.000176  loss: 1.6055 (1.5931)  time: 0.1228  data: 0.0714  max mem: 4666\n",
      "[21:07:31.657741] [21:07:31.657731] [21:07:31.657797] Epoch: [39]  [280/316]  eta: 0:00:04  lr: 0.000176  loss: 1.5927 (1.5943)  time: 0.1218  data: 0.0705  max mem: 4666\n",
      "[21:07:34.098065] [21:07:34.098055] [21:07:34.098130] Epoch: [39]  [300/316]  eta: 0:00:02  lr: 0.000176  loss: 1.6207 (1.5947)  time: 0.1220  data: 0.0707  max mem: 4666\n",
      "[21:07:35.588061] [21:07:35.588052] [21:07:35.588111] Epoch: [39]  [315/316]  eta: 0:00:00  lr: 0.000176  loss: 1.5563 (1.5947)  time: 0.1230  data: 0.0718  max mem: 4666\n",
      "[21:07:35.664451] [21:07:35.664446] [21:07:35.664462] Epoch: [39] Total time: 0:00:40 (0.1267 s / it)\n",
      "[21:07:35.664490] [21:07:35.664488] [21:07:35.664498] Averaged stats: lr: 0.000176  loss: 1.5563 (1.5947)\n",
      "[21:07:39.093272] [21:07:39.093249] [21:07:39.093433] Test:  [ 0/68]  eta: 0:01:57  loss: 0.9467 (0.9467)  acc1: 71.8750 (71.8750)  acc5: 100.0000 (100.0000)  time: 1.7309  data: 1.7140  max mem: 4666\n",
      "[21:07:40.455144] [21:07:40.455135] [21:07:40.455374] Test:  [10/68]  eta: 0:00:16  loss: 1.2145 (1.1329)  acc1: 65.6250 (61.9318)  acc5: 100.0000 (99.1477)  time: 0.2811  data: 0.2646  max mem: 4666\n",
      "[21:07:41.906891] [21:07:41.906879] [21:07:41.906981] Test:  [20/68]  eta: 0:00:10  loss: 0.6982 (0.8676)  acc1: 81.2500 (75.2976)  acc5: 100.0000 (99.4048)  time: 0.1406  data: 0.1244  max mem: 4666\n",
      "[21:07:43.301315] [21:07:43.301305] [21:07:43.301379] Test:  [30/68]  eta: 0:00:07  loss: 0.8159 (1.0831)  acc1: 65.6250 (65.4234)  acc5: 100.0000 (96.1694)  time: 0.1422  data: 0.1263  max mem: 4666\n",
      "[21:07:44.705628] [21:07:44.705616] [21:07:44.705690] Test:  [40/68]  eta: 0:00:05  loss: 1.2650 (1.1310)  acc1: 46.8750 (61.9665)  acc5: 96.8750 (96.2652)  time: 0.1399  data: 0.1239  max mem: 4666\n",
      "[21:07:46.084057] [21:07:46.084047] [21:07:46.084266] Test:  [50/68]  eta: 0:00:03  loss: 1.0575 (1.0853)  acc1: 59.3750 (64.2157)  acc5: 100.0000 (96.9975)  time: 0.1391  data: 0.1231  max mem: 4666\n",
      "[21:07:47.474536] [21:07:47.474521] [21:07:47.474607] Test:  [60/68]  eta: 0:00:01  loss: 0.9731 (1.1066)  acc1: 62.5000 (63.2172)  acc5: 100.0000 (97.4385)  time: 0.1384  data: 0.1224  max mem: 4666\n",
      "[21:07:47.581688] [21:07:47.581683] [21:07:47.581698] Test:  [67/68]  eta: 0:00:00  loss: 1.0919 (1.1144)  acc1: 62.5000 (62.9903)  acc5: 100.0000 (97.6927)  time: 0.1381  data: 0.1224  max mem: 4666\n",
      "[21:07:47.643461] [21:07:47.643456] [21:07:47.643471] Test: Total time: 0:00:10 (0.1512 s / it)\n",
      "[21:07:47.643503] [21:07:47.643500] [21:07:47.643509] * Acc@1 62.990 Acc@5 97.693 loss 1.114\n",
      "[21:07:47.643579] [21:07:47.643577] [21:07:47.643586] Accuracy of the network on the 2167 test images: 63.0%\n",
      "[21:07:47.643597] [21:07:47.643595] [21:07:47.643602] Max accuracy: 62.99%\n",
      "[21:07:49.225552] [21:07:49.225532] [21:07:49.225742] Epoch: [40]  [  0/316]  eta: 0:08:19  lr: 0.000176  loss: 1.7092 (1.7092)  time: 1.5805  data: 1.5296  max mem: 4666\n",
      "[21:07:51.694308] [21:07:51.694298] [21:07:51.694360] Epoch: [40]  [ 20/316]  eta: 0:00:57  lr: 0.000175  loss: 1.5810 (1.5804)  time: 0.1234  data: 0.0719  max mem: 4666\n",
      "[21:07:54.113791] [21:07:54.113780] [21:07:54.113841] Epoch: [40]  [ 40/316]  eta: 0:00:43  lr: 0.000175  loss: 1.6789 (1.6042)  time: 0.1209  data: 0.0695  max mem: 4666\n",
      "[21:07:56.559838] [21:07:56.559827] [21:07:56.559888] Epoch: [40]  [ 60/316]  eta: 0:00:37  lr: 0.000175  loss: 1.6715 (1.6113)  time: 0.1223  data: 0.0708  max mem: 4666\n",
      "[21:07:58.992733] [21:07:58.992721] [21:07:58.992799] Epoch: [40]  [ 80/316]  eta: 0:00:33  lr: 0.000175  loss: 1.6511 (1.6136)  time: 0.1216  data: 0.0701  max mem: 4666\n",
      "[21:08:01.511202] [21:08:01.511191] [21:08:01.511256] Epoch: [40]  [100/316]  eta: 0:00:29  lr: 0.000174  loss: 1.6274 (1.6188)  time: 0.1259  data: 0.0744  max mem: 4666\n",
      "[21:08:03.969426] [21:08:03.969415] [21:08:03.969476] Epoch: [40]  [120/316]  eta: 0:00:26  lr: 0.000174  loss: 1.5385 (1.6097)  time: 0.1229  data: 0.0713  max mem: 4666\n",
      "[21:08:06.408731] [21:08:06.408720] [21:08:06.408782] Epoch: [40]  [140/316]  eta: 0:00:23  lr: 0.000174  loss: 1.6230 (1.6105)  time: 0.1219  data: 0.0703  max mem: 4666\n",
      "[21:08:08.853891] [21:08:08.853874] [21:08:08.853941] Epoch: [40]  [160/316]  eta: 0:00:20  lr: 0.000174  loss: 1.5541 (1.6041)  time: 0.1222  data: 0.0707  max mem: 4666\n",
      "[21:08:11.283358] [21:08:11.283346] [21:08:11.283427] Epoch: [40]  [180/316]  eta: 0:00:17  lr: 0.000173  loss: 1.6438 (1.6055)  time: 0.1214  data: 0.0700  max mem: 4666\n",
      "[21:08:13.747907] [21:08:13.747898] [21:08:13.748025] Epoch: [40]  [200/316]  eta: 0:00:15  lr: 0.000173  loss: 1.5562 (1.6020)  time: 0.1232  data: 0.0716  max mem: 4666\n",
      "[21:08:16.195176] [21:08:16.195166] [21:08:16.195251] Epoch: [40]  [220/316]  eta: 0:00:12  lr: 0.000173  loss: 1.5680 (1.6015)  time: 0.1223  data: 0.0708  max mem: 4666\n",
      "[21:08:18.617089] [21:08:18.617079] [21:08:18.617145] Epoch: [40]  [240/316]  eta: 0:00:09  lr: 0.000173  loss: 1.6241 (1.6026)  time: 0.1211  data: 0.0695  max mem: 4666\n",
      "[21:08:21.051625] [21:08:21.051613] [21:08:21.051682] Epoch: [40]  [260/316]  eta: 0:00:07  lr: 0.000172  loss: 1.5714 (1.6005)  time: 0.1217  data: 0.0702  max mem: 4666\n",
      "[21:08:23.471039] [21:08:23.471029] [21:08:23.471095] Epoch: [40]  [280/316]  eta: 0:00:04  lr: 0.000172  loss: 1.6347 (1.6041)  time: 0.1209  data: 0.0695  max mem: 4666\n",
      "[21:08:25.890938] [21:08:25.890928] [21:08:25.890995] Epoch: [40]  [300/316]  eta: 0:00:02  lr: 0.000172  loss: 1.5379 (1.6038)  time: 0.1210  data: 0.0695  max mem: 4666\n",
      "[21:08:27.528983] [21:08:27.528973] [21:08:27.529027] Epoch: [40]  [315/316]  eta: 0:00:00  lr: 0.000172  loss: 1.5801 (1.6032)  time: 0.1218  data: 0.0706  max mem: 4666\n",
      "[21:08:27.600593] [21:08:27.600582] [21:08:27.600610] Epoch: [40] Total time: 0:00:39 (0.1264 s / it)\n",
      "[21:08:27.600847] [21:08:27.600843] [21:08:27.600855] Averaged stats: lr: 0.000172  loss: 1.5801 (1.6032)\n",
      "[21:08:31.045109] [21:08:31.045082] [21:08:31.045304] Test:  [ 0/68]  eta: 0:01:55  loss: 1.0024 (1.0024)  acc1: 78.1250 (78.1250)  acc5: 100.0000 (100.0000)  time: 1.6925  data: 1.6753  max mem: 4666\n",
      "[21:08:32.403850] [21:08:32.403840] [21:08:32.403902] Test:  [10/68]  eta: 0:00:16  loss: 1.2625 (1.1750)  acc1: 62.5000 (61.3636)  acc5: 100.0000 (98.8636)  time: 0.2773  data: 0.2608  max mem: 4666\n",
      "[21:08:33.828531] [21:08:33.828521] [21:08:33.828586] Test:  [20/68]  eta: 0:00:10  loss: 0.7391 (0.8979)  acc1: 78.1250 (75.4464)  acc5: 100.0000 (99.4048)  time: 0.1391  data: 0.1229  max mem: 4666\n",
      "[21:08:35.246057] [21:08:35.246044] [21:08:35.246125] Test:  [30/68]  eta: 0:00:07  loss: 0.8178 (1.0738)  acc1: 71.8750 (67.6411)  acc5: 100.0000 (96.1694)  time: 0.1420  data: 0.1260  max mem: 4666\n",
      "[21:08:36.634886] [21:08:36.634876] [21:08:36.634965] Test:  [40/68]  eta: 0:00:04  loss: 1.3663 (1.1710)  acc1: 40.6250 (60.4421)  acc5: 93.7500 (95.5793)  time: 0.1402  data: 0.1243  max mem: 4666\n",
      "[21:08:38.010978] [21:08:38.010968] [21:08:38.011255] Test:  [50/68]  eta: 0:00:03  loss: 1.2080 (1.1310)  acc1: 40.6250 (61.6422)  acc5: 100.0000 (96.4461)  time: 0.1382  data: 0.1222  max mem: 4666\n",
      "[21:08:39.401246] [21:08:39.401236] [21:08:39.401292] Test:  [60/68]  eta: 0:00:01  loss: 0.9993 (1.1144)  acc1: 68.7500 (62.7561)  acc5: 100.0000 (97.0287)  time: 0.1382  data: 0.1223  max mem: 4666\n",
      "[21:08:39.508396] [21:08:39.508391] [21:08:39.508408] Test:  [67/68]  eta: 0:00:00  loss: 0.9922 (1.1072)  acc1: 71.8750 (63.7287)  acc5: 100.0000 (97.3235)  time: 0.1380  data: 0.1223  max mem: 4666\n",
      "[21:08:39.578041] [21:08:39.578029] [21:08:39.578064] Test: Total time: 0:00:10 (0.1504 s / it)\n",
      "[21:08:39.578133] [21:08:39.578128] [21:08:39.578146] * Acc@1 63.729 Acc@5 97.323 loss 1.107\n",
      "[21:08:39.578295] [21:08:39.578290] [21:08:39.578306] Accuracy of the network on the 2167 test images: 63.7%\n",
      "[21:08:39.578323] [21:08:39.578320] [21:08:39.578334] Max accuracy: 63.73%\n",
      "[21:08:41.194302] [21:08:41.194278] [21:08:41.194489] Epoch: [41]  [  0/316]  eta: 0:08:30  lr: 0.000172  loss: 1.5970 (1.5970)  time: 1.6144  data: 1.5638  max mem: 4666\n",
      "[21:08:43.636915] [21:08:43.636903] [21:08:43.636985] Epoch: [41]  [ 20/316]  eta: 0:00:57  lr: 0.000171  loss: 1.6151 (1.5991)  time: 0.1221  data: 0.0704  max mem: 4666\n",
      "[21:08:46.125568] [21:08:46.125557] [21:08:46.125626] Epoch: [41]  [ 40/316]  eta: 0:00:44  lr: 0.000171  loss: 1.5450 (1.5813)  time: 0.1244  data: 0.0727  max mem: 4666\n",
      "[21:08:48.531033] [21:08:48.531023] [21:08:48.531089] Epoch: [41]  [ 60/316]  eta: 0:00:37  lr: 0.000171  loss: 1.6189 (1.5891)  time: 0.1202  data: 0.0685  max mem: 4666\n",
      "[21:08:50.980271] [21:08:50.980260] [21:08:50.980325] Epoch: [41]  [ 80/316]  eta: 0:00:33  lr: 0.000171  loss: 1.5783 (1.5956)  time: 0.1224  data: 0.0708  max mem: 4666\n",
      "[21:08:53.391071] [21:08:53.391061] [21:08:53.391127] Epoch: [41]  [100/316]  eta: 0:00:29  lr: 0.000171  loss: 1.6513 (1.6001)  time: 0.1205  data: 0.0691  max mem: 4666\n",
      "[21:08:55.860073] [21:08:55.860063] [21:08:55.860128] Epoch: [41]  [120/316]  eta: 0:00:26  lr: 0.000170  loss: 1.6207 (1.5969)  time: 0.1234  data: 0.0720  max mem: 4666\n",
      "[21:08:58.303509] [21:08:58.303497] [21:08:58.303568] Epoch: [41]  [140/316]  eta: 0:00:23  lr: 0.000170  loss: 1.6366 (1.6004)  time: 0.1221  data: 0.0707  max mem: 4666\n",
      "[21:09:00.682788] [21:09:00.682778] [21:09:00.682916] Epoch: [41]  [160/316]  eta: 0:00:20  lr: 0.000170  loss: 1.5567 (1.5969)  time: 0.1189  data: 0.0675  max mem: 4666\n",
      "[21:09:03.094311] [21:09:03.094300] [21:09:03.094362] Epoch: [41]  [180/316]  eta: 0:00:17  lr: 0.000170  loss: 1.6303 (1.6016)  time: 0.1205  data: 0.0692  max mem: 4666\n",
      "[21:09:05.540634] [21:09:05.540622] [21:09:05.540694] Epoch: [41]  [200/316]  eta: 0:00:14  lr: 0.000169  loss: 1.5972 (1.6009)  time: 0.1223  data: 0.0708  max mem: 4666\n",
      "[21:09:07.991059] [21:09:07.991049] [21:09:07.991112] Epoch: [41]  [220/316]  eta: 0:00:12  lr: 0.000169  loss: 1.5710 (1.5975)  time: 0.1225  data: 0.0711  max mem: 4666\n",
      "[21:09:10.490282] [21:09:10.490271] [21:09:10.490339] Epoch: [41]  [240/316]  eta: 0:00:09  lr: 0.000169  loss: 1.5936 (1.5980)  time: 0.1249  data: 0.0735  max mem: 4666\n",
      "[21:09:12.874614] [21:09:12.874604] [21:09:12.874670] Epoch: [41]  [260/316]  eta: 0:00:07  lr: 0.000169  loss: 1.6032 (1.5982)  time: 0.1192  data: 0.0678  max mem: 4666\n",
      "[21:09:15.339108] [21:09:15.339098] [21:09:15.339242] Epoch: [41]  [280/316]  eta: 0:00:04  lr: 0.000168  loss: 1.5652 (1.5966)  time: 0.1232  data: 0.0718  max mem: 4666\n",
      "[21:09:17.775000] [21:09:17.774989] [21:09:17.775054] Epoch: [41]  [300/316]  eta: 0:00:02  lr: 0.000168  loss: 1.6081 (1.5952)  time: 0.1218  data: 0.0704  max mem: 4666\n",
      "[21:09:19.447174] [21:09:19.447165] [21:09:19.447220] Epoch: [41]  [315/316]  eta: 0:00:00  lr: 0.000168  loss: 1.5213 (1.5934)  time: 0.1182  data: 0.0670  max mem: 4666\n",
      "[21:09:19.518188] [21:09:19.518180] [21:09:19.518203] Epoch: [41] Total time: 0:00:39 (0.1264 s / it)\n",
      "[21:09:19.518244] [21:09:19.518242] [21:09:19.518250] Averaged stats: lr: 0.000168  loss: 1.5213 (1.5934)\n",
      "[21:09:22.932088] [21:09:22.932066] [21:09:22.932270] Test:  [ 0/68]  eta: 0:01:56  loss: 0.9233 (0.9233)  acc1: 71.8750 (71.8750)  acc5: 96.8750 (96.8750)  time: 1.7091  data: 1.6917  max mem: 4666\n",
      "[21:09:24.299079] [21:09:24.299069] [21:09:24.299157] Test:  [10/68]  eta: 0:00:16  loss: 1.2069 (1.1565)  acc1: 65.6250 (61.9318)  acc5: 96.8750 (97.4432)  time: 0.2796  data: 0.2631  max mem: 4666\n",
      "[21:09:25.710625] [21:09:25.710614] [21:09:25.710868] Test:  [20/68]  eta: 0:00:10  loss: 0.8155 (0.9169)  acc1: 81.2500 (74.4048)  acc5: 100.0000 (98.3631)  time: 0.1388  data: 0.1226  max mem: 4666\n",
      "[21:09:27.107673] [21:09:27.107660] [21:09:27.107735] Test:  [30/68]  eta: 0:00:07  loss: 0.8714 (1.0759)  acc1: 81.2500 (67.9435)  acc5: 100.0000 (96.0685)  time: 0.1403  data: 0.1244  max mem: 4666\n",
      "[21:09:28.485548] [21:09:28.485537] [21:09:28.485604] Test:  [40/68]  eta: 0:00:04  loss: 1.1892 (1.1053)  acc1: 59.3750 (64.5579)  acc5: 96.8750 (96.1890)  time: 0.1387  data: 0.1227  max mem: 4666\n",
      "[21:09:29.868740] [21:09:29.868730] [21:09:29.868790] Test:  [50/68]  eta: 0:00:03  loss: 1.1244 (1.0617)  acc1: 56.2500 (65.2574)  acc5: 100.0000 (96.9363)  time: 0.1380  data: 0.1220  max mem: 4666\n",
      "[21:09:31.247253] [21:09:31.247244] [21:09:31.247301] Test:  [60/68]  eta: 0:00:01  loss: 0.9244 (1.0691)  acc1: 68.7500 (65.1639)  acc5: 100.0000 (97.4385)  time: 0.1380  data: 0.1221  max mem: 4666\n",
      "[21:09:31.371245] [21:09:31.371235] [21:09:31.371263] Test:  [67/68]  eta: 0:00:00  loss: 0.9651 (1.0808)  acc1: 62.5000 (63.7287)  acc5: 100.0000 (97.6465)  time: 0.1374  data: 0.1217  max mem: 4666\n",
      "[21:09:31.434442] [21:09:31.434433] [21:09:31.434460] Test: Total time: 0:00:10 (0.1502 s / it)\n",
      "[21:09:31.434499] [21:09:31.434496] [21:09:31.434506] * Acc@1 63.729 Acc@5 97.647 loss 1.081\n",
      "[21:09:31.434716] [21:09:31.434712] [21:09:31.434724] Accuracy of the network on the 2167 test images: 63.7%\n",
      "[21:09:31.434735] [21:09:31.434733] [21:09:31.434740] Max accuracy: 63.73%\n",
      "[21:09:32.955404] [21:09:32.955382] [21:09:32.955560] Epoch: [42]  [  0/316]  eta: 0:08:00  lr: 0.000168  loss: 1.5469 (1.5469)  time: 1.5193  data: 1.4693  max mem: 4666\n",
      "[21:09:35.462834] [21:09:35.462824] [21:09:35.462898] Epoch: [42]  [ 20/316]  eta: 0:00:56  lr: 0.000168  loss: 1.5910 (1.5862)  time: 0.1253  data: 0.0739  max mem: 4666\n",
      "[21:09:37.934377] [21:09:37.934367] [21:09:37.934511] Epoch: [42]  [ 40/316]  eta: 0:00:43  lr: 0.000167  loss: 1.5160 (1.5723)  time: 0.1235  data: 0.0721  max mem: 4666\n",
      "[21:09:40.502624] [21:09:40.502610] [21:09:40.502688] Epoch: [42]  [ 60/316]  eta: 0:00:38  lr: 0.000167  loss: 1.5830 (1.5726)  time: 0.1283  data: 0.0763  max mem: 4666\n",
      "[21:09:42.996029] [21:09:42.996018] [21:09:42.996090] Epoch: [42]  [ 80/316]  eta: 0:00:33  lr: 0.000167  loss: 1.6307 (1.5801)  time: 0.1246  data: 0.0728  max mem: 4666\n",
      "[21:09:45.468991] [21:09:45.468979] [21:09:45.469044] Epoch: [42]  [100/316]  eta: 0:00:29  lr: 0.000167  loss: 1.6626 (1.5901)  time: 0.1236  data: 0.0720  max mem: 4666\n",
      "[21:09:47.930844] [21:09:47.930834] [21:09:47.930899] Epoch: [42]  [120/316]  eta: 0:00:26  lr: 0.000166  loss: 1.5961 (1.5890)  time: 0.1230  data: 0.0714  max mem: 4666\n",
      "[21:09:50.365723] [21:09:50.365711] [21:09:50.365785] Epoch: [42]  [140/316]  eta: 0:00:23  lr: 0.000166  loss: 1.4800 (1.5832)  time: 0.1217  data: 0.0701  max mem: 4666\n",
      "[21:09:52.782652] [21:09:52.782641] [21:09:52.782708] Epoch: [42]  [160/316]  eta: 0:00:20  lr: 0.000166  loss: 1.5693 (1.5860)  time: 0.1208  data: 0.0691  max mem: 4666\n",
      "[21:09:55.290860] [21:09:55.290850] [21:09:55.290911] Epoch: [42]  [180/316]  eta: 0:00:17  lr: 0.000166  loss: 1.6053 (1.5893)  time: 0.1254  data: 0.0736  max mem: 4666\n",
      "[21:09:57.768975] [21:09:57.768964] [21:09:57.769023] Epoch: [42]  [200/316]  eta: 0:00:15  lr: 0.000165  loss: 1.5348 (1.5867)  time: 0.1239  data: 0.0722  max mem: 4666\n",
      "[21:10:00.221090] [21:10:00.221080] [21:10:00.221140] Epoch: [42]  [220/316]  eta: 0:00:12  lr: 0.000165  loss: 1.5818 (1.5860)  time: 0.1226  data: 0.0709  max mem: 4666\n",
      "[21:10:02.660279] [21:10:02.660269] [21:10:02.660336] Epoch: [42]  [240/316]  eta: 0:00:09  lr: 0.000165  loss: 1.5775 (1.5851)  time: 0.1219  data: 0.0703  max mem: 4666\n",
      "[21:10:05.165689] [21:10:05.165678] [21:10:05.165740] Epoch: [42]  [260/316]  eta: 0:00:07  lr: 0.000165  loss: 1.5140 (1.5807)  time: 0.1252  data: 0.0736  max mem: 4666\n"
     ]
    }
   ],
   "source": [
    "train = False\n",
    "\n",
    "if train:\n",
    "    print(f\"Start training for {args.epochs} epochs\")\n",
    "    start_time = time.time()\n",
    "    max_accuracy = 0.0\n",
    "    \n",
    "    for epoch in range(args.start_epoch, args.epochs):\n",
    "        if args.distributed:\n",
    "            data_loader_train.sampler.set_epoch(epoch)\n",
    "        train_stats = train_one_epoch(\n",
    "            model, criterion, data_loader_train,\n",
    "            optimizer, device, epoch, loss_scaler,\n",
    "            args.clip_grad, mixup_fn,\n",
    "            log_writer=log_writer,\n",
    "            args=args\n",
    "        )\n",
    "        if args.output_dir:\n",
    "            misc.save_model(\n",
    "                args=args, model=model, model_without_ddp=model_without_ddp, optimizer=optimizer,\n",
    "                loss_scaler=loss_scaler, epoch=epoch)\n",
    "\n",
    "        test_stats = evaluate(data_loader_val, model, device)\n",
    "        print(f\"Accuracy of the network on the {len(dataset_val)} test images: {test_stats['acc1']:.1f}%\")\n",
    "        max_accuracy = max(max_accuracy, test_stats[\"acc1\"])\n",
    "        print(f'Max accuracy: {max_accuracy:.2f}%')\n",
    "\n",
    "        if log_writer is not None:\n",
    "            log_writer.add_scalar('perf/test_acc1', test_stats['acc1'], epoch)\n",
    "            log_writer.add_scalar('perf/test_acc5', test_stats['acc5'], epoch)\n",
    "            log_writer.add_scalar('perf/test_loss', test_stats['loss'], epoch)\n",
    "\n",
    "        log_stats = {**{f'train_{k}': v for k, v in train_stats.items()},\n",
    "                        **{f'test_{k}': v for k, v in test_stats.items()},\n",
    "                        'epoch': epoch,\n",
    "                        'n_parameters': n_parameters}\n",
    "\n",
    "        if args.output_dir and misc.is_main_process():\n",
    "            if log_writer is not None:\n",
    "                log_writer.flush()\n",
    "            with open(os.path.join(args.output_dir, \"log.txt\"), mode=\"a\", encoding=\"utf-8\") as f:\n",
    "                f.write(json.dumps(log_stats) + \"\\n\")\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
    "    print('Training time {}'.format(total_time_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f19af4-f98c-4cac-893b-45a39ee17caa",
   "metadata": {},
   "source": [
    "# Evaluate loaded model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3c948e-620d-448f-a08e-dd3f3a8e58e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "EXPERIMENT_NAME = \"eval_vit_base\"\n",
    "\n",
    "saving_model = f\"{EXPERIMENT_NAME}/models\"\n",
    "os.makedirs(saving_model, exist_ok = True)\n",
    "os.makedirs(EXPERIMENT_NAME, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebc1e78-3a15-40df-ad5f-6c0c756b6d6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if args.eval:\n",
    "        test_stats = evaluate(data_loader_val, model, device)\n",
    "        print(f\"Accuracy of the network on the {len(dataset_val)} test images: {test_stats['acc1']:.1f}%\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285c69a2-6d2d-4368-9c9d-01bdf74613c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate_test(data_loader, model, device):\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    metric_logger = misc.MetricLogger(delimiter=\"  \")\n",
    "    header = 'Test:'\n",
    "\n",
    "    # switch to evaluation mode\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    for batch in metric_logger.log_every(data_loader, 10, header):\n",
    "        images = batch[0]\n",
    "        target = batch[-1]\n",
    "        images = images.to(device, non_blocking=True)\n",
    "        target = target.to(device, non_blocking=True)\n",
    "\n",
    "        # compute output\n",
    "        with torch.cuda.amp.autocast():\n",
    "            output = model(images)            \n",
    "            loss = criterion(output, target)#\n",
    "            pred = output.argmax(dim=1) \n",
    "        all_predictions.append(pred.cpu().numpy())# ADDED\n",
    "        all_labels.append(target.cpu().numpy())# ADDED\n",
    "        acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
    "        \n",
    "        batch_size = images.shape[0]\n",
    "        metric_logger.update(loss=loss.item())\n",
    "        metric_logger.meters['acc1'].update(acc1.item(), n=batch_size)\n",
    "        metric_logger.meters['acc5'].update(acc5.item(), n=batch_size)\n",
    "    \n",
    "    all_predictions = np.array(all_predictions)#.squeeze(0)\n",
    "    all_labels = np.array(all_labels)#.squeeze(0)\n",
    "    # gather the stats from all processes\n",
    "    metric_logger.synchronize_between_processes()\n",
    "    print('* Acc@1 {top1.global_avg:.3f} Acc@5 {top5.global_avg:.3f} loss {losses.global_avg:.3f}'\n",
    "          .format(top1=metric_logger.acc1, top5=metric_logger.acc5, losses=metric_logger.loss))\n",
    "\n",
    "    # return \n",
    "\n",
    "    return {k: meter.global_avg for k, meter in metric_logger.meters.items()}, np.concatenate(all_predictions, axis=0), np.concatenate(all_labels, axis=0)\n",
    "    \n",
    "\n",
    "metrics, all_predictions, all_labels = evaluate_test(data_loader_val, model, device)\n",
    "# print(f\"Accuracy of the network on the {len(dataset_val)} test images: {test_stats['acc1']:.1f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37251664-d171-4983-827a-b8eccbffcb9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fb3829-f24f-4748-a7a7-0b7c24e26795",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0792b94-181c-4582-8a21-6708545a63d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "unique_classes = np.unique(np.concatenate((all_labels, all_predictions)))\n",
    "unique_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15f592c-a922-439c-9656-f211f32fc0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_mat = confusion_matrix(all_labels, all_predictions, labels=unique_classes)\n",
    "conf_matrix = pd.DataFrame(confusion_mat, index=unique_classes, columns=unique_classes)\n",
    "conf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752d70d9-0d75-478a-a90a-0815db8fdb89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "unique_classes = np.unique(np.concatenate((all_labels, all_predictions)))\n",
    "confusion_mat = confusion_matrix(all_labels, all_predictions, labels=unique_classes)\n",
    "conf_matrix = pd.DataFrame(confusion_mat, index=unique_classes, columns=unique_classes)\n",
    "\n",
    "# Plot the confusion matrix using seaborn\n",
    "plt.figure(figsize=(5, 4))\n",
    "ax = sns.heatmap(conf_matrix, annot=True,  fmt='.1f', cmap=sns.cubehelix_palette(as_cmap=True), linewidths=0.1, cbar=True)\n",
    "\n",
    "# Set labels and ticks\n",
    "ax.set_xlabel('Predicted Labels')\n",
    "ax.set_ylabel('True Labels')\n",
    "\n",
    "# Set x and y ticks using the unique classes\n",
    "ax.set_xticks(range(len(unique_classes)))\n",
    "ax.set_yticks(range(len(unique_classes)))\n",
    "\n",
    "# Set x and y ticks at the center of the cells\n",
    "ax.set_xticks([i + 0.5 for i in range(len(unique_classes))])\n",
    "ax.set_yticks([i + 0.5 for i in range(len(unique_classes))])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6ac4b1-69e4-4e2b-bb75-65197ad1875c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_multiclass_roc_curve(all_labels, all_predictions, EXPERIMENT_NAME=\".\"):\n",
    "    # Step 1: Label Binarization\n",
    "    label_binarizer = LabelBinarizer()\n",
    "    y_onehot = label_binarizer.fit_transform(all_labels)\n",
    "    all_predictions_hot = label_binarizer.transform(all_predictions)\n",
    "\n",
    "    # Step 2: Calculate ROC curves\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    unique_classes = range(y_onehot.shape[1])\n",
    "    for i in unique_classes:\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_onehot[:, i], all_predictions_hot[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    # Step 3: Plot ROC curves\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "    # Micro-average ROC curve\n",
    "    fpr_micro, tpr_micro, _ = roc_curve(y_onehot.ravel(), all_predictions_hot.ravel())\n",
    "    roc_auc_micro = auc(fpr_micro, tpr_micro)\n",
    "    plt.plot(\n",
    "        fpr_micro,\n",
    "        tpr_micro,\n",
    "        label=f\"micro-average ROC curve (AUC = {roc_auc_micro:.2f})\",\n",
    "        color=\"deeppink\",\n",
    "        linestyle=\":\",\n",
    "        linewidth=4,\n",
    "    )\n",
    "\n",
    "    # Macro-average ROC curve\n",
    "    all_fpr = np.unique(np.concatenate([fpr[i] for i in unique_classes]))\n",
    "    mean_tpr = np.zeros_like(all_fpr)\n",
    "    for i in unique_classes:\n",
    "        mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
    "    mean_tpr /= len(unique_classes)\n",
    "    fpr_macro = all_fpr\n",
    "    tpr_macro = mean_tpr\n",
    "    roc_auc_macro = auc(fpr_macro, tpr_macro)\n",
    "    plt.plot(\n",
    "        fpr_macro,\n",
    "        tpr_macro,\n",
    "        label=f\"macro-average ROC curve (AUC = {roc_auc_macro:.2f})\",\n",
    "        color=\"navy\",\n",
    "        linestyle=\":\",\n",
    "        linewidth=4,\n",
    "    )\n",
    "\n",
    "    # Individual class ROC curves with unique colors\n",
    "    colors = plt.cm.rainbow(np.linspace(0, 1, len(unique_classes)))\n",
    "    for class_id, color in zip(unique_classes, colors):\n",
    "        plt.plot(\n",
    "            fpr[class_id],\n",
    "            tpr[class_id],\n",
    "            color=color,\n",
    "            label=f\"ROC curve for Class {class_id} (AUC = {roc_auc[class_id]:.2f})\",\n",
    "            linewidth=2,\n",
    "        )\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], color='gray', linestyle='--', linewidth=2)  # Add diagonal line for reference\n",
    "    plt.axis(\"equal\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"Extension of Receiver Operating Characteristic\\n to One-vs-Rest multiclass\")\n",
    "    plt.legend()\n",
    "    plt.savefig(f'{EXPERIMENT_NAME}/roc_curve.png')\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "    \n",
    "plot_multiclass_roc_curve(all_labels, all_predictions, EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806d2d76-84c6-4020-8bfb-56f545976b8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# def visualize_predictions(model, val_loader, device, type_label=None, dataset_type=1, unique_classes=np.array([0, 1, 2, 3, 4, 5, 6])):\n",
    "\n",
    "#     criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "#     metric_logger = misc.MetricLogger(delimiter=\"  \")\n",
    "#     header = 'Test:'\n",
    "\n",
    "#     # switch to evaluation mode\n",
    "#     model.eval()\n",
    "#     all_predictions = []\n",
    "#     all_labels = []\n",
    "\n",
    "#     for batch in metric_logger.log_every(val_loader, 10, header):\n",
    "#         images = batch[0]\n",
    "#         target = batch[-1]\n",
    "#         images = images.to(device, non_blocking=True)\n",
    "#         target = target.to(device, non_blocking=True)\n",
    "\n",
    "#         # compute output\n",
    "#         with torch.cuda.amp.autocast():\n",
    "#             output = model(images)            \n",
    "#             loss = criterion(output, target)#\n",
    "#             pred = output.argmax(dim=1) \n",
    "#         all_predictions.append(pred.cpu().numpy())# ADDED\n",
    "#         all_labels.append(target.cpu().numpy())# ADDED\n",
    "#         acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
    "        \n",
    "#         batch_size = images.shape[0]\n",
    "#         metric_logger.update(loss=loss.item())\n",
    "#         metric_logger.meters['acc1'].update(acc1.item(), n=batch_size)\n",
    "#         metric_logger.meters['acc5'].update(acc5.item(), n=batch_size)\n",
    "    \n",
    "#     all_predictions = np.array(all_predictions)#.squeeze(0)\n",
    "#     all_labels = np.array(all_labels)#.squeeze(0)\n",
    "\n",
    "#     if type_label is None:\n",
    "#         type_label = unique_classes\n",
    "\n",
    "#     # Create a 4x4 grid for visualization\n",
    "#     num_rows = 4\n",
    "#     num_cols = 4\n",
    "\n",
    "#     plt.figure(figsize=(12, 12))\n",
    "\n",
    "#     for i in range(num_rows * num_cols):\n",
    "#         plt.subplot(num_rows, num_cols, i + 1)\n",
    "#         idx = np.random.randint(len(all_labels))\n",
    "#         import pdb;pdb.set_trace()\n",
    "#         plt.imshow(images[idx].cpu().numpy().squeeze(), cmap='gray')\n",
    "\n",
    "#         # Use the class names instead of numeric labels for Fashion MNIST\n",
    "#         if dataset_type == 1:\n",
    "#             class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "#             predicted_class = class_names[all_predictions[idx]]\n",
    "#             actual_class = class_names[all_labels[idx]]\n",
    "#         else:\n",
    "#             predicted_class = all_predictions[idx]\n",
    "#             actual_class = all_labels[idx]\n",
    "\n",
    "#         plt.title(f'Pred: {predicted_class}\\nActual: {actual_class}')\n",
    "#         plt.axis('off')\n",
    "\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "# visualize_predictions(model, data_loader_val, device, dataset_type=2, unique_classes=unique_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970b1388-0d8d-49a4-9547-c4b0de31fcc2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "unique_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d232cf15-0edb-4546-993a-a9839441c967",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "report = classification_report(all_labels, all_predictions, target_names=unique_classes,output_dict=True)# Mostrar el informe de \n",
    "\n",
    "df = pd.DataFrame(report).transpose()\n",
    "df.to_csv(os.path.join(EXPERIMENT_NAME, \"confusion_matrix.csv\"))\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c907e5c-a3e0-4eae-9994-082b269732d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate precision, recall, and specificity (micro-averaged)\n",
    "precision = precision_score(all_labels, all_predictions, average='micro')\n",
    "recall = recall_score(all_labels, all_predictions, average='micro')\n",
    "\n",
    "# Calculate true negatives, false positives, and specificity (micro-averaged)\n",
    "tn = np.sum((all_labels != 1) & (all_predictions != 1))\n",
    "fp = np.sum((all_labels != 1) & (all_predictions == 1))\n",
    "specificity = tn / (tn + fp)\n",
    "\n",
    "# Calculate F1 score (weighted average)\n",
    "f1 = f1_score(all_labels, all_predictions, average='weighted')\n",
    "evaluation_metrics = {\n",
    "    \"Acc1\": metrics['acc1'],  # Add acc1 metric\n",
    "    \"Acc5\": metrics['acc5'],  # Add acc5 metric\n",
    "    \"loss\": metrics['loss'],  # Add acc5 metric\n",
    "    \"F1 Score\": [f1],\n",
    "    \"Precision\": [precision],\n",
    "    \"Recall\": [recall],\n",
    "    \"Specificity\": [specificity]\n",
    "}\n",
    "evaluation_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cffce5-3186-440c-bc5c-aba6fae47d45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a DataFrame from the dictionary\n",
    "df = pd.DataFrame(evaluation_metrics)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv(f'{EXPERIMENT_NAME}/evaluation_metrics_for_table.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "retfound",
   "language": "python",
   "name": "retfound"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
