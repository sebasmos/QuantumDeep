{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0340b2b5-04a4-4a7f-b5ad-19ff4a30207f",
   "metadata": {},
   "source": [
    "# Model for audio data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fa3c46-27d1-4fd0-8380-bc701cf44772",
   "metadata": {
    "tags": []
   },
   "source": [
    "Ensure the following requirements are ready: \n",
    "\n",
    "large model\n",
    "\n",
    "`CUDA_VISIBLE_DEVICES=1  python main_finetune.py --accum_iter 4 --batch_size 64 --model vit_large_patch16 --finetune mae_pretrain_vit_large.pth --epochs 100 --blr 5e-4 --layer_decay 0.65 --weight_decay 0.05 --drop_path 0.1 --mixup 0.8 --cutmix 1.0 --reprob 0.25 --dist_eval --data_path /media/enc/vera1/sebastian/data/ABGQI_mel_spectrograms --nb_classes 5 --output_dir quinn_5_classes_v7march --input_size 224 --lr 0.0001 --warmup_epochs 10`\n",
    "\n",
    "```\n",
    "numpy==1.23.5\n",
    "torch==2.0.0+cu118\n",
    "matplotlib==3.7.1\n",
    "pillow==10.2.0\n",
    "timm==0.3.2\n",
    "```\n",
    "\n",
    "Then `git clone https://github.com/facebookresearch/mae.git`\n",
    "\n",
    "Run this notebook within `mae/` folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce1c58f-d275-4b39-82cb-9ec06349b322",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "370eb3e4-97f2-401c-83ba-b849b83761c6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-07 20:27:36.016805: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-05-07 20:27:36.017747: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-05-07 20:27:36.029916: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-05-07 20:27:36.097004: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-07 20:27:41.655441: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy version: 1.26.4\n",
      "CUDA version: 11.8 - Torch versteion: 2.0.0+cu118 - device count: 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x73bce27c7570>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'../') \n",
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import Subset, DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import pandas as pd \n",
    "# from MAE code\n",
    "from util.datasets import build_dataset\n",
    "import argparse\n",
    "import util.misc as misc\n",
    "import argparse\n",
    "import datetime\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import timm\n",
    "\n",
    "assert timm.__version__ == \"0.3.2\" # version check\n",
    "from timm.models.layers import trunc_normal_\n",
    "from timm.data.mixup import Mixup\n",
    "from timm.loss import LabelSmoothingCrossEntropy, SoftTargetCrossEntropy\n",
    "\n",
    "import util.lr_decay as lrd\n",
    "import util.misc as misc\n",
    "from util.datasets import build_dataset\n",
    "from util.pos_embed import interpolate_pos_embed\n",
    "from util.misc import NativeScalerWithGradNormCount as NativeScaler\n",
    "\n",
    "import models_vit\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import models_mae\n",
    "import torch; print(f'numpy version: {np.__version__}\\nCUDA version: {torch.version.cuda} - Torch versteion: {torch.__version__} - device count: {torch.cuda.device_count()}')\n",
    "\n",
    "from engine_finetune import train_one_epoch, evaluate\n",
    "from timm.data import Mixup\n",
    "from timm.utils import accuracy\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import torch.optim as optim\n",
    "imagenet_mean = np.array([0.485, 0.456, 0.406])\n",
    "imagenet_std = np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "def show_image(image, title=''):\n",
    "    # image is [H, W, 3]\n",
    "    assert image.shape[2] == 3\n",
    "    plt.imshow(torch.clip((image * imagenet_std + imagenet_mean) * 255, 0, 255).int())\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.axis('off')\n",
    "    return\n",
    "\n",
    "def prepare_model(chkpt_dir, arch='mae_vit_large_patch16'):\n",
    "    # build model\n",
    "    model = getattr(models_mae, arch)()\n",
    "    # load model\n",
    "    checkpoint = torch.load(chkpt_dir, map_location='cpu')\n",
    "    msg = model.load_state_dict(checkpoint['model'], strict=False)\n",
    "    print(msg)\n",
    "    return model\n",
    "\n",
    "def run_one_image(img, model):\n",
    "    x = torch.tensor(img)\n",
    "\n",
    "    # make it a batch-like\n",
    "    x = x.unsqueeze(dim=0)\n",
    "    x = torch.einsum('nhwc->nchw', x)\n",
    "\n",
    "    # run MAE\n",
    "    loss, y, mask = model(x.float(), mask_ratio=0.75)\n",
    "    y = model.unpatchify(y)\n",
    "    y = torch.einsum('nchw->nhwc', y).detach().cpu()\n",
    "\n",
    "    # visualize the mask\n",
    "    mask = mask.detach()\n",
    "    mask = mask.unsqueeze(-1).repeat(1, 1, model.patch_embed.patch_size[0]**2 *3)  # (N, H*W, p*p*3)\n",
    "    mask = model.unpatchify(mask)  # 1 is removing, 0 is keeping\n",
    "    mask = torch.einsum('nchw->nhwc', mask).detach().cpu()\n",
    "\n",
    "    x = torch.einsum('nchw->nhwc', x)\n",
    "\n",
    "    # masked image\n",
    "    im_masked = x * (1 - mask)\n",
    "\n",
    "    # MAE reconstruction pasted with visible patches\n",
    "    im_paste = x * (1 - mask) + y * mask\n",
    "\n",
    "    # make the plt figure larger\n",
    "    plt.rcParams['figure.figsize'] = [24, 24]\n",
    "\n",
    "    plt.subplot(1, 4, 1)\n",
    "    show_image(x[0], \"original\")\n",
    "\n",
    "    plt.subplot(1, 4, 2)\n",
    "    show_image(im_masked[0], \"masked\")\n",
    "\n",
    "    plt.subplot(1, 4, 3)\n",
    "    show_image(y[0], \"reconstruction\")\n",
    "\n",
    "    plt.subplot(1, 4, 4)\n",
    "    show_image(im_paste[0], \"reconstruction + visible\")\n",
    "\n",
    "    plt.show()\n",
    "# Set the seed for PyTorch\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67e0e2b0-b214-4f33-a433-223f7877b1c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install qiskit_machine_learning\n",
    "# !pip install qiskit torch torchvision matplotlib\n",
    "# !pip install qiskit-machine-learning\n",
    "# !pip install torchviz\n",
    "\n",
    "# !pip install qiskit[all]\n",
    "# !pip install qiskit == 0.45.2\n",
    "# !pip install qiskit_algorithms == 0.7.1\n",
    "# !pip install qiskit-ibm-runtime == 0.17.0\n",
    "# !pip install qiskit-aer == 0.13.2\n",
    "\n",
    "# #Quentum net draw\n",
    "# !pip install pylatexenc\n",
    "\n",
    "from qiskit_machine_learning.connectors import TorchConnector\n",
    "from qiskit_machine_learning.neural_networks.estimator_qnn import EstimatorQNN\n",
    "from qiskit_machine_learning.circuit.library import QNNCircuit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f503539d-e62d-446e-86a1-e5f43a7e84e2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Parametrize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ac00b4c-52e2-4b86-a7d3-0a2734bc76c8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not using distributed mode\n",
      "[20:27:47.932845] Namespace(batch_size=256,\n",
      "epochs=10,\n",
      "accum_iter=4,\n",
      "model='mobilenet_v3',\n",
      "input_size=224,\n",
      "drop_path=0.1,\n",
      "clip_grad=None,\n",
      "weight_decay=0.05,\n",
      "lr=None,\n",
      "blr=0.0005,\n",
      "layer_decay=0.65,\n",
      "min_lr=1e-06,\n",
      "warmup_epochs=5,\n",
      "color_jitter=None,\n",
      "aa='rand-m9-mstd0.5-inc1',\n",
      "smoothing=0.1,\n",
      "reprob=0.25,\n",
      "remode='pixel',\n",
      "recount=1,\n",
      "resplit=False,\n",
      "mixup=0.8,\n",
      "cutmix=1.0,\n",
      "cutmix_minmax=None,\n",
      "mixup_prob=1.0,\n",
      "mixup_switch_prob=0.5,\n",
      "mixup_mode='batch',\n",
      "finetune='mae_pretrain_vit_base.pth',\n",
      "global_pool=True,\n",
      "data_path='/media/enc/vera1/sebastian/data/ABGQI_mel_spectrograms',\n",
      "nb_classes=5,\n",
      "output_dir='quinn_5_classes',\n",
      "log_dir='./output_dir',\n",
      "device='cuda',\n",
      "seed=0,\n",
      "resume='/media/enc/vera1/sebastian_hdd/codes/classifiers/mae/MobileNet/quinn_5_classes/checkpoint-49.pth',\n",
      "start_epoch=0,\n",
      "eval=True,\n",
      "dist_eval=False,\n",
      "num_workers=10,\n",
      "pin_mem=True,\n",
      "world_size=1,\n",
      "local_rank=-1,\n",
      "dist_on_itp=False,\n",
      "dist_url='env://',\n",
      "distributed=False)\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser('MAE fine-tuning for image classification', add_help=False)\n",
    "parser.add_argument('--batch_size', default=256, type=int,\n",
    "                        help='Batch size per GPU (effective batch size is batch_size * accum_iter * # gpus')\n",
    "parser.add_argument('--epochs', default=10, type=int)\n",
    "parser.add_argument('--accum_iter', default=4, type=int,\n",
    "                        help='Accumulate gradient iterations (for increasing the effective batch size under memory constraints)')\n",
    "\n",
    "    # Model parameters\n",
    "parser.add_argument('--model', default='mobilenet_v3', type=str, metavar='MODEL',\n",
    "                        help='Name of model to train')\n",
    "\n",
    "parser.add_argument('--input_size', default=224, type=int,\n",
    "                        help='images input size')\n",
    "\n",
    "parser.add_argument('--drop_path', type=float, default=0.1, metavar='PCT',\n",
    "                        help='Drop path rate (default: 0.1)')\n",
    "\n",
    "    # Optimizer parameters\n",
    "parser.add_argument('--clip_grad', type=float, default=None, metavar='NORM',\n",
    "                        help='Clip gradient norm (default: None, no clipping)')\n",
    "parser.add_argument('--weight_decay', type=float, default=0.05,\n",
    "                        help='weight decay (default: 0.05)')\n",
    "\n",
    "parser.add_argument('--lr', type=float, default=None, metavar='LR',\n",
    "                        help='learning rate (absolute lr)')\n",
    "parser.add_argument('--blr', type=float, default=5e-4, metavar='LR',\n",
    "                        help='base learning rate: absolute_lr = base_lr * total_batch_size / 256')\n",
    "parser.add_argument('--layer_decay', type=float, default=0.65,\n",
    "                        help='layer-wise lr decay from ELECTRA/BEiT')\n",
    "\n",
    "parser.add_argument('--min_lr', type=float, default=1e-6, metavar='LR',\n",
    "                        help='lower lr bound for cyclic schedulers that hit 0')\n",
    "\n",
    "parser.add_argument('--warmup_epochs', type=int, default=5, metavar='N',\n",
    "                        help='epochs to warmup LR')\n",
    "\n",
    "    # Augmentation parameters\n",
    "parser.add_argument('--color_jitter', type=float, default=None, metavar='PCT',\n",
    "                        help='Color jitter factor (enabled only when not using Auto/RandAug)')\n",
    "parser.add_argument('--aa', type=str, default='rand-m9-mstd0.5-inc1', metavar='NAME',\n",
    "                        help='Use AutoAugment policy. \"v0\" or \"original\". \" + \"(default: rand-m9-mstd0.5-inc1)'),\n",
    "parser.add_argument('--smoothing', type=float, default=0.1,\n",
    "                        help='Label smoothing (default: 0.1)')\n",
    "\n",
    "    # * Random Erase params\n",
    "parser.add_argument('--reprob', type=float, default=0.25, metavar='PCT',\n",
    "                        help='Random erase prob (default: 0.25)')\n",
    "parser.add_argument('--remode', type=str, default='pixel',\n",
    "                        help='Random erase mode (default: \"pixel\")')\n",
    "parser.add_argument('--recount', type=int, default=1,\n",
    "                        help='Random erase count (default: 1)')\n",
    "parser.add_argument('--resplit', action='store_true', default=False,\n",
    "                        help='Do not random erase first (clean) augmentation split')\n",
    "    # * Mixup params\n",
    "parser.add_argument('--mixup', type=float, default=0.8,\n",
    "                        help='mixup alpha, mixup enabled if > 0.')\n",
    "parser.add_argument('--cutmix', type=float, default=1.0,\n",
    "                        help='cutmix alpha, cutmix enabled if > 0.')\n",
    "parser.add_argument('--cutmix_minmax', type=float, nargs='+', default=None,\n",
    "                        help='cutmix min/max ratio, overrides alpha and enables cutmix if set (default: None)')\n",
    "parser.add_argument('--mixup_prob', type=float, default=1.0,\n",
    "                        help='Probability of performing mixup or cutmix when either/both is enabled')\n",
    "parser.add_argument('--mixup_switch_prob', type=float, default=0.5,\n",
    "                        help='Probability of switching to cutmix when both mixup and cutmix enabled')\n",
    "\n",
    "parser.add_argument('--mixup_mode', type=str, default='batch',\n",
    "                        help='How to apply mixup/cutmix params. Per \"batch\", \"pair\", or \"elem\"')\n",
    "\n",
    "    # * Finetuning params\n",
    "parser.add_argument('--finetune', default='mae_pretrain_vit_base.pth',\n",
    "                        help='finetune from checkpoint')\n",
    "parser.add_argument('--global_pool', action='store_true')\n",
    "parser.set_defaults(global_pool=True)\n",
    "parser.add_argument('--cls_token', action='store_false', dest='global_pool',\n",
    "                        help='Use class token instead of global pool for classification')\n",
    "\n",
    "    # Dataset parameters\n",
    "parser.add_argument('--data_path', default='/media/enc/vera1/sebastian/data/ABGQI_mel_spectrograms', type=str,\n",
    "                        help='dataset path')\n",
    "parser.add_argument('--nb_classes', default=5, type=int,\n",
    "                        help='number of the classification types')\n",
    "\n",
    "parser.add_argument('--output_dir', default='quinn_5_classes',\n",
    "                        help='path where to save, empty for no saving')\n",
    "parser.add_argument('--log_dir', default='./output_dir',\n",
    "                        help='path where to tensorboard log')\n",
    "parser.add_argument('--device', default='cuda',\n",
    "                        help='device to use for training / testing')\n",
    "parser.add_argument('--seed', default=0, type=int)\n",
    "parser.add_argument('--resume', default=\"/media/enc/vera1/sebastian_hdd/codes/classifiers/mae/MobileNet/quinn_5_classes/checkpoint-49.pth\",\n",
    "                        help='resume from checkpoint')\n",
    "\n",
    "parser.add_argument('--start_epoch', default=0, type=int, metavar='N',\n",
    "                        help='start epoch')\n",
    "parser.add_argument('--eval',default=True, action='store_true',\n",
    "                        help='Perform evaluation only')\n",
    "parser.add_argument('--dist_eval', action='store_true', default=False,\n",
    "                        help='Enabling distributed evaluation (recommended during training for faster monitor')\n",
    "parser.add_argument('--num_workers', default=10, type=int)\n",
    "parser.add_argument('--pin_mem', action='store_true',\n",
    "                        help='Pin CPU memory in DataLoader for more efficient (sometimes) transfer to GPU.')\n",
    "parser.add_argument('--no_pin_mem', action='store_false', dest='pin_mem')\n",
    "parser.set_defaults(pin_mem=True)\n",
    "\n",
    "    # distributed training parameters\n",
    "parser.add_argument('--world_size', default=1, type=int,\n",
    "                        help='number of distributed processes')\n",
    "parser.add_argument('--local_rank', default=-1, type=int)\n",
    "parser.add_argument('--dist_on_itp', action='store_true')\n",
    "parser.add_argument('--dist_url', default='env://',\n",
    "                        help='url used to set up distributed training')\n",
    "args, unknown = parser.parse_known_args()\n",
    "misc.init_distributed_mode(args)\n",
    "print(\"{}\".format(args).replace(', ', ',\\n'))\n",
    "os.makedirs(args.output_dir, exist_ok=True)\n",
    "device = torch.device(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b584b6c0-0251-40ac-bcb2-b9ecbf0d3c8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from torchvision.models import mobilenet_v3_large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c12a818d-2e05-4bda-bb6f-c64ccd96db97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model_ft = mobilenet_v3_large(pretrained=True, progress=True)\n",
    "# model_ft.classifier[-1] = nn.Linear(1280, args.nb_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14b4721a-265b-44b4-ad59-5b0d021c846f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# base_model = torch.hub.load('pytorch/vision:v0.10.0', 'mobilenet_v2', pretrained=True)\n",
    "# Remove FC and Global pooling layers to allow for ABGQI fine-tuning\n",
    "# base_model_output = nn.Sequential(*list(base_model.children())[:-3])  # Remove the last 3 layers\n",
    "# print(base_model_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e73e12-e66b-40eb-b5a2-88a23ef3ba96",
   "metadata": {},
   "source": [
    "## Designing a frozen mobileNetv2 \n",
    "Deleting only the classifier layer, then replacing it with a linearizer. https://stackoverflow.com/questions/69321848/fine-tuning-pretrained-model-mobilenet-v3-large-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a086e166-f939-4c49-b504-a944a7ac78a0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "CircuitError",
     "evalue": "'Name conflict on adding parameter: x[0]'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCircuitError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 37\u001b[0m\n\u001b[1;32m     29\u001b[0m     qnn \u001b[38;5;241m=\u001b[39m QNNCircuit(circuit\u001b[38;5;241m=\u001b[39mqc,\n\u001b[1;32m     30\u001b[0m                      input_params\u001b[38;5;241m=\u001b[39mfeature_map\u001b[38;5;241m.\u001b[39mparameters,\n\u001b[1;32m     31\u001b[0m                      weight_params\u001b[38;5;241m=\u001b[39mansatz\u001b[38;5;241m.\u001b[39mparameters,\n\u001b[1;32m     32\u001b[0m                      output_shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m5\u001b[39m,),\n\u001b[1;32m     33\u001b[0m                      output_params\u001b[38;5;241m=\u001b[39m[Parameter(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtheta_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m)],\n\u001b[1;32m     34\u001b[0m                      measurement_error_mitigation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m qnn\n\u001b[0;32m---> 37\u001b[0m qnn \u001b[38;5;241m=\u001b[39m create_qnn()\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mqNet\u001b[39;00m(Module):\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, qnn):\n",
      "Cell \u001b[0;32mIn[17], line 24\u001b[0m, in \u001b[0;36mcreate_qnn\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m ansatz \u001b[38;5;241m=\u001b[39m RealAmplitudes(\u001b[38;5;241m5\u001b[39m, reps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     23\u001b[0m qc \u001b[38;5;241m=\u001b[39m QNNCircuit(\u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m---> 24\u001b[0m qc\u001b[38;5;241m.\u001b[39mcompose(feature_map, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     25\u001b[0m qc\u001b[38;5;241m.\u001b[39mcompose(ansatz, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Create a Quantum Neural Network (QNN) with 5 output neurons\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/qiskit/circuit/library/blueprintcircuit.py:133\u001b[0m, in \u001b[0;36mBlueprintCircuit.compose\u001b[0;34m(self, other, qubits, clbits, front, inplace, wrap)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_built:\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build()\n\u001b[0;32m--> 133\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mcompose(other, qubits, clbits, front, inplace, wrap)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/qiskit/circuit/quantumcircuit.py:1017\u001b[0m, in \u001b[0;36mQuantumCircuit.compose\u001b[0;34m(self, other, qubits, clbits, front, inplace, wrap)\u001b[0m\n\u001b[1;32m   1014\u001b[0m     dest\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m   1016\u001b[0m circuit_scope \u001b[38;5;241m=\u001b[39m dest\u001b[38;5;241m.\u001b[39m_current_scope()\n\u001b[0;32m-> 1017\u001b[0m circuit_scope\u001b[38;5;241m.\u001b[39mextend(mapped_instrs)\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m append_existing:\n\u001b[1;32m   1019\u001b[0m     circuit_scope\u001b[38;5;241m.\u001b[39mextend(append_existing)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/qiskit/circuit/quantumcircuit.py:4973\u001b[0m, in \u001b[0;36m_OuterCircuitScopeInterface.extend\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   4971\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextend\u001b[39m(\u001b[38;5;28mself\u001b[39m, data: CircuitData):\n\u001b[1;32m   4972\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcircuit\u001b[38;5;241m.\u001b[39m_data\u001b[38;5;241m.\u001b[39mextend(data)\n\u001b[0;32m-> 4973\u001b[0m     data\u001b[38;5;241m.\u001b[39mforeach_op(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcircuit\u001b[38;5;241m.\u001b[39m_track_operation)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/qiskit/circuit/quantumcircuit.py:1347\u001b[0m, in \u001b[0;36mQuantumCircuit._track_operation\u001b[0;34m(self, operation)\u001b[0m\n\u001b[1;32m   1345\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Sync all non-data-list internal data structures for a newly tracked operation.\"\"\"\u001b[39;00m\n\u001b[1;32m   1346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(operation, Instruction):\n\u001b[0;32m-> 1347\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_parameter_table(operation)\n\u001b[1;32m   1348\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mduration \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1349\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munit \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/qiskit/circuit/quantumcircuit.py:1364\u001b[0m, in \u001b[0;36mQuantumCircuit._update_parameter_table\u001b[0;34m(self, instruction)\u001b[0m\n\u001b[1;32m   1362\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1363\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m parameter\u001b[38;5;241m.\u001b[39mname \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parameter_table\u001b[38;5;241m.\u001b[39mget_names():\n\u001b[0;32m-> 1364\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m CircuitError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mName conflict on adding parameter: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparameter\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1365\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parameter_table[parameter] \u001b[38;5;241m=\u001b[39m ParameterReferences(\n\u001b[1;32m   1366\u001b[0m         ((instruction, param_index),)\n\u001b[1;32m   1367\u001b[0m     )\n\u001b[1;32m   1369\u001b[0m     \u001b[38;5;66;03m# clear cache if new parameter is added\u001b[39;00m\n",
      "\u001b[0;31mCircuitError\u001b[0m: 'Name conflict on adding parameter: x[0]'"
     ]
    }
   ],
   "source": [
    "from torch.nn import Module, Linear\n",
    "from qiskit_machine_learning.connectors.torch_connector import TorchConnector\n",
    "\n",
    "from torch import Tensor\n",
    "from torch.nn import Linear, CrossEntropyLoss, MSELoss\n",
    "from torch.optim import LBFGS\n",
    "# from qiskit_machine_learning.neural_networks import CircuitQNN\n",
    "from qiskit_machine_learning.circuit.library import QNNCircuit\n",
    "\n",
    "from qiskit.circuit.library import RealAmplitudes, ZZFeatureMap\n",
    "\n",
    "\n",
    "from qiskit import QuantumCircuit\n",
    "from qiskit.circuit import Parameter\n",
    "from qiskit.circuit.library import RealAmplitudes, ZZFeatureMap\n",
    "from qiskit_algorithms.utils import algorithm_globals\n",
    "from qiskit_machine_learning.neural_networks import SamplerQNN, EstimatorQNN\n",
    "from qiskit_machine_learning.connectors import TorchConnector\n",
    "from qiskit_machine_learning.circuit.library import QNNCircuit\n",
    "\n",
    "def create_qnn():\n",
    "    feature_map = ZZFeatureMap(5)\n",
    "    ansatz = RealAmplitudes(5, reps=1)\n",
    "    qc = QNNCircuit(5)\n",
    "    qc.compose(feature_map, inplace=True)\n",
    "    qc.compose(ansatz, inplace=True)\n",
    "\n",
    "    # Create a Quantum Neural Network (QNN) with 5 output neurons\n",
    "    \n",
    "    qnn = QNNCircuit(circuit=qc,\n",
    "                     input_params=feature_map.parameters,\n",
    "                     weight_params=ansatz.parameters,\n",
    "                     output_shape=(5,),\n",
    "                     output_params=[Parameter(f\"theta_{i}\") for i in range(5)],\n",
    "                     measurement_error_mitigation=True)\n",
    "    return qnn\n",
    "\n",
    "qnn = create_qnn()\n",
    "\n",
    "class qNet(Module):\n",
    "    def __init__(self, qnn):\n",
    "        super(qNet, self).__init__()\n",
    "        # Load the pre-trained MobileNetV3 model\n",
    "        self.mobilenet = models.mobilenet_v3_large(pretrained=True, progress=True)\n",
    "        # Freeze all layers except the classifier\n",
    "        for param in self.mobilenet.parameters():\n",
    "            param.requires_grad = False\n",
    "        num_classes = args.nb_classes\n",
    "        in_features = self.mobilenet.classifier[-1].in_features\n",
    "        self.mobilenet.classifier[-1] = nn.Linear(in_features, num_classes)\n",
    "        num_layers_unfreeze = 50\n",
    "        # Unfreeze the last layer for fine-tuning\n",
    "        for param in self.mobilenet.classifier[-num_layers_unfreeze:].parameters():\n",
    "            param.requires_grad = True\n",
    "        # Ensure that qnn is a PyTorch neural network object\n",
    "        self.qnn = TorchConnector(qnn)\n",
    "        \n",
    "        self.fc1 = nn.Linear(num_classes, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass the input through the MobileNetV3 model\n",
    "        x = self.mobilenet(x)\n",
    "        # Apply the quantum network in the forward section\n",
    "        x = self.qnn(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten the output\n",
    "        return self.fc1(x)\n",
    "\n",
    "\n",
    "# # # Create random input data\n",
    "batch_size = 1\n",
    "channels = 3\n",
    "height = 224\n",
    "width = 224\n",
    "random_input = torch.randn(batch_size, channels, height, width)\n",
    "\n",
    "model = qNet(qnn)\n",
    "\n",
    "output = model(random_input)\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f01755d-c0d1-461a-ae1e-382bf748c5c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9bfc32-d08e-42ab-ada5-5276ae658b4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a25a7da-4ed2-4708-b4f7-7e93ec2aad88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f71334-2275-4e31-b163-6893815b4f10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model(random_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e52885-7e00-4ebd-8f8e-a21c2afb0011",
   "metadata": {},
   "source": [
    "since pytorch > 1.8, then have to update this i ~/anaconda3/lib/python3.11/site-packages/timm/models/layers/helpers.py with https://github.com/huggingface/pytorch-image-models/issues/420 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcb2899-533c-4472-b263-b94f4d90f256",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "misc.init_distributed_mode(args)\n",
    "\n",
    "    # print('job dir: {}'.format(os.path.dirname(os.path.realpath(__file__))))\n",
    "print(\"{}\".format(args).replace(', ', ',\\n'))\n",
    "\n",
    "device = torch.device(args.device)\n",
    "\n",
    "seed = args.seed + misc.get_rank()\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "cudnn.benchmark = True\n",
    "\n",
    "dataset_train = build_dataset(is_train=True, args=args)\n",
    "dataset_val = build_dataset(is_train=False, args=args)\n",
    "\n",
    "if True:  # args.distributed:\n",
    "        num_tasks = misc.get_world_size()\n",
    "        global_rank = misc.get_rank()\n",
    "        sampler_train = torch.utils.data.DistributedSampler(\n",
    "            dataset_train, num_replicas=num_tasks, rank=global_rank, shuffle=True\n",
    "        )\n",
    "        print(\"Sampler_train = %s\" % str(sampler_train))\n",
    "        if args.dist_eval:\n",
    "            if len(dataset_val) % num_tasks != 0:\n",
    "                print('Warning: Enabling distributed evaluation with an eval dataset not divisible by process number. '\n",
    "                      'This will slightly alter validation results as extra duplicate entries are added to achieve '\n",
    "                      'equal num of samples per-process.')\n",
    "            sampler_val = torch.utils.data.DistributedSampler(\n",
    "                dataset_val, num_replicas=num_tasks, rank=global_rank, shuffle=True)  # shuffle=True to reduce monitor bias\n",
    "        else:\n",
    "            sampler_val = torch.utils.data.SequentialSampler(dataset_val)\n",
    "else:\n",
    "        sampler_train = torch.utils.data.RandomSampler(dataset_train)\n",
    "        sampler_val = torch.utils.data.SequentialSampler(dataset_val)\n",
    "\n",
    "if global_rank == 0 and args.log_dir is not None and not args.eval:\n",
    "        os.makedirs(args.log_dir, exist_ok=True)\n",
    "        log_writer = SummaryWriter(log_dir=args.log_dir)\n",
    "else:\n",
    "        log_writer = None\n",
    "\n",
    "data_loader_train = torch.utils.data.DataLoader(\n",
    "        dataset_train, sampler=sampler_train,\n",
    "        batch_size=args.batch_size,\n",
    "        num_workers=args.num_workers,\n",
    "        pin_memory=args.pin_mem,\n",
    "        drop_last=True,\n",
    ")\n",
    "\n",
    "data_loader_val = torch.utils.data.DataLoader(\n",
    "        dataset_val, sampler=sampler_val,\n",
    "        batch_size=args.batch_size,\n",
    "        num_workers=args.num_workers,\n",
    "        pin_memory=args.pin_mem,\n",
    "        drop_last=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd05d05-9434-4c06-a03a-801f4f2a59ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mixup_fn = None\n",
    "mixup_active = args.mixup > 0 or args.cutmix > 0. or args.cutmix_minmax is not None\n",
    "if mixup_active:\n",
    "        print(\"Mixup is activated!\")\n",
    "        mixup_fn = Mixup(\n",
    "            mixup_alpha=args.mixup, cutmix_alpha=args.cutmix, cutmix_minmax=args.cutmix_minmax,\n",
    "            prob=args.mixup_prob, switch_prob=args.mixup_switch_prob, mode=args.mixup_mode,\n",
    "            label_smoothing=args.smoothing, num_classes=args.nb_classes)\n",
    "    \n",
    "# model = models_vit.__dict__[args.model](\n",
    "#         num_classes=args.nb_classes,\n",
    "#         drop_path_rate=args.drop_path,\n",
    "#         global_pool=args.global_pool,\n",
    "# )\n",
    "model = stage_1_model\n",
    "\n",
    "if args.finetune and not args.eval:\n",
    "        print(\"args.finetune and not args.eval\")\n",
    "        checkpoint = torch.load(args.finetune, map_location='cpu')\n",
    "\n",
    "        print(\"Load pre-trained checkpoint from: %s\" % args.finetune)\n",
    "        checkpoint_model = checkpoint['model']\n",
    "        state_dict = model.state_dict()\n",
    "        for k in ['head.weight', 'head.bias']:\n",
    "            if k in checkpoint_model and checkpoint_model[k].shape != state_dict[k].shape:\n",
    "                print(f\"Removing key {k} from pretrained checkpoint\")\n",
    "                del checkpoint_model[k]\n",
    "\n",
    "        # interpolate position embedding\n",
    "        interpolate_pos_embed(model, checkpoint_model)\n",
    "\n",
    "        # load pre-trained model\n",
    "        msg = model.load_state_dict(checkpoint_model, strict=False)\n",
    "        print(msg)\n",
    "\n",
    "        if args.global_pool:\n",
    "            assert set(msg.missing_keys) == {'head.weight', 'head.bias', 'fc_norm.weight', 'fc_norm.bias'}\n",
    "        else:\n",
    "            assert set(msg.missing_keys) == {'head.weight', 'head.bias'}\n",
    "\n",
    "        # manually initialize fc layer\n",
    "        trunc_normal_(model.head.weight, std=2e-5)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "model_without_ddp = model\n",
    "\n",
    "n_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"Model = %s\" % str(model_without_ddp))\n",
    "print('number of params (M): %.2f' % (n_parameters / 1.e6))\n",
    "\n",
    "eff_batch_size = args.batch_size * args.accum_iter * misc.get_world_size()\n",
    "    \n",
    "if args.lr is None:  # only base_lr is specified\n",
    "        args.lr = args.blr * eff_batch_size / 256\n",
    "\n",
    "print(\"base lr: %.2e\" % (args.lr * 256 / eff_batch_size))\n",
    "print(\"actual lr: %.2e\" % args.lr)\n",
    "\n",
    "print(\"accumulate grad iterations: %d\" % args.accum_iter)\n",
    "print(\"effective batch size: %d\" % eff_batch_size)\n",
    "\n",
    "if args.distributed:\n",
    "        model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.gpu])\n",
    "        model_without_ddp = model.module\n",
    "\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=args.lr)#SEB\n",
    "# optimizer = torch.optim.AdamW(param_groups, lr=args.lr)\n",
    "loss_scaler = NativeScaler()\n",
    "\n",
    "if mixup_fn is not None:\n",
    "        # smoothing is handled with mixup label transform\n",
    "        criterion = SoftTargetCrossEntropy()\n",
    "elif args.smoothing > 0.:\n",
    "        criterion = LabelSmoothingCrossEntropy(smoothing=args.smoothing)\n",
    "else:\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "print(\"criterion = %s\" % str(criterion))\n",
    "\n",
    "misc.load_model(args=args, model_without_ddp=model_without_ddp, optimizer=optimizer, loss_scaler=loss_scaler)\n",
    "\n",
    "if args.eval:\n",
    "        test_stats = evaluate(data_loader_val, model, device)\n",
    "        print(f\"Accuracy of the network on the {len(dataset_val)} test images: {test_stats['acc1']:.1f}%\")\n",
    "        # exit(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aee713c-2882-48de-945a-c11123262717",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Train model\n",
    "Run a conda environment or alternatively just run the cell below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980c8d1d-2082-4ef6-b434-4a971277ec72",
   "metadata": {},
   "source": [
    "### Base model\n",
    "Finetuning using the base vit pretrained model  which is downloaded from here (https://github.com/facebookresearch/mae?tab=readme-ov-file#fine-tuning-with-pre-trained-checkpoints). The way to find the correct finetunng is explaned here (https://github.com/facebookresearch/mae/issues?q=is%3Aissue+is%3Aopen+classes )\n",
    "\n",
    "`python main_finetune.py --accum_iter 4 --batch_size 32 --model vit_base_patch16 --finetune mae_pretrain_vit_base.pth --epochs 100 --blr 5e-4 --layer_decay 0.65 --weight_decay 0.05 --drop_path 0.1 --mixup 0.8 --cutmix 1.0 --reprob 0.25 --dist_eval --data_path /media/enc/vera1/sebastian/data/Data-set-Urban_Esc/ --nb_classes 7`\n",
    "\n",
    "Expected results: \n",
    "\n",
    "```\n",
    "[04:05:12.377034] * Acc@1 83.388 Acc@5 99.836 loss 0.572\n",
    "[04:05:12.377152] Accuracy of the network on the 608 test images: 83.4%\n",
    "[04:05:12.377165] Max accuracy: 83.55%\n",
    "[04:05:12.378265] Training time 0:46:57\n",
    "```\n",
    "\n",
    "### Large model\n",
    "\n",
    "\n",
    "`python main_finetune.py --accum_iter 4 --batch_size 16 --model vit_large_patch16 --finetune mae_pretrain_vit_large.pth --epochs 100 --blr 5e-4 --layer_decay 0.65 --weight_decay 0.05 --drop_path 0.1 --mixup 0.8 --cutmix 1.0 --reprob 0.25 --dist_eval --data_path /media/enc/vera1/sebastian/data/Data-set-Urban_Esc/ --nb_classes 7 --output_dir EXP_large_vit`\n",
    "\n",
    "Expected results: \n",
    "\n",
    "```\n",
    "[06:10:18.181183] Test: Total time: 0:00:04 (0.1237 s / it)\n",
    "[06:10:18.181592] * Acc@1 83.059 Acc@5 99.671 loss 0.586\n",
    "[06:10:18.181741] Accuracy of the network on the 608 test images: 83.1%\n",
    "[06:10:18.181759] Max accuracy: 83.06%\n",
    "[06:10:18.182753] Training time 1:18:19\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfd5d05-fb33-4759-aa29-37debddc0e3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = True\n",
    "\n",
    "if train:\n",
    "    print(f\"Start training for {args.epochs} epochs with batch size of {args.batch_size}\")\n",
    "    start_time = time.time()\n",
    "    max_accuracy = 0.0\n",
    "    \n",
    "    for epoch in range(args.start_epoch, args.epochs):\n",
    "        if args.distributed:\n",
    "            data_loader_train.sampler.set_epoch(epoch)\n",
    "        train_stats = train_one_epoch(\n",
    "            model, criterion, data_loader_train,\n",
    "            optimizer, device, epoch, loss_scaler,\n",
    "            args.clip_grad, mixup_fn,\n",
    "            log_writer=log_writer,\n",
    "            args=args\n",
    "        )\n",
    "        if args.output_dir:\n",
    "            misc.save_model(\n",
    "                args=args, model=model, model_without_ddp=model_without_ddp, optimizer=optimizer,\n",
    "                loss_scaler=loss_scaler, epoch=epoch)\n",
    "\n",
    "        test_stats = evaluate(data_loader_val, model, device)\n",
    "        print(f\"Accuracy of the network on the {len(dataset_val)} test images: {test_stats['acc1']:.1f}%\")\n",
    "        max_accuracy = max(max_accuracy, test_stats[\"acc1\"])\n",
    "        print(f'Max accuracy: {max_accuracy:.2f}%')\n",
    "\n",
    "        if log_writer is not None:\n",
    "            log_writer.add_scalar('perf/test_acc1', test_stats['acc1'], epoch)\n",
    "            log_writer.add_scalar('perf/test_acc5', test_stats['acc5'], epoch)\n",
    "            log_writer.add_scalar('perf/test_loss', test_stats['loss'], epoch)\n",
    "\n",
    "        log_stats = {**{f'train_{k}': v for k, v in train_stats.items()},\n",
    "                        **{f'test_{k}': v for k, v in test_stats.items()},\n",
    "                        'epoch': epoch,\n",
    "                        'n_parameters': n_parameters}\n",
    "\n",
    "        if args.output_dir and misc.is_main_process():\n",
    "            if log_writer is not None:\n",
    "                log_writer.flush()\n",
    "            with open(os.path.join(args.output_dir, \"log.txt\"), mode=\"a\", encoding=\"utf-8\") as f:\n",
    "                f.write(json.dumps(log_stats) + \"\\n\")\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
    "    print('Training time {}'.format(total_time_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f19af4-f98c-4cac-893b-45a39ee17caa",
   "metadata": {},
   "source": [
    "# Evaluate loaded model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f3c948e-620d-448f-a08e-dd3f3a8e58e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "EXPERIMENT_NAME = \"mobileNet\"\n",
    "\n",
    "saving_model = f\"{EXPERIMENT_NAME}/models\"\n",
    "os.makedirs(saving_model, exist_ok = True)\n",
    "os.makedirs(EXPERIMENT_NAME, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ebc1e78-3a15-40df-ad5f-6c0c756b6d6b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_loader_val' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39meval:\n\u001b[0;32m----> 2\u001b[0m         test_stats \u001b[38;5;241m=\u001b[39m evaluate(data_loader_val, model, device)\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy of the network on the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(dataset_val)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m test images: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_stats[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124macc1\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_loader_val' is not defined"
     ]
    }
   ],
   "source": [
    "if args.eval:\n",
    "        test_stats = evaluate(data_loader_val, model, device)\n",
    "        print(f\"Accuracy of the network on the {len(dataset_val)} test images: {test_stats['acc1']:.1f}%\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285c69a2-6d2d-4368-9c9d-01bdf74613c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate_test(data_loader, model, device):\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    metric_logger = misc.MetricLogger(delimiter=\"  \")\n",
    "    header = 'Test:'\n",
    "\n",
    "    # switch to evaluation mode\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    for batch in metric_logger.log_every(data_loader, 10, header):\n",
    "        images = batch[0]\n",
    "        target = batch[-1]\n",
    "        images = images.to(device, non_blocking=True)\n",
    "        target = target.to(device, non_blocking=True)\n",
    "\n",
    "        # compute output\n",
    "        with torch.cuda.amp.autocast():\n",
    "            output = model(images)            \n",
    "            loss = criterion(output, target)#\n",
    "            pred = output.argmax(dim=1) \n",
    "        all_predictions.append(pred.cpu().numpy())# ADDED\n",
    "        all_labels.append(target.cpu().numpy())# ADDED\n",
    "        acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
    "        \n",
    "        batch_size = images.shape[0]\n",
    "        metric_logger.update(loss=loss.item())\n",
    "        metric_logger.meters['acc1'].update(acc1.item(), n=batch_size)\n",
    "        metric_logger.meters['acc5'].update(acc5.item(), n=batch_size)\n",
    "    \n",
    "    all_predictions = np.array(all_predictions)#.squeeze(0)\n",
    "    all_labels = np.array(all_labels)#.squeeze(0)\n",
    "    # gather the stats from all processes\n",
    "    metric_logger.synchronize_between_processes()\n",
    "    print('* Acc@1 {top1.global_avg:.3f} Acc@5 {top5.global_avg:.3f} loss {losses.global_avg:.3f}'\n",
    "          .format(top1=metric_logger.acc1, top5=metric_logger.acc5, losses=metric_logger.loss))\n",
    "\n",
    "    # return \n",
    "\n",
    "    return {k: meter.global_avg for k, meter in metric_logger.meters.items()}, np.concatenate(all_predictions, axis=0), np.concatenate(all_labels, axis=0)\n",
    "    \n",
    "\n",
    "metrics, all_predictions, all_labels = evaluate_test(data_loader_val, model, device)\n",
    "# print(f\"Accuracy of the network on the {len(dataset_val)} test images: {test_stats['acc1']:.1f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37251664-d171-4983-827a-b8eccbffcb9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fb3829-f24f-4748-a7a7-0b7c24e26795",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0792b94-181c-4582-8a21-6708545a63d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "unique_classes = np.unique(np.concatenate((all_labels, all_predictions)))\n",
    "unique_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15f592c-a922-439c-9656-f211f32fc0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_mat = confusion_matrix(all_labels, all_predictions, labels=unique_classes)\n",
    "conf_matrix = pd.DataFrame(confusion_mat, index=unique_classes, columns=unique_classes)\n",
    "conf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752d70d9-0d75-478a-a90a-0815db8fdb89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "unique_classes = np.unique(np.concatenate((all_labels, all_predictions)))\n",
    "confusion_mat = confusion_matrix(all_labels, all_predictions, labels=unique_classes)\n",
    "conf_matrix = pd.DataFrame(confusion_mat, index=unique_classes, columns=unique_classes)\n",
    "\n",
    "# Plot the confusion matrix using seaborn\n",
    "plt.figure(figsize=(5, 4))\n",
    "ax = sns.heatmap(conf_matrix, annot=True,  fmt='.1f', cmap=sns.cubehelix_palette(as_cmap=True), linewidths=0.1, cbar=True)\n",
    "\n",
    "# Set labels and ticks\n",
    "ax.set_xlabel('Predicted Labels')\n",
    "ax.set_ylabel('True Labels')\n",
    "\n",
    "# Set x and y ticks using the unique classes\n",
    "ax.set_xticks(range(len(unique_classes)))\n",
    "ax.set_yticks(range(len(unique_classes)))\n",
    "\n",
    "# Set x and y ticks at the center of the cells\n",
    "ax.set_xticks([i + 0.5 for i in range(len(unique_classes))])\n",
    "ax.set_yticks([i + 0.5 for i in range(len(unique_classes))])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6ac4b1-69e4-4e2b-bb75-65197ad1875c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_multiclass_roc_curve(all_labels, all_predictions, EXPERIMENT_NAME=\".\"):\n",
    "    # Step 1: Label Binarization\n",
    "    label_binarizer = LabelBinarizer()\n",
    "    y_onehot = label_binarizer.fit_transform(all_labels)\n",
    "    all_predictions_hot = label_binarizer.transform(all_predictions)\n",
    "\n",
    "    # Step 2: Calculate ROC curves\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    unique_classes = range(y_onehot.shape[1])\n",
    "    for i in unique_classes:\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_onehot[:, i], all_predictions_hot[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    # Step 3: Plot ROC curves\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "    # Micro-average ROC curve\n",
    "    fpr_micro, tpr_micro, _ = roc_curve(y_onehot.ravel(), all_predictions_hot.ravel())\n",
    "    roc_auc_micro = auc(fpr_micro, tpr_micro)\n",
    "    plt.plot(\n",
    "        fpr_micro,\n",
    "        tpr_micro,\n",
    "        label=f\"micro-average ROC curve (AUC = {roc_auc_micro:.2f})\",\n",
    "        color=\"deeppink\",\n",
    "        linestyle=\":\",\n",
    "        linewidth=4,\n",
    "    )\n",
    "\n",
    "    # Macro-average ROC curve\n",
    "    all_fpr = np.unique(np.concatenate([fpr[i] for i in unique_classes]))\n",
    "    mean_tpr = np.zeros_like(all_fpr)\n",
    "    for i in unique_classes:\n",
    "        mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
    "    mean_tpr /= len(unique_classes)\n",
    "    fpr_macro = all_fpr\n",
    "    tpr_macro = mean_tpr\n",
    "    roc_auc_macro = auc(fpr_macro, tpr_macro)\n",
    "    plt.plot(\n",
    "        fpr_macro,\n",
    "        tpr_macro,\n",
    "        label=f\"macro-average ROC curve (AUC = {roc_auc_macro:.2f})\",\n",
    "        color=\"navy\",\n",
    "        linestyle=\":\",\n",
    "        linewidth=4,\n",
    "    )\n",
    "\n",
    "    # Individual class ROC curves with unique colors\n",
    "    colors = plt.cm.rainbow(np.linspace(0, 1, len(unique_classes)))\n",
    "    for class_id, color in zip(unique_classes, colors):\n",
    "        plt.plot(\n",
    "            fpr[class_id],\n",
    "            tpr[class_id],\n",
    "            color=color,\n",
    "            label=f\"ROC curve for Class {class_id} (AUC = {roc_auc[class_id]:.2f})\",\n",
    "            linewidth=2,\n",
    "        )\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], color='gray', linestyle='--', linewidth=2)  # Add diagonal line for reference\n",
    "    plt.axis(\"equal\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"Extension of Receiver Operating Characteristic\\n to One-vs-Rest multiclass\")\n",
    "    plt.legend()\n",
    "    plt.savefig(f'{EXPERIMENT_NAME}/roc_curve.png')\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "    \n",
    "plot_multiclass_roc_curve(all_labels, all_predictions, EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806d2d76-84c6-4020-8bfb-56f545976b8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# def visualize_predictions(model, val_loader, device, type_label=None, dataset_type=1, unique_classes=np.array([0, 1, 2, 3, 4, 5, 6])):\n",
    "\n",
    "#     criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "#     metric_logger = misc.MetricLogger(delimiter=\"  \")\n",
    "#     header = 'Test:'\n",
    "\n",
    "#     # switch to evaluation mode\n",
    "#     model.eval()\n",
    "#     all_predictions = []\n",
    "#     all_labels = []\n",
    "\n",
    "#     for batch in metric_logger.log_every(val_loader, 10, header):\n",
    "#         images = batch[0]\n",
    "#         target = batch[-1]\n",
    "#         images = images.to(device, non_blocking=True)\n",
    "#         target = target.to(device, non_blocking=True)\n",
    "\n",
    "#         # compute output\n",
    "#         with torch.cuda.amp.autocast():\n",
    "#             output = model(images)            \n",
    "#             loss = criterion(output, target)#\n",
    "#             pred = output.argmax(dim=1) \n",
    "#         all_predictions.append(pred.cpu().numpy())# ADDED\n",
    "#         all_labels.append(target.cpu().numpy())# ADDED\n",
    "#         acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
    "        \n",
    "#         batch_size = images.shape[0]\n",
    "#         metric_logger.update(loss=loss.item())\n",
    "#         metric_logger.meters['acc1'].update(acc1.item(), n=batch_size)\n",
    "#         metric_logger.meters['acc5'].update(acc5.item(), n=batch_size)\n",
    "    \n",
    "#     all_predictions = np.array(all_predictions)#.squeeze(0)\n",
    "#     all_labels = np.array(all_labels)#.squeeze(0)\n",
    "\n",
    "#     if type_label is None:\n",
    "#         type_label = unique_classes\n",
    "\n",
    "#     # Create a 4x4 grid for visualization\n",
    "#     num_rows = 4\n",
    "#     num_cols = 4\n",
    "\n",
    "#     plt.figure(figsize=(12, 12))\n",
    "\n",
    "#     for i in range(num_rows * num_cols):\n",
    "#         plt.subplot(num_rows, num_cols, i + 1)\n",
    "#         idx = np.random.randint(len(all_labels))\n",
    "#         import pdb;pdb.set_trace()\n",
    "#         plt.imshow(images[idx].cpu().numpy().squeeze(), cmap='gray')\n",
    "\n",
    "#         # Use the class names instead of numeric labels for Fashion MNIST\n",
    "#         if dataset_type == 1:\n",
    "#             class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "#             predicted_class = class_names[all_predictions[idx]]\n",
    "#             actual_class = class_names[all_labels[idx]]\n",
    "#         else:\n",
    "#             predicted_class = all_predictions[idx]\n",
    "#             actual_class = all_labels[idx]\n",
    "\n",
    "#         plt.title(f'Pred: {predicted_class}\\nActual: {actual_class}')\n",
    "#         plt.axis('off')\n",
    "\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "# visualize_predictions(model, data_loader_val, device, dataset_type=2, unique_classes=unique_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970b1388-0d8d-49a4-9547-c4b0de31fcc2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "unique_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d232cf15-0edb-4546-993a-a9839441c967",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "report = classification_report(all_labels, all_predictions, target_names=unique_classes,output_dict=True)# Mostrar el informe de \n",
    "\n",
    "df = pd.DataFrame(report).transpose()\n",
    "df.to_csv(os.path.join(EXPERIMENT_NAME, \"confusion_matrix.csv\"))\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a60e2d-eebc-4547-91be-3d7dac0226da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c907e5c-a3e0-4eae-9994-082b269732d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate precision, recall, and specificity (micro-averaged)\n",
    "precision = precision_score(all_labels, all_predictions, average='micro')\n",
    "recall = recall_score(all_labels, all_predictions, average='micro')\n",
    "\n",
    "# Calculate true negatives, false positives, and specificity (micro-averaged)\n",
    "tn = np.sum((all_labels != 1) & (all_predictions != 1))\n",
    "fp = np.sum((all_labels != 1) & (all_predictions == 1))\n",
    "specificity = tn / (tn + fp)\n",
    "\n",
    "# Calculate F1 score (weighted average)\n",
    "f1 = f1_score(all_labels, all_predictions, average='weighted')\n",
    "evaluation_metrics = {\n",
    "    \"Acc1\": metrics['acc1'],  # Add acc1 metric\n",
    "    \"Acc5\": metrics['acc5'],  # Add acc5 metric\n",
    "    \"loss\": metrics['loss'],  # Add acc5 metric\n",
    "    \"F1 Score\": [f1],\n",
    "    \"Precision\": [precision],\n",
    "    \"Recall\": [recall],\n",
    "    \"Specificity\": [specificity]\n",
    "}\n",
    "evaluation_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cffce5-3186-440c-bc5c-aba6fae47d45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a DataFrame from the dictionary\n",
    "df = pd.DataFrame(evaluation_metrics)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv(f'{EXPERIMENT_NAME}/evaluation_metrics_for_table.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d15f4cc-9861-4be1-97c3-976f8e8014b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252dbe67-f9c2-4ac0-92cb-be76a4d58d3d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "retfound",
   "language": "python",
   "name": "retfound"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
